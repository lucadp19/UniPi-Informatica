\chapter{Applicazioni lineari}

\section{Applicazioni lineari}

\begin{definition}[Applicazione lineare]
    Siano $V, W$ spazi vettoriali. Allora un'applicazione $f : V \to W$ si dice lineare
    se
    \begin{align}
        &f(\vec{0_V}) = \vec{0_W} \\
        &f(\vec{v} + \vec{w}) = f(\vec{v}) + f(\vec{w}) &&\forall v, w \in V \\
        &f(k\vec{v}) = kf(\vec{v})                    &&\forall v\in V, k \in \R 
    \end{align}
    $V$ si dice dominio dell'applicazione lineare, $W$ si dice codominio.
\end{definition}

\begin{definition}[Immagine di un'applicazione lineare]
    Siano $V, W$ spazi vettoriali e sia $f : V \to W$ lineare. Allora si dice immagine di $f$ l'insieme \begin{equation}
        \Imm{f} = \left\{ f(\vec{v}) \mid \vec v \in V\right\}.
    \end{equation}
\end{definition}

\begin{remark}
    Se $f : V \to W$ allora $\Imm{f} \subseteq W$. In particolare si puo' dimostrare che $\Imm{f}$ e' un sottospazio di $W$, e dunque che $0 \leq \dim\Imm{f} \leq \dim W$.
\end{remark}

\begin{definition}[Kernel di un'applicazione lineare]
    Siano $V, W$ spazi vettoriali e sia $f : V \to W$ lineare. Allora si dice kernel (o nucleo) di $f$ l'insieme \begin{equation}
        \ker{f} = \left\{ \vec{v} \in V \mid f(\vec v) = \vec{0_W}\right\}.
    \end{equation}
\end{definition}

\begin{proposition}[Una funzione mappa una base del dominio in un insieme di generatori del codominio]\label{base_mappata_generatori_immagine}
    Sia $f : V \to W$. Allora se $\basis{\vec{v_1}, \dots, \vec{v_n}}$ e' una base di $V$ segue che $\left\{ f(\vec{v_1}), \dots, f(\vec{v_n})\right\}$ e' un insieme di generatori di $\Imm{f}$.
\end{proposition}
\begin{proof}
    Sia $\vec{w} \in \Imm{f}$ generico; allora questo equivale a dire che esiste $\vec{v} \in V$ tale che $f(\vec{v}) = \vec{w}$.
    Dato che $\basis{\vec{v_1}, \dots, \vec{v_n}}$ e' una base di $V$, allora possiamo scrivere $\vec{v}$ come $a_1\vec{v_1} + \dots a_n\vec{v_n}$, dunque 
    \begin{equation*}
        \vec{w} = f(a_1\vec{v_1} + \dots + a_n\vec{v_n}) = a_1f(\vec{v_1}) + \dots + a_nf(\vec{v_n}).
    \end{equation*}
    Dunque per la generalita' di $w$ segue che ogni elemento di $\Imm{f}$ appartiene allo span di $f(\vec{v_1}), \dots, f(\vec{v_n})$, cioe' $\left\{ f(\vec{v_1}), \dots, f(\vec{v_n})\right\}$ e' un insieme di generatori di $\Imm{f}$.
\end{proof}

\begin{theorem} 
    [Teorema delle dimensioni] \label{th_dimensioni}
    Siano $V, W$ spazi vettoriali e sia $f : V \to W$ lineare. Allora vale il seguente fatto:
    \begin{equation}
        \dim V = \dim \Imm f + \dim \ker f.
    \end{equation}
\end{theorem}
\begin{proof}
    Sia $k$ la dimensione di $\ker f$ e $n$ la dimensione di $V$.
    Sia $\alpha = \basis{\vec{v_1}, \dots, \vec{v_k}}$ una base di $\ker f$. Dato che $\ker f$ e' un sottospazio di $V$, per il teorema del completamento ad una base (\ref{th_completamento}) possiamo completare $\alpha$ ad una base $\beta = \basis{\vec{v_1}, \dots, \vec{v_k}, \vec{u_1}, \dots, \vec{u_{n-k}}}$ di $V$.

    Per la proposizione \ref{base_mappata_generatori_immagine} segue che l'immagine della base $\beta$, cioe' $f(\beta) = \basis{f(\vec{v_1}), \dots, f(\vec{v_k}), f(\vec{u_1}), \dots, f(\vec{u_{n-k}})}$, e' una insieme di generatori di $\Imm{f}$. Dato che $\vec{v_1}, \dots, \vec{v_k} \in \ker f$, allora \begin{alignat*}
        {1}
        \Imm{f} &= \Span{f(\vec{v_1}), \dots, f(\vec{v_k}), f(\vec{u_1}), \dots, f(\vec{u_{n-k}})}\\
        &= \Span{0, \dots, 0, f(\vec{u_1}), \dots, f(\vec{u_{n-k}})}\\
        &= \Span{f(\vec{u_1}), \dots, f(\vec{u_{n-k}})}.
    \end{alignat*}

    Se $f(\vec{u_1}), \dots, f(\vec{u_{n-k}})$ sono indipendenti allora segue che essi formano una base di $\Imm{f}$, cioe' che $\dim \Imm{f} = n - k = \dim V - \dim \ker f$.

    Consideriamo quindi una generica combinazione lineare \[
        x_1f(\vec{u_1}) + \dots + x_{n-k}f(\vec{u_{n-k}})
    \] e dimostriamo che imponendola uguale a $\vec 0$ segue che i coefficienti della combinazione devono essere uguali a 0.
    \begin{alignat*}
        {1}
        &x_1f(\vec{u_1}) + \dots + x_{n-k}f(\vec{u_{n-k}}) = \vec 0 \\
        \iff &f(x_1\vec{u_1} + \dots + x_{n-k}\vec{u_{n-k}}) = \vec 0 \\
        \intertext{che per definizione di kernel significa}
        \iff &x_1\vec{u_1} + \dots + x_{n-k}\vec{u_{n-k}} \in \ker f.
    \end{alignat*}
    Dato che $\alpha$ e' una base di $\ker f$ allora segue che
    \begin{alignat*}{1}
        &x_1\vec{u_1} + \dots + x_{n-k}\vec{u_{n-k}} = a_1\vec{v_1} + \dots + a_k\vec{v_k} \\
        \iff &x_1\vec{u_1} + \dots + x_{n-k}\vec{u_{n-k}} - a_1\vec{v_1} - \dots - a_k\vec{v_k} = \vec 0.
    \end{alignat*}
    Ma $\beta = \basis{\vec{v_1}, \dots, \vec{v_k}, \vec{u_1}, \dots, \vec{u_{n-k}}}$ e' una base di $V$, dunque i vettori che la compongono devono essere indipendenti, da cui segue \[
        x_1 = \dots = x_{n-k} = 0.   
    \]
    Dunque $\basis{f(\vec{u_1}), \dots, f(\vec{u_{n-k}})}$ e' una base di $\Imm{f}$ e dunque segue che $\dim V = \dim \ker f + \dim \Imm{f}$, come volevasi dimostrare.
\end{proof}

Una conseguenza diretta del teorema delle dimensioni e' che data una matrice $A$ e riducendola a scalini tramite mosse di riga non cambia la dimensione dello spazio delle colonne.
\begin{proposition}[La dimensione dello spazio delle colonne e' invariante sotto mosse di riga]\label{invarianza_dim_colonne_per_mosse_riga}
    Sia $A \in \M_{n\times m}$ e siano $C_1, \dots, C_m \in \R^n$ le colonne della matrice. Sia $S$ la matrice ottenuta riducendo a scalini per riga la matrice $A$, e siano $C'_1, \dots, C'_m$ le colonne di $S$. Allora \begin{equation}
        \dim \Span{C_1, \dots, C_m} = \dim \Span{C'_1, \dots, C'_m}.
    \end{equation}
\end{proposition}
\begin{proof}
    Dato che $S$ e' ottenuta riducendo $A$ a scalini, allora le soluzioni dei sistemi $A\vec{x} = \vec 0$ e $S\vec{x} = \vec 0$ devono essere le stesse. 
    
    Siano $L_A : \R^m \to \R^n$ e $L_S : \R^m \to \R^n$ le applicazioni lineari associate ad $A$ e $S$; trascrivendo le equazioni di prima in termini delle applicazioni otteniamo che $L_A(\vec x) = \vec 0$ se e solo se $L_S(\vec x) = \vec 0$. 
    
    Allora segue che $\ker L_A = \ker L_S$, dunque $\dim \ker L_A = \dim \ker L_S$. Per la proposizione \ref{span_colonne=immagine_applicazione_associata} e per il teorema delle dimensioni (\ref{th_dimensioni}) segue quindi che
    \begin{alignat*}
        {1}
        \dim \Span{C_1, \dots, C_m} &= \dim \Imm{L_A}\\
        &= \dim \R^m - \dim \ker L_A\\
        &= \dim \R^m - \dim \ker L_S\\
        &= \dim \Imm{L_S} \\
        &= \dim \Span{C'_1, \dots, C'_m}
    \end{alignat*}
    che e' la tesi.
\end{proof}

Ovviamente lo stesso ragionamento ci dice che ridurre una matrice a scalini tramite mosse di colonna non cambia la dimensione dello spazio delle righe.

\begin{corollary}[La dimensione dello spazio delle righe e' invariante sotto mosse di colonna]\label{invarianza_dim_righe_per_mosse_colonna}
    Sia $A \in \M_{n\times m}$ e siano $R_1, \dots, R_m \in \R^n$ le righe della matrice. Sia $S$ la matrice ottenuta riducendo a scalini per colonna la matrice $A$, e siano $R'_1, \dots, R'_m$ le righe di $S$. Allora \begin{equation}
        \dim \Span{R_1, \dots, R_m} = \dim \Span{R'_1, \dots, R'_m}.
    \end{equation}
\end{corollary}
\begin{proof}
    Consideriamo la matrice $A^T$ e riduciamola per righe, ottenendo la matrice $S$. Per la proposizione \ref{invarianza_dim_colonne_per_mosse_riga} la dimensione dello spazio delle righe di $S$ e' uguale alla dimensione dello spazio delle righe di $A^T$. Notiamo inoltre che $S$ e' la trasposta della matrice ottenuta riducendo $A$ per colonne, dunque la dimensione dello spazio delle colonne di $S^T$ e' la dimensione dello spazio delle colonne di $A$, cioe' la tesi.
\end{proof}

\section{Applicazioni iniettive e surgettive}

Le applicazioni lineari sono funzioni, dunque possono essere iniettive e surgettive, ma essendo lineari hanno delle proprieta' particolari.

\begin{definition}[Applicazione iniettiva]
    Siano $V, W$ spazi vettoriali e sia $f : V \to W$ lineare. Allora $f$ si dice iniettiva se per ogni $\vec{v}, \vec{u} \in V$ vale che $f(\vec{v}) = f(\vec{u})$ se e solo se $\vec{v} = \vec{u}$.
\end{definition}

\begin{proposition}[Un'applicazione e' iniettiva se e solo se il kernel e' nullo]\label{ker_funzione_iniettiva}
    Siano $V, W$ spazi vettoriali e sia $f : V \to W$ lineare. Allora $f$ e' iniettiva se e solo se $\ker f = \{\vec{0_V}\}$. 
\end{proposition}
\begin{proof}
    Notiamo che dato che $f$ e' lineare allora per definizione $f(\vec{0_V}) = \vec{0_W}$, dunque $\vec{0_V} \in \ker f$.
    \begin{description}
        \item [($\implies$).] Supponiamo che $f$ sia iniettiva e supponiamo che per qualche $\vec{v} \in V$ valga $\vec{v} \in \ker f$. Allora per definizione di kernel $f(\vec{v}) = \vec{0_W} = f(\vec{0_V})$, dunque per iniettivita' di $f$ da $f(\vec{v}) = f(\vec{0_V})$ segue che $\vec v = \vec{0_V}$. Dunque $\ker f = \{\vec{0_V}\}$.
        \item [($\impliedby$).] Supponiamo che $\ker f = \left\{ \vec{0_V}\right\}$. Per dimostrare che $f$ e' iniettiva e' sufficiente dimostrare che per ogni $\vec{v}, \vec{w} \in V$ segue che $f(\vec{v}) = f(\vec{w}) \implies \vec{v} = \vec{w}$.
        \begin{alignat*}{1}
            &f(\vec{v}) = f(\vec{w}) \\
            \iff &f(\vec{v}) - f(\vec{w}) = \vec{0_W} \\
            \iff &f(\vec{v} - \vec{w}) = \vec{0_W} \\
            \iff &\vec{v} - \vec{w} \in \ker f \\
            \intertext{ma l'unico elemento di $\ker f$ e' $\vec{0_V}$, dunque}
            \implies &\vec{v} - \vec{w} = \vec{0_V}\\
            \iff &\vec{v} = \vec{w}
        \end{alignat*}
        cioe' $f$ e' iniettiva. \qedhere
    \end{description}
\end{proof}

\begin{corollary}[L'immagine di un'applicazione iniettiva ha la stessa dimensione del dominio]\label{iniettiva_allora_dimIm_uguale_dimV}
    Se $f$ e' iniettiva allora $\dim \Imm{f} = \dim V$.
\end{corollary}
\begin{proof}
    Infatti per la proposizione \ref{ker_funzione_iniettiva} $\dim \ker f = 0$, dunque per il teorema delle dimensioni (\ref{th_dimensioni}) segue che $\dim V = \dim \Imm{f} + \dim \ker f = \dim \Imm{f}$.
\end{proof}

\begin{corollary}
    Siano $V, W$ spazi vettoriali tali che $\dim V > \dim W$. Allora non puo' esistere $f : V \to W$ iniettiva.
\end{corollary}
\begin{proof}
    Infatti per il corollario \ref{iniettiva_allora_dimIm_uguale_dimV} segue che $\dim \Imm{f} = \dim V$, ma $\dim \Imm{f} < \dim W$ dunque non puo' essere che $\dim V > \dim W$.
\end{proof}

\begin{proposition}[Un'applicazione iniettiva mappa insiemi linearmente indipendenti in insiemi linearmente indipendenti]\label{indipendenti_mappati_indipendenti}
    Siano $V, W$ spazi vettoriali, $\vec{v_1}, \dots, \vec{v_n} \in V$ linearmente indipendenti e sia $f : V \to W$ lineare. Se $f$ e' iniettiva allora segue che $f(\vec{v_1}), \dots, f(\vec{v_n})$ sono linearmente indipendenti. 
\end{proposition}
\begin{proof}
    Consideriamo una combinazione lineare di $f(\vec{v_1}), \dots, f(\vec{v_n})$ e dimostriamo che imponendola uguale al vettore nullo segue che i coefficienti devono essere tutti nulli.
    \begin{alignat*}
        {1}
        &x_1f(\vec{v_1}) + \dots + x_nf(\vec{v_n}) = \vec{0_W}\\
        \iff &f(x_1\vec{v_1} + \dots + x_n\vec{v_n}) = \vec{0_W}\\
        \intertext{Per la proposizione \ref{ker_funzione_iniettiva} segue che}
        \iff &x_1\vec{v_1} + \dots + x_n\vec{v_n} = \vec{0_V}\\
        \intertext{Ma i vettori $\vec{v_1}, \dots, \vec{v_n}$ sono linearmente indipendenti, dunque l'unica combinazione lineare che li annulla e' quella a coefficienti nulli, cioe'}
        \iff &x_1 = \dots = x_n = 0
    \end{alignat*}
    Quindi una combinazione lineare di $f(\vec{v_1}), \dots, f(\vec{v_n})$ e' uguale al vettore nullo se e solo se tutti i coefficienti sono nulli, dunque $f(\vec{v_1}), \dots, f(\vec{v_n})$ sono linearmente indipendenti.
\end{proof}

\begin{definition}[Surgettiva]
    Siano $V, W$ spazi vettoriali e sia $f : V \to W$ lineare. Allora $f$ si dice surgettiva se per ogni $\vec{w} \in W$ esiste $\vec{v} \in V$ tale che $f(\vec{v}) = \vec{w}$.
\end{definition}

\begin{remark}
    Una funzione $f : V \to W$ e' surgettiva se e solo se $\Imm{f} = W$.
\end{remark}

\begin{corollary}[Un'applicazione surgettiva mappa una base del dominio ad un insieme di generatori del codominio]\label{base_mappata_generatori_codominio}
    Sia $f : V \to W$ surgettiva. Allora se $\basis{\vec{v_1}, \dots, \vec{v_n}}$ e' una base di $V$ segue che $\left\{ f(\vec{v_1}), \dots, f(\vec{v_n})\right\}$ e' un insieme di generatori di $W$.
\end{corollary}
\begin{proof}
    Segue direttamente dalla proposizione \ref{base_mappata_generatori_immagine}: infatti se una funzione e' surgettiva allora $\Imm{f} = W$, dunque se $\left\{ f(\vec{v_1}), \dots, f(\vec{v_n})\right\}$ e' un insieme di generatori di $\Imm f$ segue che e' anche un insieme di generatori di $W$.
\end{proof}

\section{Isomorfismi}

\begin{definition}[Applicazione bigettiva]
    Siano $V, W$ spazi vettoriali e sia $f : V \to W$ lineare. Allora $f$ si dice bigettiva se $f$ e' sia iniettiva che surgettiva.
\end{definition}

\begin{definition}[Applicazione invertibile]
    Una funzione $f : V \to W$ si dice invertibile se esiste $f^{-1} : W \to V$ tale che \begin{equation}
        f(\vec{v}) = \vec{w} \iff f^{-1}(\vec{w}) = \vec{v}
    \end{equation}
    Se $f$ e' invertibile allora $f^{-1}$ e' unica e si chiama inversa di $f$.
\end{definition}

\begin{remark}
    Un'applicazione lineare e' invertibile se e solo se e' bigettiva. 
\end{remark}

\begin{definition}[Isomorfismo]
    Siano $V, W$ spazi vettoriali e sia $f : V \to W$ lineare. Allora se $f$ e' bigettiva si dice che $f$ e' un isomorfismo.
    
    Se esiste un isomorfismo tra gli spazi $V$ e $W$ allora si dice che $V$ e' isomorfo a $W$, e si indica con $V \cong W$.
\end{definition}

\begin{remark}
    Le seguenti affermazioni sono equivalenti:
    \begin{itemize}
        \item $f$ e' bigettiva;
        \item $f$ e' invertibile;
        \item $f$ e' un isomorfismo.
    \end{itemize}
\end{remark}

Gli isomorfismi preservano la linearita' dello spazio vettoriale e tutte le sue proprieta', come ci dicono le seguenti proposizioni.

\begin{proposition}[Un isomorfismo mappa una base del dominio in una base del codominio]
    Sia $V$ uno spazio vettoriale, $\alpha = \basis{\vec{v_1}, \dots, \vec{v_n}}$ una base di $V$. Allora se $f : V \to W$ e' un isomorfismo segue che $\beta = \basis{f(\vec{v_1}), \dots, f(\vec{v_n})}$ e' una base di $W$ (cioe' gli isomorfismi mappano basi in basi).
\end{proposition}
\begin{proof}
    Dato che $f$ e' un isomorfismo allora $f$ e' bigettiva.

    Dunque dato che $f$ e' iniettiva essa mappa un insieme di vettori indipendenti (come la base $\alpha$ di $V$) in un insieme di vettori linearmente indipendenti per la proposizione \ref{indipendenti_mappati_indipendenti}, dunque $\beta$ e' un insieme di vettori linearmente indipendenti. 

    Inoltre, dato che $f$ e' surgettiva, per la proposizione \ref{base_mappata_generatori_codominio} essa mappa una base di $V$ in un insieme di generatori del codominio $W$, dunque i vettori di $\beta$ generano $W$.

    Dunque $\beta$ e' un insieme di generatori linearmente indipendenti, e quindi e' una base di $W$.
\end{proof}

\begin{proposition}[Gli spazi isomorfi hanno la stessa dimensione]
    Se $V$ e' uno spazio vettoriale di dimensione $n = \dim V$, allora $V$ e' isomorfo a tutti e soli gli spazi vettoriali di dimensione $n$.
\end{proposition}
\begin{proof}
    Deriva direttamente dalla proposizione precedente: infatti ogni isomorfismo che ha come dominio $V$ deve portare una base di $V$ in una base di $W$, dunque la dimensione di $V$ deve essere uguale a quella di $W$.
\end{proof}

Quindi per calcolare una base di un sottospazio $W$ di uno spazio $V$ spesso conviene passare allo spazio dei vettori colonna $\R^n$ isomorfo allo spazio $V$, calcolare la base del sottospazio $\tilde{W}$ isomorfo a $W$ e infine tornare allo spazio di partenza.

\begin{example}
    Sia $V = \R[x]^{\leq 2}$ e sia $W \subseteq V$ il sottospazio di $V$ tale che $p \in W \iff p(2) = 0$. Dimostrare che $W$ e' un sottospazio e trovarne una base.
\end{example}
\begin{solution}
    Svolgiamo i due punti separatamente.
    \begin{enumerate}
        \item Dimostriamo che $W$ e' un sottospazio di $V$.
        \begin{itemize}
            \item Sia $\vec{0_V} \in V$ tale che $\vec{0_V}(x) = 0 + 0x + 0x^2$. Allora $\vec{0_V}(2) = 0 + 0\cdot 2 + 0 \cdot 4 = 0$, dunque $\vec{0_V} \in W$.
            \item Supponiamo $p, q \in W$ e mostriamo che $p+q \in W$. Dunque \[
                (p+q)(2) = p(2) + q(2) = 0 + 0 = 0    
            \] dunque $p + q \in W$.
            \item Supponiamo $p \in W$ e mostriamo che $kp \in W$ per un generico $k \in \R$. Dunque \[
                (kp)(2) = kp(2) = k \cdot 0 = 0    
            \] cioe' $kp \in W$ per ogni $k \in \R$.
        \end{itemize} 
        Dunque abbiamo dimostrato che $W$ e' un sottospazio di $V$.
        \item Cerchiamo ora una base per $W$.
         
        Consideriamo un generico $p \in V$, cioe' $p(x) = a + bx + cx^2$. La condizione che definisce $W$ e' $p(2) = a + 2b + 4c = 0$. 
    
        Passiamo ora allo spazio isomorfo $\R^3$. Il vettore corrispondente a $p$ in $\R^3$ e' $\vec{\tilde{p}} = \begin{psmallmatrix} a \\ b \\ c \end{psmallmatrix}$, mentre la condizione di appartenenza allo spazio $\widetilde{W} \subseteq \R^3$ isomorfo a $W$ e' sempre $a+2b+4c = 0$. Cerchiamo una base di $\widetilde{W}$ passando alla forma parametrica, cioe' cercando di esplicitare la condizione di appartenenza allo spazio e inserendola nella definizione stessa del vettore.
        Dato che la condizione e' data dal sistema $a+2b+4c = 0$ che ha due variabili libere, scelgo $b, c$ libere ottenendo $a = -2b - 4c$. Sostituendo in $\vec{\tilde{p}}$:\[
            \vec{\tilde p} = \begin{pmatrix} a \\ b \\ c \end{pmatrix} = \begin{pmatrix}
                -2b-4c\\b\\c
            \end{pmatrix} = b\begin{pmatrix} -2 \\ 1 \\ 0 \end{pmatrix} + c\begin{pmatrix} -4 \\ 0 \\ 1 \end{pmatrix}
        \]
        Dato che ogni vettore generico di $\widetilde{W}$ puo' essere scritto come combinazione lineare dei due vettori $\vec{\tilde{w}_1} = \begin{psmallmatrix} -2 \\ 1 \\ 0 \end{psmallmatrix}$ e $\vec{\tilde{w}_2} = \begin{psmallmatrix} -4 \\ 0 \\ 1 \end{psmallmatrix}$, allora segue che essi sono generatori di $W$. 
            
        Controlliamo ora che siano linearmente indipendenti riducendo a scalini per riga (secondo la proposizione \ref{estrarre_una_base}) la matrice che ha come colonne $\vec{\tilde{w}_1}$ e $\vec{\tilde{w}_2}$.
        \begin{equation*}
            \begin{pmatrix}[c|c]
                -2 & -4 \\ 1 & 0 \\ 0 & 1
            \end{pmatrix} \xrightarrow[]{R_2 + \frac12R_1}
            \begin{pmatrix}[c|c]
                -2 & -4 \\ 0 & -2 \\ 0 & 1
            \end{pmatrix} \xrightarrow[]{R_3 + \frac12R_2}
            \begin{pmatrix}[c|c]
                -2 & -4 \\ 0 & -2 \\ 0 & 0
            \end{pmatrix}
        \end{equation*}
        Dato che ci sono tanti pivot quante colonne segue che tutti i vettori originali sono indipendenti.
        I vettori $\vec{\tilde{w}_1}$ e $\vec{\tilde{w}_2}$ sono quindi indipendenti e generano $\widetilde{W}$: segue che $\basis{\vec{\tilde{w}_1}, \vec{\tilde{w}_1}}$ e' una base di $\widetilde{W}$, quindi $\dim \widetilde{W} = 2$.

        Tornando allo spazio originale, i vettori corrispondenti alla base sono quindi $w_1(x) = (-2 + x)$ e $w_2(x) = (-4 + x^2)$. L'insieme ordinato $\basis{(-2+x), (-4+x^2)}$ forma dunque una base di $W$ e dunque $\dim W = 2$.
    \end{enumerate}
\end{solution}

\section{Matrice associata ad una funzione}

Come avevamo visto nel primo capitolo, le matrici sono associate ad applicazioni lineari da vettori colonna in vettori colonna. Possiamo generalizzare questo concetto e definire una matrice associata ad ogni applicazione lineare.

\begin{definition}[Matrice associata ad un'applicazione]\label{matrice_associata}
    Siano $V, W$ spazi vettoriali, $f : V \to W$ lineare, $\alpha$ base di $V$ e $\beta$ base di $W$. Allora si dice chiama \textbf{matrice associata all'applicazione lineare} $f$ la matrice $[f]^{\alpha}_{\beta}$ tale che
    \begin{equation}
        \forall \vec{v} \in V. \quad  [f]^{\alpha}_{\beta} \cdot (\vec{v})_{\alpha}= \left(f(\vec{v}) \right)_{\beta}.
    \end{equation}
    Ovvero se $f$ mappa $\vec{v}$ a $\vec{w}$ allora $[f]^{\alpha}_{\beta}$ e' una matrice che porta (tramite il prodotto) il vettore colonna delle coordinate di $\vec{v}$ rispetto ad una base $\alpha$ nel vettore colonna delle coordinate di $\vec{w}$ rispetto ad una base $\beta$.
\end{definition}

Per trovare la matrice associata ad $f$ rispetto alle basi $\alpha = \basis{\vec{v_1}, \dots, \vec{v_n}}$ e $\beta = \basis{\vec{w_1}, \dots, \vec{w_m}}$ possiamo seguire questo procedimento:
\begin{itemize}
    \item calcoliamo $f(\vec{v_1}) = \vec{u_1}, \dots, f(\vec{v_n}) = \vec{u_n}$;
    \item scriviamo $\vec{u_i}$ in termini della base $\beta$, cioe' \begin{equation*}
        \vec{u_i} = a_{1i}\vec{w_1} + \dots + a_{mi}\vec{w_m} \iff [\vec{u_i}]_{\beta} = \begin{pmatrix}
            a_{1i} \\ \vdots \\ a_{mi}
        \end{pmatrix};
    \end{equation*}
    \item notiamo che dato che $\vec{v_i}$ e' l'$i$-esimo vettore della base $\alpha$, allora la sua rappresentazione in termini della base sara' un vettore colonna con tutti $0$ tranne un $1$ in posizione $i$;
    \item per la proposizione \ref{j-esima_colonna} il risultato del prodotto $[f]^{\alpha}_{\beta} \cdot (\vec{v_i})_{\alpha}$ sara' l'$i$-esima colonna della matrice $[f]^{\alpha}_{\beta}$, ma $[f]^{\alpha}_{\beta} \cdot (\vec{v_i})_{\alpha}$ deve essere uguale a $[f(v_i)]_{\beta} = [\vec{u_i}]_{\beta}$, dunque l'$i$-esima colonna di $[f]^{\alpha}_{\beta}$ sara' data dal vettore colonna $[\vec{u_i}]_{\beta}$;
    \item dunque la matrice avra' per colonne i vettori $[\vec{u_1}]_{\beta}, \dots, [\vec{u_n}]_{\beta}$, cioe'
    \begin{equation}
        [f]^{\alpha}_{\beta} = \begin{pmatrix}
            a_{11} & a_{12} & \dots & a_{1n} \\
            a_{21} & a_{22} & \dots & a_{2n} \\
            \vdots & \vdots & \vdots& \vdots \\
            a_{m1} & a_{m2} & \dots & a_{mn} \\
        \end{pmatrix}.
    \end{equation}
\end{itemize}


\begin{example}
    Sia $V = \M_{2\times 2}(\R)$ e sia $\alpha = \basis{\begin{psmallmatrix}1&0\\0&0\end{psmallmatrix}, \begin{psmallmatrix}0&1\\0&0\end{psmallmatrix}, \begin{psmallmatrix}0&0\\1&0\end{psmallmatrix}, \begin{psmallmatrix}0&0\\0&1\end{psmallmatrix}}$ una sua base.    
    Sia $A = \begin{psmallmatrix}0&1\\1&0\end{psmallmatrix} \in V$ e $f : V \to V$ tale che $f(B) = AB - BA$.
    \begin{enumerate}
        \item Dimostrare che $f$ e' lineare.
        \item Calcolare $[f]^{\alpha}_{\alpha}$.
        \item Dare una base di $\Imm{f}$.
        \item Dare una base di $\ker f$.
    \end{enumerate}
\end{example}
\begin{solution}
    Verifichiamo i quattro punti.
    \begin{enumerate}
        \item Dimostriamo che $f$ e' lineare.
            \begin{alignat*}{2}
                &\text{(a) } f(\vec{0}) = f\left(\begin{psmallmatrix}0&0\\0&0\end{psmallmatrix}\right) = A\begin{psmallmatrix}0&0\\0&0\end{psmallmatrix} - \begin{psmallmatrix}0&0\\0&0\end{psmallmatrix}A = \begin{psmallmatrix}0&0\\0&0\end{psmallmatrix} - \begin{psmallmatrix}0&0\\0&0\end{psmallmatrix} = \vec 0 \\
                &\begin{alignedat}{1}
                    \text{(b) } f(B + C) &= A(B + C) - (B + C)A \\
                    &= AB + AC - BC - CA \\
                    &= (AB - BA) + (AC - CA) \\
                    &= f(B) + f(C)
                \end{alignedat}\\
                &\begin{alignedat}{1}
                    \text{(c) } f(kB) &= A(kB) - (kB)A \\
                    &= k(AB) - k(BA) \\
                    &= k(AB - BA) \\
                    &= kf(B)
                \end{alignedat}
            \end{alignat*}
            dunque $f$ e' lineare.
        \item Seguo il procedimento per ottenere $[f]^{\alpha}_{\alpha}$. Innanzitutto calcolo il risultato di $f$ sui vettori della base $\alpha$:
        \begin{alignat*}{1}
            &f\left(\begin{psmallmatrix}1&0\\0&0\end{psmallmatrix}\right) = 
            \begin{psmallmatrix}0&1\\1&0\end{psmallmatrix}\begin{psmallmatrix}1&0\\0&0\end{psmallmatrix} - \begin{psmallmatrix}1&0\\0&0\end{psmallmatrix}\begin{psmallmatrix}0&1\\1&0\end{psmallmatrix} = \begin{psmallmatrix}0&0\\1&0\end{psmallmatrix} - \begin{psmallmatrix}0&1\\0&0\end{psmallmatrix} = \begin{psmallmatrix}0&-1\\1&0\end{psmallmatrix} \\
            &f\left(\begin{psmallmatrix}0&1\\0&0\end{psmallmatrix}\right) = 
            \begin{psmallmatrix}0&1\\1&0\end{psmallmatrix}\begin{psmallmatrix}0&1\\0&0\end{psmallmatrix} - \begin{psmallmatrix}0&1\\0&0\end{psmallmatrix}\begin{psmallmatrix}0&1\\1&0\end{psmallmatrix} = \begin{psmallmatrix}0&0\\0&1\end{psmallmatrix} - \begin{psmallmatrix}1&0\\0&0\end{psmallmatrix} = \begin{psmallmatrix}-1&0\\0&1\end{psmallmatrix} \\
            &f\left(\begin{psmallmatrix}0&0\\1&0\end{psmallmatrix}\right) = 
            \begin{psmallmatrix}0&1\\1&0\end{psmallmatrix}\begin{psmallmatrix}0&0\\1&0\end{psmallmatrix} - \begin{psmallmatrix}0&0\\1&0\end{psmallmatrix}\begin{psmallmatrix}0&1\\1&0\end{psmallmatrix} = \begin{psmallmatrix}1&0\\0&0\end{psmallmatrix} - \begin{psmallmatrix}0&0\\0&1\end{psmallmatrix} = \begin{psmallmatrix}1&0\\0&-1\end{psmallmatrix} \\
            &f\left(\begin{psmallmatrix}0&0\\0&1\end{psmallmatrix}\right) = 
            \begin{psmallmatrix}0&1\\1&0\end{psmallmatrix}\begin{psmallmatrix}0&0\\0&1\end{psmallmatrix} - \begin{psmallmatrix}0&0\\0&1\end{psmallmatrix}\begin{psmallmatrix}0&1\\1&0\end{psmallmatrix} = \begin{psmallmatrix}0&1\\0&0\end{psmallmatrix} - \begin{psmallmatrix}0&0\\1&0\end{psmallmatrix} = \begin{psmallmatrix}0&1\\-1&0\end{psmallmatrix}
        \end{alignat*}
        dunque le loro coordinate rispetto alla base $\alpha$ sono
        \begin{align*}
            &[f(\vec{v_1})]_{\alpha} = \begin{pmatrix}
                0 \\ -1 \\ 1 \\ 0
            \end{pmatrix} &[f(\vec{v_2})]_{\alpha} = \begin{pmatrix}
                -1 \\ 0 \\ 0 \\ 1
            \end{pmatrix}
            \\&[f(\vec{v_3})]_{\alpha} = \begin{pmatrix}
                1 \\ 0 \\ 0 \\ -1
            \end{pmatrix} &[f(\vec{v_4})]_{\alpha} = \begin{pmatrix}
                0 \\ 1 \\ -1 \\ 0
            \end{pmatrix}
        \end{align*}
        cioe' \begin{equation*}
            [f]^{\alpha}_{\alpha} = \begin{pmatrix}
                0 & -1 & 1 & 0 \\ -1 & 0 & 0 & 1 \\
                1 & 0 & 0 & -1 \\ 0 & 1 & -1 & 0
            \end{pmatrix}.
        \end{equation*}
        \item Per la proposizione \ref{base_mappata_generatori_immagine} sappiamo che l'insieme
        $\{f(\vec{v_1}), f(\vec{v_2}), f(\vec{v_3}), f(\vec{v_4})\}$ e' un insieme di generatori dell'immagine della funzione. 
        Per eliminare i vettori indipendenti passiamo all'isomorfismo con $\R^4$ tramite la base di partenza $\alpha$. Chiamiamo $\widetilde{W}$ lo spazio isomorfo a $\Imm{f}$, allora notiamo che il ruolo di $f$ nel nuovo spazio e' dato dalla matrice $[f]^{\alpha}_{\alpha}$, dunque il corrispondente insieme di generatori di $\widetilde{W}$ sara' \begin{gather*}
            \left\{ [f]^{\alpha}_{\alpha}[v_1]_{\alpha}, [f]^{\alpha}_{\alpha}[v_2]_{\alpha}, [f]^{\alpha}_{\alpha}[v_3]_{\alpha}, [f]^{\alpha}_{\alpha}[v_4]_{\alpha}\right\} \\
            \intertext{che e' uguale a }
            \left\{\begin{psmallmatrix} 0 \\ -1 \\ 1 \\ 0 \end{psmallmatrix}, \begin{psmallmatrix} -1 \\ 0 \\ 0 \\ 1 \end{psmallmatrix}, \begin{psmallmatrix} 1 \\ 0 \\ 0 \\ -1 \end{psmallmatrix}, \begin{psmallmatrix} 0 \\ 1 \\ -1 \\ 0 \end{psmallmatrix}\right\}  
        \end{gather*}
        Semplifichiamolo tramite mosse di colonna: \begin{gather*}
            \begin{pmatrix}[c|c|c|c]
                0 & -1 & 1 & 0 \\ -1 & 0 & 0 & 1 \\
                1 & 0 & 0 & -1 \\ 0 & 1 & -1 & 0
            \end{pmatrix} \xrightarrow[R_2 + R_3]{R_1 + R_4}
            \begin{pmatrix}[c|c|c|c]
                0 & 0 & 1 & 0 \\ 0 & 0 & 0 & 1 \\
                0 & 0 & 0 & -1 \\ 0 & 0 & -1 & 0
            \end{pmatrix} \xrightarrow[]{}
            \begin{pmatrix}[c|c|c|c]
                1 & 0 & 0 & 0\\0 & 1 & 0 & 0 \\
                0 & -1 & 0 & 0\\ -1 & 0 & 0 & 0
            \end{pmatrix}
        \end{gather*}
        dunque i vettori $\begin{psmallmatrix} 1 \\ 0 \\ 0 \\ -1 \end{psmallmatrix}, \begin{psmallmatrix} 0 \\ 1 \\ -1 \\ 0 \end{psmallmatrix}$ sono indipendenti e generano $\widetilde{W}$, dunque sono una base di $\widetilde{W}$.

        Tornando allo spazio originale otteniamo che una base di $\Imm{f}$ e' data da \[
            \beta = \basis{\begin{pmatrix} 1 & 0 \\ 0 & -1 \end{pmatrix}, \begin{pmatrix} 0 & 1 \\ -1 & 0 \end{pmatrix}}.
        \] e dunque $\dim \Imm{f} = 2$.
        \item Per definizione di kernel \[
            \ker f = \left\{ \begin{psmallmatrix}x&y\\z&t \end{psmallmatrix}\in \M_{2\times 2}(\R) \mid f\left(\begin{psmallmatrix}x&y\\z&t \end{psmallmatrix}\right) = \begin{psmallmatrix}0&0\\0&0 \end{psmallmatrix}\right\}.
        \] Sia $\widetilde{V} \subseteq \R^4$ lo spazio isomorfo a $\ker f$ tramite l'isomorfismo dato dalle coordinate dei vettori rispetto alla base $\alpha$. Allora \begin{alignat*}{1}
            \widetilde{V} &= \left\{ \begin{pmatrix}x\\y\\z\\t \end{pmatrix}\in \R^4 \mid [f]^{\alpha}_{\alpha}\begin{pmatrix}x\\y\\z\\t \end{pmatrix} = \begin{pmatrix}0\\0\\0\\0 \end{pmatrix}\right\} \\
            &= \left\{ \begin{pmatrix}x\\y\\z\\t \end{pmatrix}\in \R^4 \mid \begin{pmatrix}
                0 & -1 & 1 & 0 \\ -1 & 0 & 0 & 1 \\
                1 & 0 & 0 & -1 \\ 0 & 1 & -1 & 0
            \end{pmatrix}\begin{pmatrix}x\\y\\z\\t \end{pmatrix} = \begin{pmatrix}0\\0\\0\\0 \end{pmatrix}\right\}
        \end{alignat*}
        Dunque $\widetilde{V}$ e' formato da tutti e solo i vettori $\vec{x} \in \R^4$ che sono soluzione del sistema lineare $[f]^{\alpha}_{\alpha}\vec{x} = \vec{0}$. Risolviamolo tramite eliminazione gaussiana:
        \begin{gather*}
            \begin{pmatrix}
                0  & -1 & 1  & 0  \\ 
                -1 & 0  & 0  & 1  \\
                1  & 0  & 0  & -1 \\ 
                0  & 1  & -1 & 0
            \end{pmatrix} \xrightarrow[]{scambio}
            \begin{pmatrix}
                1  & 0  & 0  & -1 \\
                0  & 1  & -1 & 0  \\
                0  & -1 & 1  & 0  \\ 
                -1 & 0  & 0  & 1
            \end{pmatrix} \xrightarrow[R_4 + R_1]{R_3 + R_2} \\
            \xrightarrow[R_4 + R_1]{R_3 + R_2} \begin{pmatrix}
                1  & 0  & 0  & -1 \\
                0  & 1  & -1 & 0  \\
                0  & 0  & 0  & 0  \\ 
                0  & 0  & 0  & 0
            \end{pmatrix} \iff \left\{
                \begin{array}{@{}roror }
                    x & - & t & = & 0 \\
                    y & - & z & = & 0
                \end{array}
            \right. \iff \left\{
                \begin{array}{@{}ror }
                    x & = & t\\
                    y & = & z
                \end{array}
            \right.\\
            \intertext{dunque scegliendo $z, t \in \R$ libere otteniamo}
            \iff \begin{pmatrix} x \\ y \\ z \\t \end{pmatrix} = \begin{pmatrix} t \\ z \\ z \\ t \end{pmatrix} = z\begin{pmatrix} 0 \\ 1 \\ 1 \\ 0 \end{pmatrix} + t\begin{pmatrix} 1 \\ 0 \\ 0 \\ 1 \end{pmatrix}
        \end{gather*}
        Dunque $\tilde{\gamma} = \basis{\begin{psmallmatrix} 0 \\ 1 \\ 1 \\ 0 \end{psmallmatrix}, \begin{psmallmatrix} 1 \\ 0 \\ 0 \\ 1 \end{psmallmatrix}}$ e' un insieme di generatori di $\widetilde{V}$. Inoltre sono anche indipendenti (poiche' hanno pivot ad altezze diverse), dunque $\tilde{\gamma}$ e' una base di $\widetilde{V}$. 
            
        Tornando tramite la base $\alpha$ allo spazio iniziale otteniamo che \[
            \gamma = \basis{\begin{pmatrix} 0 & 1 \\ 1 & 0 \end{pmatrix}, \begin{pmatrix} 1 & 0 \\ 0 & 1 \end{pmatrix}}  
        \] e' una base di $\ker f$, dunque $\dim \ker f = 2$.
    \end{enumerate}
\end{solution}

\subsection{Inversa di una matrice}

\begin{definition}[Matrice inversa]
    Sia $A \in \M_{n \times n}(\R)$ una matrice quadrata. Allora se $A$ e' invertibile si dice che la sua inversa e' la matrice $A^{-1}$ tale che \[
        AA^{-1} = A^{-1}A = I_n.
    \]
\end{definition}

Una matrice e' invertibile se e solo se la sua applicazione lineare associata e' invertibile, ovvero se e solo se e' bigettiva.

\begin{proposition}
    Siano $V, W$ spazi vettoriali e sia $f : V \to W$. Allora vale che \[
        [f]^{-1} = [f^{-1}]    
    \] ovvero l'inversa della matrice associata ad $f$ e' la matrice associata all'inversa di $f$.
\end{proposition}

Possiamo sfruttare le inverse per risolvere sistemi lineari: \[
    A\vec x = \vec b \iff A^{-1}A\vec x = A^{-1}\vec b \iff \vec x = A^{-1}\vec b.
\]

\subsubsection{Metodo di Gauss per l'inversa}
Possiamo trovare la matrice inversa di una matrice $A \in  \M_{n \times n}(\R)$ in questo modo. 

Consideriamo la matrice $n \times 2n$ formata affiancando alla matrice originale la matrice identita' $n \times n$, e indichiamola con $[A | I_n]$. Tramite mosse di riga riduciamo questa matrice alla forma $[I_n | B]$: allora $B = A^{-1}$.

Questa strategia funziona perche' risolvere il sistema $[A | I_n]$ tramite mosse di Gauss-Jordan e' equivalente a cercare una soluzione della seguente equazione matriciale: \[
    A\begin{pmatrix}
        b_{11} & \dots & b_{1n} \\
        \vdots & \vdots & \vdots \\
        b_{n1} & \dots & b_{nn} \\
    \end{pmatrix} = I_n.
\] Dato che la matrice inversa e' unica, allora la matrice $B$ ottenuta in questo modo dovra' essere l'inversa di $A$.

\begin{proposition}[Inversa del prodotto]
    \label{inversa_prodotto}
    Siano $A, B$ tali che $AB$ e' invertibile. Allora vale che $(AB)^{-1} = B^{-1}A^{-1}$.
\end{proposition}
\begin{proof}
    Per definizione di inversa deve valere che \begin{alignat*}
        {1}
        (AB)^{-1}(AB) = I_n \\
        \iff (AB)^{-1}AB = I_n \\
        \iff (AB)^{-1}ABB^{-1} = B^{-1}\\
        \iff (AB)^{-1}A = B^{-1}\\
        \iff (AB)^{-1}AA^{-1} = B^{-1}A^{-1}\\
        \iff (AB)^{-1} = B^{-1}A^{-1}
    \end{alignat*} 
    che e' la tesi.
\end{proof}

\begin{example}
    Trovare l'inversa di \[
        A = \begin{pmatrix}
            2  & -1 & 0 \\
            -1 & 2  & -1 \\
            0  & -1 & 2 \\
        \end{pmatrix}. 
    \]
\end{example}
\begin{solution}
    Usiamo il metodo di Gauss.
    \begin{gather*}
        \begin{pmatrix}[ccc|ccc]
            2  & -1 & 0  & 1 & 0 & 0\\
            -1 & 2  & -1 & 0 & 1 & 0\\
            0  & -1 & 2  & 0 & 0 & 1\\
        \end{pmatrix} \xrightarrow[]{R_2 + \frac12 R_1}
        \begin{pmatrix}[ccc|ccc]
            2 & -1      & 0  & 1        & 0 & 0\\
            0 & \nicefrac32 & -1 & \nicefrac12  & 1 & 0\\
            0 & -1      & 2  & 0        & 0 & 1\\
        \end{pmatrix} \xrightarrow[]{R_3 + \frac23 R_2} \\
        \begin{pmatrix}[ccc|ccc]
            2 & -1      & 0         & 1        & 0       & 0\\
            0 & \nicefrac32 & -1        & \nicefrac12  & 1       & 0\\
            0 & 0       & \nicefrac43   & \nicefrac13  & \nicefrac23 & 1\\
        \end{pmatrix} \xrightarrow[]{R_2 + \frac34 R_3}
        \begin{pmatrix}[ccc|ccc]
            2 & -1      & 0         & 1        & 0       & 0\\
            0 & \nicefrac32 & 0         & \nicefrac34  & \nicefrac32 & \nicefrac34\\
            0 & 0       & \nicefrac43   & \nicefrac13  & \nicefrac23 & 1\\
        \end{pmatrix} \xrightarrow[]{R_1 + \frac23 R_2}\\
        \begin{pmatrix}[ccc|ccc]
            2 & 0       & 0         & \nicefrac32  & 1       & \nicefrac12\\
            0 & \nicefrac32 & 0         & \nicefrac34  & \nicefrac32 & \nicefrac34\\
            0 & 0       & \nicefrac43   & \nicefrac13  & \nicefrac23 & 1\\
        \end{pmatrix} \xrightarrow[\frac34 \times R_3]{\frac12 \times R_1, \nicefrac23 \times R_2}
        \begin{pmatrix}[ccc|ccc]
            1 & 0 & 0 & \nicefrac34 & \nicefrac12 & \nicefrac14\\
            0 & 1 & 0 & \nicefrac12 & 1       & \nicefrac12\\
            0 & 0 & 1 & \nicefrac14 & \nicefrac12 & \nicefrac34 \\
        \end{pmatrix} \\
        \implies A^{-1} = \begin{pmatrix}
            \nicefrac34 & \nicefrac12 & \nicefrac14\\
            \nicefrac12 & 1       & \nicefrac12\\
            \nicefrac14 & \nicefrac12 & \nicefrac34 \\
        \end{pmatrix}
    \end{gather*}
\end{solution}

\subsubsection{Inversa di una matrice $2 \times 2$}

Per trovare l'inversa di una matrice $2 \times 2$ possiamo usare questa formula:
\[
    A = \begin{pmatrix}
        a & b \\
        c & d \\
    \end{pmatrix} \implies 
    A^{-1} = \frac{1}{ad-bc}\begin{pmatrix}
        d  & -b \\
        -c & a \\
    \end{pmatrix}.
\] Il numero $ad - bc$ e' il determinante della matrice $2 \times 2$ e analizzeremo il suo significato in un capitolo successivo.

Ad esempio \[
    A = A = \begin{pmatrix}
        3 & 2 \\
        4 & 7 \\
    \end{pmatrix} \implies 
    A^{-1} = \frac{1}{3 \cdot 7 - 2 \cdot 4}\begin{pmatrix}
        7  & -2 \\
        -4 & 3 \\
    \end{pmatrix} = \frac{1}{13}\begin{pmatrix}
        7  & -2 \\
        -4 & 3 \\
    \end{pmatrix}.    
\]

\subsection{Composizione di funzioni come moltiplicazione tra matrici}

Consideriamo tre spazi vettoriali $U, V, W$ e due funzioni $f, g$ tali che \[
    U \xmapsto{f} V \xmapsto{g} W  \quad  \iff  \quad  U \xmapsto{g \circ f} W
\] dunque per ogni $u \in U$ segue che $(g \circ f)(u) = g(f(u))$. 

Consideriamo ora $\alpha$ base di $U$, $\beta$ base di $V$ e $\gamma$ base di $W$ e cerchiamo di rappresentare la relazione tra gli insiemi tramite le matrici associate alle funzioni: \[
    U_{\alpha} \xmapsto{[f]^{\alpha}_{\beta}} V_{\beta} \xmapsto{[g]^{\beta}_{\gamma}} W_{\gamma}  \quad  \iff  \quad  U_{\alpha} \xmapsto{[g \circ f]^{\alpha}_{\gamma}} W_{\gamma}.
\] Dunque per lo stesso ragionamento di prima dovra' valere che \[
    [g \circ f]^{\alpha}_{\gamma}\cdot (u)_{\alpha} = [g]^{\beta}_{\gamma} \cdot \left( [f]^{\alpha}_{\beta} \cdot (u)_{\alpha} \right).
\]

\begin{theorem}[Composizione come prodotto tra matrici]
    Siano $U,\ V,\ W$ spazi vettoriali e siano $\alpha$, $\beta$ e $\gamma$ delle basi rispettivamente di $U,\ V$ e $W$. Siano $f : U \to V$, $g : V \to W$ e siano $[f]{\alpha}_{\beta}$ e $[g]^{\beta}_{\gamma}$ le matrici associate a $f$ e $g$.

    Allora vale che \begin{equation}
        [g \circ f]^{\alpha}_{\gamma} = [g]^{\beta}_{\gamma} \cdot [f]^{\alpha}_{\beta}.
    \end{equation}
\end{theorem}
\begin{proof}
    Sia $u \in U$ generico e dimostriamo che $[g \circ f]^{\alpha}_{\gamma} \cdot (u)_{\alpha} = [g]^{\beta}_{\gamma} \cdot [f]^{\alpha}_{\beta} \cdot (u)_{\alpha}$, ricordando la definizione di matrice associata ad una funzione (\ref{matrice_associata}).
    \begin{align*}
        [g]^{\beta}_{\gamma} \cdot [f]^{\alpha}_{\beta} \cdot (u)_{\alpha} &= [g]^{\beta}_{\gamma} \cdot ([f]^{\alpha}_{\beta} \cdot (u)_{\alpha}) \\
        &= [g]^{\beta}_{\gamma} \cdot (f(u))_{\beta} &&\text{per def. di matrice associata ad $f$}\\
        &= (g(f(u)))_{\gamma} &&\text{per def. di matrice associata a $g$}\\
        &= ((g \circ f)(u))_{\gamma}\\
        &= [g \circ f]^{\alpha}_{\gamma} \cdot (u)_{\alpha} &&\text{per def. di matrice associata a $g \circ f$.}
    \end{align*}
    Dunque dato che la relazione vale per qualunque $u \in U$, allora segue che \[
        [g \circ f]^{\alpha}_{\gamma} = [g]^{\beta}_{\gamma} \cdot [f]^{\alpha}_{\beta}    
    \] che e' la tesi.
\end{proof}

\subsection{Matrice del cambiamento di base}

\begin{definition}[Funzione identita']
    Sia $V$ uno spazio vettoriale. Allora si dice funzione identita' la funzione $\id_V : V \to V$ tale che \[
        \forall v \in V. \quad \id_V(v) = v.
    \]
\end{definition}

Scriveremo sempre $\id$ sottointendendo lo spazio di riferimento.

Possiamo costruire una matrice che trasforma le coordinate di un vettore rispetto ad una base nelle coordinate di un vettore rispetto ad un'altra base sfruttando la funzione identita'.

\begin{definition}[Matrice del cambiamento di base]
    Sia $V$ uno spazio vettoriale, $\alpha$ e $\beta$ basi di $V$. 
    
    Allora dato un vettore $v \in V$ la matrice $[\id]^{\alpha}_{\beta}$ porta le coordinate di $v$ rispetto ad $\alpha$ nelle coordinate di $v$ rispetto a $\beta$, ovvero \begin{equation}
        [\id]^{\alpha}_{\beta} \cdot (v)_{\alpha} = (v)_{\beta}.
    \end{equation}
    $[\id]^{\alpha}_{\beta}$ si dice \textbf{matrice del cambiamento di base}.
\end{definition}

\begin{example}
    Sia $\id : \R^2 \to \R^2$, sia $\mathcal{C}$ la base canonica di $\R^2$ e sia \[
        \alpha = \basis{\begin{pmatrix} 1 \\ 3 \end{pmatrix}, \begin{pmatrix} 2 \\ 1 \end{pmatrix}} = \basis{\vec{\alpha_1}, \vec{\alpha_2}}
    \] un'altra base di $\R^2$. 
    
    Calcolare $[\id]^{\alpha}_{\mathcal{C}}$ e $[\id]^{\mathcal{C}}_{\alpha}$.
\end{example}
\begin{solution} Scriviamo \[
        [\id]^{\alpha}_{\mathcal{C}} = \begin{pmatrix} a_{11} & a_{12} \\ a_{21} & a_{22} \end{pmatrix}.
    \] Allora:
    \begin{gather*}
        \begin{pmatrix} a_{11} \\ a_{21} \end{pmatrix} = [\id]^{\alpha}_{\mathcal{C}} [\vec{\alpha_1}]_{\alpha} = [\id(\vec{\alpha_1})]_{\mathcal{C}}= [\vec{\alpha_1}]_{\mathcal{C}} = \vec{\alpha_1}  \\
        \begin{pmatrix} a_{12} \\ a_{22} \end{pmatrix} = [\id]^{\alpha}_{\mathcal{C}} [\vec{\alpha_2}]_{\alpha} = [\id(\vec{\alpha_2})]_{\mathcal{C}}= [\vec{\alpha_2}]_{\mathcal{C}} = \vec{\alpha_2}
    \end{gather*}
    dunque \[
        [\id]^{\alpha}_{\mathcal{C}} = \begin{pmatrix} 1 & 2 \\ 3 & 1 \end{pmatrix}.
    \] Per calcolare la matrice che porta i vettori dalla base $\mathcal{C}$ alla base $\alpha$ basta notare che essa deve invertire il comportamento di $[\id]^{\alpha}_{\mathcal{C}}$, ovvero \[
        [\id]^{\mathcal{C}}_{\alpha} = ([\id]^{\alpha}_{\mathcal{C}})^{-1} = \frac{1}{1\cdot 1 - 2 \cdot 3} \begin{pmatrix} 1 & -2 \\ -3 & 1 \end{pmatrix} = -\frac{1}{5} \begin{pmatrix} 1 & -2 \\ -3 & 1 \end{pmatrix}.
    \]
\end{solution}

Le prossime proposizioni aiutano a scrivere piu' semplicemente le matrici di cambiamento di base e le matrici associate ad applicazioni lineari.

\begin{proposition}
    Sia $\alpha = \basis{\vec{\alpha_1}, \dots, \vec{\alpha_n}}$ una base di $\R^n$ e $\mathcal{C}$ la base canonica di $\R^n$. Allora la matrice di cambiamento di base $[\id]^{\alpha}_{\mathcal{C}}$ e' la matrice che ha come colonne $\vec{\alpha_1}, \dots, \vec{\alpha_n}$.
\end{proposition}
\begin{proof}
    Consideriamo $[\id]^{\alpha}_{\mathcal{C}} [\vec{\alpha_i}]_{\alpha}$. Allora \begin{alignat*}
        {1}
        [\id]^{\alpha}_{\mathcal{C}} [\vec{\alpha_i}]_{\alpha} &= [\id(\vec{\alpha_i})]_{\mathcal{C}} \\
        &= [\vec{\alpha_i}]_{\mathcal{C}}\\
        &= \vec{\alpha_i}
    \end{alignat*} dato che rispetto alle basi canoniche le coordinate del vettore $\vec{\alpha_i}$ e' proprio il vettore stesso.
    
    Cerchiamo ora di ricavare $[\vec{\alpha_i}]_{\alpha}$. Il vettore $\vec{\alpha_i}$ rispetto alla base $\alpha$ si puo' scrivere come \begin{alignat*}
        {1}
        &\vec{\alpha_i} = 0\vec{\alpha_1} + \dots + 1\vec{\alpha_i} + \dots + 0\vec{\alpha_n} \\
        \implies &[\vec{\alpha_i}]_{\alpha} = (a_{j1}) = \begin{cases}
            1 & \text{ se } j = i\\
            0 & \text{ altrimenti}\\
        \end{cases}
    \end{alignat*}
    cioe' il vettore $[\vec{\alpha_i}]_{\alpha}$ e' un vettore colonna con tutti zeri tranne un uno in posizione $i$.

    Dunque per la proposizione \ref{j-esima_colonna} il prodotto $[\id]^{\alpha}_{\mathcal{C}} [\vec{\alpha_i}]_{\alpha}$ deve dare la $i$-esima colonna della matrice $[\id]^{\alpha}_{\mathcal{C}}$, che deve essere quindi $\vec{\alpha_i}$.

    Dato che questo ragionamento vale per ogni $i$, la matrice di cambiamento di base dalla base $\alpha$ alla canonica ha come colonne i vettori $\vec{\alpha_1}, \dots, \vec{\alpha_n}$.
\end{proof}
