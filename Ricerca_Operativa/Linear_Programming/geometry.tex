\chapter{Programmazione Lineare}

\section{Geometria della PL}

Ricordiamo che un problema di ottimizzazione si dice di \emph{programmazione lineare} se:
\begin{enumerate}[label={(\arabic*)}]
    \item la funzione obiettivo $f : \R^n \to \R$ è \emph{lineare}, ovvero per ogni $\vec x, \vec y \in \R^n$ vale che \[
        f(\alpha\vec x + \beta\vec y) = \alpha f(\vec x) + \beta f(\vec y).
    \] Equivalentemente ogni funzione lineare è esprimibile nella forma \[
        f(\vec x) = \vec c \cdot \vec x = {\vec c}\trans\vec x = \sum_{j = 1}^n c_jx_j,    
    \] dove $\vec c \in \R^n$ è il \emph{vettore dei costi}.
    \item l'insieme ammissibile è definito da un numero finito di \emph{vincoli lineari}, ovvero della forma \begin{align*}
        \vec{a_i} \cdot \vec x \leq b_i, &\text{oppure} &\vec{a_i} \cdot \vec x = b_i, &\text{oppure} &\vec{a_i} \cdot \vec x \geq b_i    
    \end{align*} con $\vec{a_i} \in \R^n$, $b_i \in \R$. 
\end{enumerate}

Sia $m$ il numero di vincoli lineari: possiamo scrivere il problema di programmazione lineare in una forma più compatta, detta \emph{forma standard}: \begin{equation}
    \max \set{\vec c \cdot \vec x \suchthat A\vec x \leq \vec b} \tag{($\PP$)}
\end{equation} dove $A \in \Mat{m}{n}{\R}$ è la matrice che ha come $j$-esima riga il vettore $\vec{a_j}$, e il vettore $\vec b \in \R^m$ è il vettore che ha come $j$-esima coordinata il numero $b_j$.

Ogni problema di PL può essere ricondotto alla forma standard tramite le seguenti trasformazioni:
\begin{align*}
    % &\text{(1):} 
    &\max \sum_{j = 1}^n c_jx_j &\equiv &{}-\min \sum_{j = 1}^n (-c_j)x_j \\
    % &\text{(2):} 
    &\sum_{j = 1}^n a_{ij}x_j \geq b_j &\equiv &{}\sum_{j = 1}^n (-a_{ij})x_j \leq -b_j\\
    % &\text{(3):} 
    &\sum_{j = 1}^n a_{ij}x_j = b_j &\equiv &{} \sum_{j = 1}^n a_{ij}x_j \leq b_j, \; \sum_{j = 1}^n a_{ij}x_j \geq b_j  
\end{align*}

Iniziamo ora a studiare la geometria di un problema di programmazione lineare.

\begin{definition}
    [Iperpiano e semispazi]
    Sia $V$ uno spazio vettoriale su $\R$ di dimensione $n$. Un suo sottospazio (vettoriale o affine) di dimensione $n-1$ si dice \emph{iperpiano}.

    Se lo spazio vettoriale è $\R^n$, un iperpiano (affine) è della forma \begin{equation}
        \label{eq:iperpiano} P_i \deq \set{\vec x \in \R^n \suchthat A_i\vec x = b_i}
    \end{equation} dove $A_i, \vec x \in \R^n$, $b_i \in \R$.

    Ogni iperpiano in $\R^n$ delimita due \emph{semispazi} della forma \begin{equation}
        \label{eq:semispazi} S_i \deq \set{\vec x \in \R^n \suchthat A_i\vec x \leq b_i}, \quad S_i^\prime \deq \set{\vec x \in \R^n \suchthat A_i\vec x \geq b_i}.
    \end{equation}
\end{definition}

I vincoli di un problema di PL sono definiti come \emph{intersezione di semispazi}: infatti ogni vincolo rappresenta un semispazio e la regione ammissibile è l'intersezione di tutti i semispazi definiti dai vincoli.

\begin{definition}
    [Poliedri e politopi]
    Sia $P \subseteq \R^n$. $P$ si dice \emph{poliedro} se è l'intersezione di un numero finito di semispazi di $R^n$, ovvero \begin{equation}
        P \deq \set{\vec x \in \R^n \suchthat A_i\vec x \leq b_i \;\forall i = 1, \dots, m}.
    \end{equation}

    In particolare se $P$ è limitato $P$ si dice \emph{politopo}.
\end{definition}

Possiamo rendere più compatta la scrittura di un poliedro condensando i vincoli in un'unica matrice. Un poliedro $P$ è quindi definito da \[
    P \deq \set{\vec x \in \R^n \suchthat A\vec x \leq \vec b}, 
\] dove $A \in \Mat{m}{n}{\R}$ è la matrice che ha per righe i vettori $A_1, \dots, A_m$ e $\vec b \in \R^m$ è il vettore che ha per componenti i vincoli $b_1, \dots, b_m$.

Un'altra caratteristica importante dei poliedri è la loro convessità.

\begin{definition}
    [Insieme convesso]
    Sia $C \subseteq \R^n$: $C$ si dice \emph{convesso} se per ogni $\vec x, \vec y \in \C$ il segmento che congiunge $\vec x$ e $\vec y$ è tutto contenuto in $C$, ovvero per ogni $\alpha \in \closedInt*{0, 1}$ vale che \[
        \alpha\vec x + (1 - \alpha)\vec y \in C.    
    \]
\end{definition}

La definizione di insieme convesso ci dice che ogni punto del segmento che congiunge $\vec x$ e $\vec y$ è in $C$: \begin{align*}
    \alpha\vec x + (1 - \alpha)\vec y = \vec y + \alpha(\vec x - \vec y) \in C.
\end{align*} Ma il vettore $\vec x - \vec y$ è il vettore che congiunge i due punti, quindi scalandolo di un fattore $\alpha \in \closedInt*{0, 1}$ otteniamo tutti i punti nel segmento.

Si può dimostrare che i semispazi sono insiemi convessi e che l'intersezione finita di semispazi è ancora convessa, da cui segue che ogni poliedro è convesso.

\paragraph{Facce e vertici} Consideriamo un poliedro $P$ tale che \[
    P = \set{\vec x \in \R^n \suchthat A\vec x \leq \vec b}    
\] e consideriamo un sottoinsieme dei suoi indici di riga, ovvero un insieme $I \subseteq \set{1, \dots, m}$.

Indichiamo con $A_I$ la sottomatrice che si ottiene da $A$ considerando solo le righe il cui indice è contenuto in $I$ (ad esempio se $I = \set{1, 3}$ la matrice $A_I$ è formata dalla prima e dalla terza riga della matrice $A$) e con $\vec b_I$ il sottovettore ottenuto considerando solo le componenti i cui indici sono in $I$. Infine sia $\bar I$ il complementare di $I$, ovvero \[
    \bar I = \set{1, \dots, m} \setminus I.    
\]

La regione definita da \[
    P_I \deq \set{\vec x \in \R^n \suchthat A_I}    
\] è ancora un poliedro ed è contenuta in $P$: è l'insieme di tutti i punti di $P$ che soddisfano strettamente (ovvero come uguaglianze) i vincoli il cui indice è in $I$. Se $P_I \neq \varnothing$ questo insieme si dice \emph{faccia} di $P$.

Osserviamo che in particolare anche il poliedro stesso è una sua faccia (basta scegliere $I = \varnothing$); le facce diverse dal poliedro si dicono \emph{facce proprie}.

Un tipo particolare di faccia è quella identificata da un insieme di indici $I$ tali che la sottomatrice $A_I$ abbia rango $n$: il sistema $A_I\vec x = \vec b_I$ ha una e una sola soluzione (chiamiamola $\vecbar{x}$), dunque la faccia $P_I$ è formata dal singolo punto $\vecbar x$: esso si dice \emph{vertice del poliedro}.

\begin{definition}
    [Vertice di un poliedro]
    Sia $P \deq \set{\vec x \in \R^n \suchthat A\vec x \leq \vec b}$ un poliedro e sia $I \subseteq \set{1, \dots, m}$ tale che la sottomatrice $A_I$ abbia rango $n$.

    Allora il punto $\vecbar x$ tale che \[
        A_I\vec x = \vec b_I   
    \] si dice \emph{vertice del poliedro}.
\end{definition}

\paragraph{Basi} Siccome per avere un vertice dobbiamo trovare una sottomatrice di rango $n$, possiamo cercare una famiglia $B$ formata da $n$ indici di riga tale che $A_B$ sia invertibile. Tale sottoinsieme di indici viene detto \emph{base} e la sottomatrice $A_B$ si dice \emph{sottomatrice di base}.

Siccome $A_B$ è invertibile vi è un unico punto che soddisfa $A_B\vec x = \vec b_B$: tale punto è detto \emph{soluzione di base} ed è definito da \begin{equation*}
    \vecbar x \deq A_B\inv \vec b_B.    
\end{equation*}

Se $\vecbar x \in P$, ovvero $\vecbar x$ soddisfa anche tutti gli altri vincoli, ovvero \[
    A_i\vecbar x \leq b_i \qquad \forall i \notin B    
\] allora la soluzione di base $\vecbar x$ si dice \emph{ammissibile}, altrimenti $\vecbar x$ è una soluzione di base \emph{non ammissibile}.

Le soluzioni di base sono importanti nello studio di problemi di PL in quanto si può dimostrare che i vertici di un poliedro sono tutte e sole le soluzioni di base ammissibili.

Se la matrice $A$ associata al poliedro $P$ ha rango minore di $n$ il poliedro non ha vertici. Tuttavia per studiare un problema in cui il poliedro non ha vertici ci si può sempre ricondurre ad un problema in cui il poliedro ha almeno un vertice, dunque d'ora in avanti assumeremo che la matrice $A$ abbia rango maggiore o uguale ad $n$.

\paragraph{Indici attivi} Consideriamo infine un punto $\vecbar{x} \in P$: siccome è un punto del poliedro $P$ allora dovranno valere tutti i vincoli 
\[
    A_i\vecbar{x} \leq b_i    
\] per $i = 1, \dots, m$. Alcuni di questi vincoli possono essere soddisfatti come uguaglianze, ovvero per qualche $i \in \set{1, \dots, m}$ può valere \[
    A_i\vecbar{x} = b_i.    
\] Possiamo quindi definire l'insieme degli indici attivi in $\vecbar{x}$:

\begin{definition}
    [Indici attivi]
    Sia $P = \set{\vec x \in \R^n \suchthat A\vec x \leq \vec b}$ un poliedro e sia $\vecbar{x} \in P$. Si dice \emph{insieme degli indici attivi in $\vec{\bar{x}}$} l'insieme \begin{equation}
        I(\vecbar{x}) = \set{i \in \set{1, \dots, m} \suchthat A_i\vecbar{x} = b_i}.
    \end{equation}
\end{definition}

\section{Decomposizione di poliedri}

Abbiamo già mostrato che i poliedri sono insiemi convessi. Questa proprietà è di fondamentale importanza, in quanto ci permetterà di esprimere ogni poliedro in due forme: la forma basata sulle facce (ovvero sui vincoli) e la forma basata sui vertici del poliedro.

Diamo alcune definizioni iniziali.
\begin{definition}
    [Combinazione convessa]
    Sia $\vec x \in \R^n$. Si dice che $\vec x$ è \emph{combinazione convessa} di $\vec x^1, \dots, \vec x^k \in \R^n$ se esistono $\alpha_1, \dots, \alpha_k \geq 0$ tali che \begin{equation}
        \vec x = \alpha_1\vec x^1 + \dots + \alpha_k\vec x^k, \quad \text{con } \sum_{i = 1}^n \alpha_i = 1.
    \end{equation}
\end{definition}

Notiamo che siccome gli $\alpha_i$ sono tutti non negativi e la loro somma deve essere uguale a $1$ segue che \[
    \alpha_1, \dots, \alpha_n \in \closedInt*{0, 1}.    
\]

\begin{example}
    Il vettore $(2, 2)$ è combinazione convessa di $(4, 0)$ e $(1, 3)$. Infatti \[
        \begin{pmatrix} 2 \\ 2 \end{pmatrix} = \frac{1}{3}\begin{pmatrix} 4 \\ 0 \end{pmatrix} + \frac{2}{3}\begin{pmatrix} 1 \\ 3 \end{pmatrix}.
    \] La combinazione è convessa in quanto $\nicefrac{1}{3} + \nicefrac{2}{3} = 1$ e i coefficienti sono entrambi non negativi.
\end{example}

\begin{definition}
    [Inviluppo convesso]
    Sia $K \subseteq \R^n$ un insieme finito, ovvero \[
        K = \set{\vec x^1, \dots, \vec x^k}.
    \] 
    
    Si dice \emph{inviluppo convesso} di $K$ l'insieme di tutte le combinazioni convesse di punti di $K$, ovvero \begin{equation}
        \conv{K} \deq \set{\sum_{i = 0}^k \alpha_i\vec x^i \suchthat \alpha_1, \dots, \alpha_k \geq 0,\; \sum_{i=0}^k \alpha_i = 1}.
    \end{equation}
\end{definition}

In particolare si può dimostrare che l'inviluppo complesso di un insieme finito di punti $\conv{K}$ è il più piccolo insieme convesso che contiene tutti i punti di $K$: è quindi il politopo che ha come vertici i punti di $K$. L'inviluppo complesso ci consente quindi di rappresentare tutti i poliedri limitati; per rappresentare quelli illimitati abbiamo bisogno di altri strumenti.

\begin{definition}
    [Cono convesso] Un insieme $C \subseteq \R^n$ si dice \emph{cono} se per ogni $\vec x \in C$ segue che \begin{equation}
        \alpha\vec x \in C
    \end{equation} per ogni $\alpha \geq 0$.

    In particolare se $C$ è un insieme convesso vale che per ogni $\vec x, \vec y \in C$ \[
        \alpha\vec x + \beta\vec y \in C    
    \] per ogni $\alpha, \beta \geq 0$. In questo caso $C$ si dice \emph{cono convesso}.
\end{definition}

\begin{proposition}
    Sia $P$ un poliedro della forma \begin{equation}
        P \deq \set{\vec x \suchthat A\vec x \leq \vec 0}.
    \end{equation} Allora $P$ è un cono convesso.
\end{proposition}

In particolare un poliedro che è un cono convesso si dice \emph{cono poliedrico}.

\begin{definition}
    [Combinazioni coniche] Sia $\vec x \in \R^n$. $\vec x$ si dice \emph{combinazione conica} di $\vec x^1, \dots, \vec x^k \in \R^n$ se esistono $\alpha_1, \dots, \alpha_k \geq 0$ tali che \begin{equation}
        \vec x = \sum_{i = 1}^k \alpha_i\vec x^i.
    \end{equation}
\end{definition}

\begin{definition}
    [Inviluppo conico] Sia $K \subseteq \R^n$ un insieme finito, ovvero \[
        K = \set{\vec x^1, \dots, \vec x^k}.    
    \] Si dice \emph{inviluppo conico} di $K$ l'insieme di tutte le combinazioni coniche di punti di $K$, ovvero \begin{equation}
        \cono{K} \deq \set{\sum_{i = 0}^k \alpha_i\vec x^i \suchthat \alpha_1, \dots, \alpha_k \geq 0}.
    \end{equation}
\end{definition}

\subsection{Direzioni di recessione e linealità}

\begin{definition}
    [Direzione di recessione]
    Sia $P \deq \set{\vec x \in R^n \suchthat A\vec x \leq \vec b}$ un poliedro. $\vec d \in \R^n$ si dice \emph{direzione di recessione per $P$} se per ogni $\alpha \geq 0$ vale che \[
        \vec x + \alpha\vec d \in P.    
    \] L'insieme delle direzioni di recessione di un poliedro è indicata con $\rec(P)$.
\end{definition}

Intuitivamente un vettore $\vec d$ è di recessione se da qualunque punto $\vec x$ del poliedro $P$ muovendosi lungo la direzione e il verso di $\vec d$ si rimane sempre in $P$.

\begin{proposition}\label[prop]{prop:caratt_dir_recessione}
    Sia $P \deq \set{\vec x \in R^n \suchthat A\vec x \leq \vec b}$ un poliedro. Allora $\rec(P)$ è un cono poliedrico ed in particolare \begin{equation}
        \rec(P) = \set{\vec d \in R^n \suchthat A\vec d \leq \vec 0}
    \end{equation}
\end{proposition}
\begin{proof}
    Mostriamo che $\vec d$ è una direzione di recessione se e solo se $A\vec d \leq \vec 0$.
    \begin{description}
        \item[($\implies$)] Sia $\vec x \in P$ qualsiasi. Siccome $\vec d$ è di recessione, segue che $\vec x + \alpha\vec d \in P$ per ogni $\alpha \geq 0$, ovvero \[
            A(\vec x + \alpha\vec d) \leq \vec b.
        \]
        Tuttavia vale che \begin{align*}
            A(\vec x + \alpha\vec d) &= A\vec x + \alpha A\vec d \tag{$A\vec x \leq \vec b$}\\
            &\leq \vec b + \alpha A\vec d.
        \end{align*} 
        Questa espressione è minore di $\vec b$ se e solo se $\alpha A\vec d \leq \vec 0$, ovvero (siccome $\alpha \geq 0$) se e solo se $A\vec d \leq \vec 0$.
        \item{($\impliedby$)} Supponiamo $A\vec d \leq \vec 0$ e mostriamo che per qualsiasi $\vec x \in P$, $\alpha \geq 0$ vale che $\vec x + \alpha\vec d \in P$. Ciò significa che \[
            A(\vec x + \alpha\vec d) = A\vec x + \alpha\vec d \leq \vec b  
        \] da cui segue la tesi. \qedhere
    \end{description}
\end{proof}

\begin{remark}
    Per qualsiasi poliedro $P$ l'insieme $\rec P$ non è mai vuoto: infatti contiene sempre almeno lo zero. Tuttavia $\vec 0$ non è una direzione (se ci spostiamo lungo il vettore nullo non compiamo nessuno spostamento), dunque o $\rec P = \set{\vec 0}$ oppure esiste una direzione di recessione $\vec d \neq \vec 0$.

    Questo significa che un poliedro è limitato se e solo se $\rec P = \set{\vec 0}$.
\end{remark}

\begin{definition}
    [Direzione di linealità]
    Sia $P \deq \set{\vec x \in R^n \suchthat A\vec x \leq \vec b}$ un poliedro. $\vec d \in \R^n$ si dice \emph{direzione di linealità per $P$} se per ogni $\alpha \in \R$ vale che \[
        \vec x + \alpha\vec d \in P.    
    \] L'insieme delle direzioni di linealità di un poliedro è indicata con $\lineal P$.
\end{definition}

Osserviamo che una direzione $\vec d$ è di linealità se e solo se \[
    \vec d, -\vec d \in \rec P.    
\] \begin{proof} 
    Sia $\vec x \in P$ qualunque.
    \begin{description} 
        \item[($\implies$)] Supponiamo $\vec d$ sia di linealità: $\vec x \alpha\vec d \in P$ per ogni $\alpha \in \R$ significa che la relazione vale per ogni $\alpha \geq 0$ e per ogni $\alpha \leq 0$. La prima implica immediatamente che $\vec d$ è di recessione. 
        
        Per la seconda siccome $\alpha \leq 0$ possiamo scriverlo come $\alpha = -\beta$ (con $\beta \geq 0$); allora \[
            \vec x + \alpha\vec d = \vec x - \beta\vec d = \vec x + \beta(-\vec d) \in P,
        \] dunque $-\vec d$ è di recessione.
        \item[($\impliedby$)] Supponiamo che $\vec d, -\vec d$ siano entrambi di recessione. Allora per ogni $\alpha \geq 0$ vale che \[
            \vec x + \alpha\vec d, \vec x - \alpha\vec d \in P.    
        \] Dunque $\vec x + \alpha\vec d \in P$ per ogni $\alpha \in \R$, dunque $\vec d$ è una direzione di linealità.
    \end{description}
\end{proof}

Questa osservazione ci consente di caratterizzare semplicemente l'insieme delle direzioni di linealità di un poliedro.

\begin{proposition}
    Sia $P \deq \set{\vec x \in R^n \suchthat A\vec x \leq \vec b}$ un poliedro. Vale che \begin{equation}
        \lineal P = \set{\vec x \in \R^n \suchthat A\vec x = \vec 0}.    
    \end{equation}
\end{proposition}
\begin{proof}
    Per l'osservazione precedente $\vec d \in \lineal P$ se e solo se $\vec d, -\vec d$ sono entrambi di recessione. Per la \autoref{prop:caratt_dir_recessione} questo è equivalente ad affermare che \[
        A\vec d \leq \vec 0, \quad -A\vec d \leq \vec 0.
    \] La seconda disuguaglianza è equivalente a $A\vec d \geq \vec 0$, dunque \[
        \vec 0 \leq A\vec d \leq \vec 0    
    \] da cui segue che $\vec d$ è di linealità se e solo se $A\vec d = \vec 0$.
\end{proof}

Possiamo a questo punto enunciare il Teorema di Decomposizione dei Poliedri.

\begin{theorem}
    [Teorema di Decomposizione dei Poliedri]
    Sia $P \subseteq \R^n$. $P$ è un poliedro se e solo se esistono due insiemi finiti e non vuoti $\set{\vec v^1, \dots, \vec v^s} \subseteq \R^n$, $\set{\vec d^1, \dots, \vec d^t} \subseteq \R^n$ tali che \begin{equation}
        P = \conv*{\vec v^1, \dots, \vec v^s} + \cono*{\vec d^1, \dots, \vec d^t}.
    \end{equation}

    In tal caso $\cono*{\vec d^1, \dots, \vec d^t} = \rec P$. Inoltre, se $\lineal P = \set{\vec 0}$ allora una rappresentazione minimale di $\set{\vec v^1, \dots, \vec v^s}$ contiene tutti e soli i vertici di $P$.
\end{theorem}

\begin{corollary}
    Sia $P$ un poliedro tale che $\lineal P = \set{\vec 0}$.

    Se $P$ è un politopo, allora $P = \conv*{\vec v^1, \dots, \vec v^s}$ e $\rec P = \set{\vec{0}}$. Invece se $P$ è un cono poliedrico allora $P = \rec{P}$ e l'unico vertice di $P$ è $\vec 0$.
\end{corollary}

\section{Teorema Fondamentale della PL}

Cominciamo ad applicare le conoscenze sulla geometria della Programmazione Lineare ad un problema generico in forma canonica, come il seguente:
\begin{align}
    \tag{$\PP$} \label{eq:linear_problem}
    \begin{split} 
        \max              &\;\; \vec c \cdot \vec x \\
        \text{s. t.} &\;\; \vec x \in P = \set{\vec x \in \R^n \suchthat A\vec x \leq \vec b}
    \end{split}
\end{align}

\begin{definition}
    [Direzione ammissibile]
    Sia $\vec x \in P$. Un vettore $\vec \xi \in \R^n$ si dice \emph{direzione ammissibile} per $\vec x$ se esiste un $\bar\lambda \geq 0$ tale che \begin{equation}
        \vec x + \lambda\vec \xi \in P \quad \text{per ogni } \lambda \in \closedInt*{0, \bar{\lambda}}.
    \end{equation}
\end{definition}

Dunque una direzione è ammissibile se e solo se spostandoci dal punto $\vec x$ nella direzione e nel verso di $\vec \xi$ rimaniamo nel poliedro, purché lo spostamento sia sufficientemente piccolo, ovvero minore di $\bar \lambda$.

La prossima proposizione caratterizza le direzioni ammissibili per $\vec x$.

\begin{proposition}
    Sia $\vec x \in P$. Allora $\vec \xi$ è ammissibile per $\vec x$ se e solo se \[
        A_{I(\vec x)}\vec \xi \leq \vec 0,
    \] dove $I(\vec x)$ è l'insieme dei vincoli attivi in $\vec x$.
\end{proposition}
\begin{proof}
    $\vec \xi$ è ammissibile se e solo se il punto $\vec x + \lambda\vec \xi$ è ancora nel poliedro, ovvero se \begin{equation}\label{eq:cond_ammissib}
        A(\vec x + \lambda\vec \xi) = A\vec x + \lambda A\vec \xi \leq \vec b.
    \end{equation} Questa relazione deve valere per tutti gli indici di riga $i$ della matrice: se $i \notin I(\vec x)$ significa che $A_i\vec x < b_i$, dunque la \eqref{eq:cond_ammissib} vale a patto che il passo $\lambda$ sia sufficientemente piccolo.

    Invece se $i \in I(\vec x)$ abbiamo che $A_i\vec x = b_i$, dunque segue che \[
        A_i\vec x + \lambda A_i\vec \xi = b_i + \lambda A_i\vec \xi,
    \] e questa quantità è minore o uguale a $b_i$ se e solo se $A_i\vec \xi \leq 0$, da cui segue la tesi.
\end{proof}

Dunque l'insieme delle direzioni ammissibili per $\vec x \in P$ è dato dal cono poliedrico \[
    C(\vec x) \deq \set{\vec \xi \in \R^n \suchthat A_{I(\vec x)}\vec \xi \leq \vec 0}.    
\]

\begin{remark}
    Notiamo che se $\vec x$ è un punto \emph{interno} al poliedro, ovvero se $I(\vec x) = \varnothing$, allora ogni direzione è una direzione ammissibile per $\vec x$.
\end{remark}

\begin{definition}
    [Direzione di crescita]
    Sia $\vec x \in P$. Una direzione $\vec \xi \in \R^n$ si dice \emph{direzione di crescita} per $\vec x$ se \begin{equation}\label{eq:def_crescita}
        \vec c \cdot (\vec x + \vec \xi) \geq \vec c \cdot \vec x.
    \end{equation}
\end{definition}

Una direzione è dunque di crescita se spostandoci lungo di essa aumentiamo il valore della funzione obiettivo. Notiamo inoltre che l'equazione \eqref{eq:def_crescita} è equivalente a \[
    \vec c \cdot \vec \xi \geq 0, 
\] dunque le direzioni di crescita sono indipendenti dal punto $\vec x$ considerato. Vale quindi la seguente proposizione.

\begin{proposition}
    Una direzione $\vec \xi$ è di crescita se e solo se \begin{equation}
        \vec c \cdot \vec x \geq 0.
    \end{equation}
\end{proposition}

Le direzioni di crescita ci consentono di caratterizzare meglio le soluzioni ottime di un problema di Programmazione Lineare, come specificato dal seguente lemma.

\begin{lemma}
    Sia $\vecbar x \in P$. Allora $\vecbar x$ è soluzione ottima del problema \eqref{eq:linear_problem} se e solo se non esistono direzioni di crescita ammissibili per $\vecbar x$.
\end{lemma}
% \begin{proof}
    % non so se mi va di farla
% \end{proof}

Ricordando le definizioni di direzione di crescita e direzione ammissibile possiamo affermare che $\vecbar x$ è una soluzione ottima del problema \eqref{eq:linear_problem} se e solo se il seguente sistema \begin{equation*}
    \begin{cases}
        A_{I(\vecbar x)}\vec \xi \leq \vec 0\\
        \vec c \cdot \vec \xi > 0
    \end{cases}
\end{equation*}
non ha soluzione. Infatti una soluzione del sistema è una direzione ammissibile per $\vecbar x$ (per la prima equazione) e di crescita (per la seconda). Vedremo più avanti un modo per capire se questo sistema ha soluzione oppure no.

Enunciamo ora il Teorema Fondamentale della Programmazione Lineare.

\begin{theorem}
    [Teorema Fondamentale della PL]
    Sia \eqref{eq:linear_problem} un problema di programmazione lineare in forma canonica e supponiamo che la regione ammissibile $P$ sia tale che \[
        P = \conv*{\vec v^1, \dots, \vec v^s} + \cono*{\vec d^1, \dots, \vec d^t}.  
    \]

    Allora valgono i seguenti fatti.
    \begin{enumerate}
        \item Il valore ottimo di \eqref*{eq:linear_problem} è finito se e solo se nessuna direzione di recessione di \eqref*{eq:linear_problem} è di crescita, ovvero se \[
            \vec c \cdot \vec d^j \leq 0    
        \] per ogni $j \in \set{1, \dots, t}$.
        \item Se \eqref*{eq:linear_problem} ha ottimo finito, allora esiste un $i \in \set{1, \dots, s}$ tale che $v^i$ è una soluzione ottima di \eqref*{eq:linear_problem}.
    \end{enumerate}
\end{theorem}

In particolare se la matrice associata al problema ha rango $n$ segue che la soluzione ottima del problema \eqref*{eq:linear_problem} è in uno dei vertici del poliedro $P$.