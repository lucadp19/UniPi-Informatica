\chapter{Applicazioni lineari}

\section{Applicazioni lineari}

\begin{definition}
    Siano $V, W$ spazi vettoriali. Allora un'applicazione $f : V \to W$ si dice lineare
    se
    \begin{align}
        &f(\bm{0_V}) = \bm{0_W} \\
        &f(\bm{v} + \bm{w}) = f(\bm{v}) + f(\bm{w}) &&\forall v, w \in V \\
        &f(k\bm{v}) = kf(\bm{v})                    &&\forall v\in V, k \in \R 
    \end{align}
    $V$ si dice dominio dell'applicazione lineare, $W$ si dice codominio.
\end{definition}

\begin{definition}
    Siano $V, W$ spazi vettoriali e sia $f : V \to W$ lineare. Allora si dice immagine di $f$ l'insieme \begin{equation}
        \Imm{f} = \left\{ f(\bm{v}) \mid \bm v \in V\right\}.
    \end{equation}
\end{definition}

\begin{remark}
    Se $f : V \to W$ allora $\Imm{f} \subseteq W$. In particolare si puo' dimostrare che $\Imm{f}$ e' un sottospazio di $W$, e dunque che $0 \leq \dim\Imm{f} \leq \dim W$.
\end{remark}

\begin{definition}
    Siano $V, W$ spazi vettoriali e sia $f : V \to W$ lineare. Allora si dice kernel (o nucleo) di $f$ l'insieme \begin{equation}
        \ker{f} = \left\{ \bm{v} \in V \mid f(\bm v) = \bm{0_W}\right\}.
    \end{equation}
\end{definition}

\begin{theorem} 
    [delle dimensioni] \label{th_dimensioni}
    Siano $V, W$ spazi vettoriali e sia $f : V \to W$ lineare. Allora vale il seguente fatto:
    \begin{equation}
        \dim V = \dim \Imm f + \dim \ker f.
    \end{equation}
\end{theorem}
\begin{proof}
    Sia $k$ la dimensione di $\ker f$ e $n$ la dimensione di $V$.
    Sia $\alpha = \ang{\bm{v_1}, \dots, \bm{v_k}}$ una base di $\ker f$. Dato che $\ker f$ e' un sottospazio di $V$, per il teorema del completamento ad una base (\ref{th_completamento}) possiamo completare $\alpha$ ad una base $\beta = \ang{\bm{v_1}, \dots, \bm{v_k}, \bm{u_1}, \dots, \bm{u_{n-k}}}$ di $V$.

    Per la proposizione \ref{base_mappata_generatori_immagine} segue che l'immagine della base $\beta$, cioe' $f(\beta) = \ang{f(\bm{v_1}), \dots, f(\bm{v_k}), f(\bm{u_1}), \dots, f(\bm{u_{n-k}})}$, e' una insieme di generatori di $\Imm{f}$. Dato che $\bm{v_1}, \dots, \bm{v_k} \in \ker f$, allora \begin{alignat*}
        {1}
        \Imm{f} &= \Span{f(\bm{v_1}), \dots, f(\bm{v_k}), f(\bm{u_1}), \dots, f(\bm{u_{n-k}})}\\
        &= \Span{0, \dots, 0, f(\bm{u_1}), \dots, f(\bm{u_{n-k}})}\\
        &= \Span{f(\bm{u_1}), \dots, f(\bm{u_{n-k}})}.
    \end{alignat*}

    Se $f(\bm{u_1}), \dots, f(\bm{u_{n-k}})$ sono indipendenti allora segue che essi formano una base di $\Imm{f}$, cioe' che $\dim \Imm{f} = n - k = \dim V - \dim \ker f$.

    Consideriamo quindi una generica combinazione lineare \[
        x_1f(\bm{u_1}) + \dots + x_{n-k}f(\bm{u_{n-k}})
    \] e dimostriamo che imponendola uguale a $\bm 0$ segue che i coefficienti della combinazione devono essere uguali a 0.
    \begin{alignat*}
        {1}
        &x_1f(\bm{u_1}) + \dots + x_{n-k}f(\bm{u_{n-k}}) = \bm 0 \\
        \iff &f(x_1\bm{u_1} + \dots + x_{n-k}\bm{u_{n-k}}) = \bm 0 \\
        \intertext{che per definizione di kernel significa}
        \iff &x_1\bm{u_1} + \dots + x_{n-k}\bm{u_{n-k}} \in \ker f.
    \end{alignat*}
    Dato che $\alpha$ e' una base di $\ker f$ allora segue che
    \begin{alignat*}{1}
        &x_1\bm{u_1} + \dots + x_{n-k}\bm{u_{n-k}} = a_1\bm{v_1} + \dots + a_k\bm{v_k} \\
        \iff &x_1\bm{u_1} + \dots + x_{n-k}\bm{u_{n-k}} - a_1\bm{v_1} - \dots - a_k\bm{v_k} = \bm 0.
    \end{alignat*}
    Ma $\beta = \ang{\bm{v_1}, \dots, \bm{v_k}, \bm{u_1}, \dots, \bm{u_{n-k}}}$ e' una base di $V$, dunque i vettori che la compongono devono essere indipendenti, da cui segue \[
        x_1 = \dots = x_{n-k} = 0.   
    \]
    Dunque $\ang{f(\bm{u_1}), \dots, f(\bm{u_{n-k}})}$ e' una base di $\Imm{f}$ e dunque segue che $\dim V = \dim \ker f + \dim \Imm{f}$, come volevasi dimostrare.
\end{proof}

Una conseguenza diretta del teorema delle dimensioni e' che data una matrice $A$ e riducendola a scalini tramite mosse di riga non cambia la dimensione dello spazio delle colonne.
\begin{proposition}\label{invarianza_dim_colonne_per_mosse_riga}
    Sia $A \in \M_{n\times m}$ e siano $C_1, \dots, C_m \in \R^n$ le colonne della matrice. Sia $S$ la matrice ottenuta riducendo a scalini per riga la matrice $A$, e siano $C'_1, \dots, C'_m$ le colonne di $S$. Allora \begin{equation}
        \dim \Span{C_1, \dots, C_m} = \dim \Span{C'_1, \dots, C'_m}.
    \end{equation}
\end{proposition}
\begin{proof}
    Dato che $S$ e' ottenuta riducendo $A$ a scalini, allora le soluzioni dei sistemi $A\bm{x} = \bm 0$ e $S\bm{x} = \bm 0$ devono essere le stesse. 
    
    Siano $L_A : \R^m \to \R^n$ e $L_S : \R^m \to \R^n$ le applicazioni lineari associate ad $A$ e $S$; trascrivendo le equazioni di prima in termini delle applicazioni otteniamo che $L_A(\bm x) = \bm 0$ se e solo se $L_S(\bm x) = \bm 0$. 
    
    Allora segue che $\ker L_A = \ker L_S$, dunque $\dim \ker L_A = \dim \ker L_S$. Per la proposizione \ref{span_colonne=immagine_applicazione_associata} e per il teorema delle dimensioni (\ref{th_dimensioni}) segue quindi che
    \begin{alignat*}
        {1}
        \dim \Span{C_1, \dots, C_m} &= \dim \Imm{L_A}\\
        &= \dim \R^m - \dim \ker L_A\\
        &= \dim \R^m - \dim \ker L_S\\
        &= \dim \Imm{L_S} \\
        &= \dim \Span{C'_1, \dots, C'_m}
    \end{alignat*}
    che e' la tesi.
\end{proof}

Ovviamente lo stesso ragionamento ci dice che ridurre una matrice a scalini tramite mosse di colonna non cambia la dimensione dello spazio delle righe.

\begin{corollary}\label{invarianza_dim_righe_per_mosse_colonna}
    Sia $A \in \M_{n\times m}$ e siano $R_1, \dots, R_m \in \R^n$ le righe della matrice. Sia $S$ la matrice ottenuta riducendo a scalini per colonna la matrice $A$, e siano $R'_1, \dots, R'_m$ le righe di $S$. Allora \begin{equation}
        \dim \Span{R_1, \dots, R_m} = \dim \Span{R'_1, \dots, R'_m}.
    \end{equation}
\end{corollary}
\begin{proof}
    Consideriamo la matrice $A^T$ e riduciamola per righe, ottenendo la matrice $S$. Per la proposizione \ref{invarianza_dim_colonne_per_mosse_riga} la dimensione dello spazio delle righe di $S$ e' uguale alla dimensione dello spazio delle righe di $A^T$. Notiamo inoltre che $S$ e' la trasposta della matrice ottenuta riducendo $A$ per colonne, dunque la dimensione dello spazio delle colonne di $S^T$ e' la dimensione dello spazio delle colonne di $A$, cioe' la tesi.
\end{proof}

\section{Applicazioni iniettive e surgettive}

Le applicazioni lineari sono funzioni, dunque possono essere iniettive e surgettive, ma essendo lineari hanno delle proprieta' particolari.

\begin{definition}
    Siano $V, W$ spazi vettoriali e sia $f : V \to W$ lineare. Allora $f$ si dice iniettiva se per ogni $\bm{v}, \bm{u} \in V$ vale che $f(\bm{v}) = f(\bm{u})$ se e solo se $\bm{v} = \bm{u}$.
\end{definition}

\begin{proposition}\label{ker_funzione_iniettiva}
    Siano $V, W$ spazi vettoriali e sia $f : V \to W$ lineare. Allora $f$ e' iniettiva se e solo se $\ker f = \{\bm{0_V}\}$. 
\end{proposition}
\begin{proof}
    Notiamo che dato che $f$ e' lineare allora per definizione $f(\bm{0_V}) = \bm{0_W}$, dunque $\bm{0_V} \in \ker f$.
    \begin{description}
        \item [($\implies$).] Supponiamo che $f$ sia iniettiva e supponiamo che per qualche $\bm{v} \in V$ valga $\bm{v} \in \ker f$. Allora per definizione di kernel $f(\bm{v}) = \bm{0_W} = f(\bm{0_V})$, dunque per iniettivita' di $f$ da $f(\bm{v}) = f(\bm{0_V})$ segue che $\bm v = \bm{0_V}$. Dunque $\ker f = \{\bm{0_V}\}$.
        \item [($\impliedby$).] Supponiamo che $\ker f = \left\{ \bm{0_V}\right\}$. Per dimostrare che $f$ e' iniettiva e' sufficiente dimostrare che per ogni $\bm{v}, \bm{w} \in V$ segue che $f(\bm{v}) = f(\bm{w}) \implies \bm{v} = \bm{w}$.
        \begin{alignat*}{1}
            &f(\bm{v}) = f(\bm{w}) \\
            \iff &f(\bm{v}) - f(\bm{w}) = \bm{0_W} \\
            \iff &f(\bm{v} - \bm{w}) = \bm{0_W} \\
            \iff &\bm{v} - \bm{w} \in \ker f \\
            \intertext{ma l'unico elemento di $\ker f$ e' $\bm{0_V}$, dunque}
            \implies &\bm{v} - \bm{w} = \bm{0_V}\\
            \iff &\bm{v} = \bm{w}
        \end{alignat*}
        cioe' $f$ e' iniettiva. \qedhere
    \end{description}
\end{proof}

\begin{corollary}\label{iniettiva_allora_dimIm_uguale_dimV}
    Se $f$ e' iniettiva allora $\dim \Imm{f} = \dim V$.
\end{corollary}
\begin{proof}
    Infatti per la proposizione \ref{ker_funzione_iniettiva} $\dim \ker f = 0$, dunque per il teorema delle dimensioni (\ref{th_dimensioni}) segue che $\dim V = \dim \Imm{f} + \dim \ker f = \dim \Imm{f}$.
\end{proof}

\begin{corollary}
    Siano $V, W$ spazi vettoriali tali che $\dim V > \dim W$. Allora non puo' esistere $f : V \to W$ iniettiva.
\end{corollary}
\begin{proof}
    Infatti per il corollario \ref{iniettiva_allora_dimIm_uguale_dimV} segue che $\dim \Imm{f} = \dim V$, ma $\dim \Imm{f} < \dim W$ dunque non puo' essere che $\dim V > \dim W$.
\end{proof}

\begin{proposition}\label{indipendenti_mappati_indipendenti}
    Siano $V, W$ spazi vettoriali, $\bm{v_1}, \dots, \bm{v_n} \in V$ linearmente indipendenti e sia $f : V \to W$ lineare. Se $f$ e' iniettiva allora segue che $f(\bm{v_1}), \dots, f(\bm{v_n})$ sono linearmente indipendenti. 
\end{proposition}
\begin{proof}
    Consideriamo una combinazione lineare di $f(\bm{v_1}), \dots, f(\bm{v_n})$ e dimostriamo che imponendola uguale al vettore nullo segue che i coefficienti devono essere tutti nulli.
    \begin{alignat*}
        {1}
        &x_1f(\bm{v_1}) + \dots + x_nf(\bm{v_n}) = \bm{0_W}\\
        \iff &f(x_1\bm{v_1} + \dots + x_n\bm{v_n}) = \bm{0_W}\\
        \intertext{Per la proposizione \ref{ker_funzione_iniettiva} segue che}
        \iff &x_1\bm{v_1} + \dots + x_n\bm{v_n} = \bm{0_V}\\
        \intertext{Ma i vettori $\bm{v_1}, \dots, \bm{v_n}$ sono linearmente indipendenti, dunque l'unica combinazione lineare che li annulla e' quella a coefficienti nulli, cioe'}
        \iff &x_1 = \dots = x_n = 0
    \end{alignat*}
    Quindi una combinazione lineare di $f(\bm{v_1}), \dots, f(\bm{v_n})$ e' uguale al vettore nullo se e solo se tutti i coefficienti sono nulli, dunque $f(\bm{v_1}), \dots, f(\bm{v_n})$ sono linearmente indipendenti.
\end{proof}

\begin{definition}
    Siano $V, W$ spazi vettoriali e sia $f : V \to W$ lineare. Allora $f$ si dice surgettiva se per ogni $\bm{w} \in W$ esiste $\bm{v} \in V$ tale che $f(\bm{v}) = \bm{w}$.
\end{definition}

\begin{remark}
    Una funzione $f : V \to W$ e' surgettiva se e solo se $\Imm{f} = W$.
\end{remark}

\begin{proposition}\label{base_mappata_generatori_immagine}
    Sia $f : V \to W$. Allora se $\ang{\bm{v_1}, \dots, \bm{v_n}}$ e' una base di $V$ segue che $\left\{ f(\bm{v_1}), \dots, f(\bm{v_n})\right\}$ e' un insieme di generatori di $\Imm{f}$.
\end{proposition}
\begin{proof}
    Sia $\bm{w} \in \Imm{f}$ generico; allora questo equivale a dire che esiste $\bm{v} \in V$ tale che $f(\bm{v}) = \bm{w}$.
    Dato che $\ang{\bm{v_1}, \dots, \bm{v_n}}$ e' una base di $V$, allora possiamo scrivere $\bm{v}$ come $a_1\bm{v_1} + \dots a_n\bm{v_n}$, dunque 
    \begin{equation*}
        \bm{w} = f(a_1\bm{v_1} + \dots + a_n\bm{v_n}) = a_1f(\bm{v_1}) + \dots + a_nf(\bm{v_n}).
    \end{equation*}
    Dunque per la generalita' di $w$ segue che ogni elemento di $\Imm{f}$ appartiene allo span di $f(\bm{v_1}), \dots, f(\bm{v_n})$, cioe' $\left\{ f(\bm{v_1}), \dots, f(\bm{v_n})\right\}$ e' un insieme di generatori di $\Imm{f}$.
\end{proof}

\begin{corollary}\label{base_mappata_generatori_codominio}
    Sia $f : V \to W$ surgettiva. Allora se $\ang{\bm{v_1}, \dots, \bm{v_n}}$ e' una base di $V$ segue che $\left\{ f(\bm{v_1}), \dots, f(\bm{v_n})\right\}$ e' un insieme di generatori di $W$.
\end{corollary}
\begin{proof}
    Segue direttamente dalla proposizione \ref{base_mappata_generatori_immagine}: infatti se una funzione e' surgettiva allora $\Imm{f} = W$, dunque se $\left\{ f(\bm{v_1}), \dots, f(\bm{v_n})\right\}$ e' un insieme di generatori di $\Imm f$ segue che e' anche un insieme di generatori di $W$.
\end{proof}

\section{Isomorfismi}

\begin{definition}
    Siano $V, W$ spazi vettoriali e sia $f : V \to W$ lineare. Allora $f$ si dice bigettiva se $f$ e' sia iniettiva che surgettiva.
\end{definition}

\begin{definition}
    Una funzione $f : V \to W$ si dice invertibile se esiste $f^{-1} : W \to V$ tale che \begin{equation}
        f(\bm{v}) = \bm{w} \iff f^{-1}(\bm{w}) = \bm{v}
    \end{equation}
    Se $f$ e' invertibile allora $f^{-1}$ e' unica e si chiama inversa di $f$.
\end{definition}

\begin{remark}
    Un'applicazione lineare e' invertibile se e solo se e' bigettiva. 
\end{remark}

\begin{definition}
    Siano $V, W$ spazi vettoriali e sia $f : V \to W$ lineare. Allora se $f$ e' bigettiva si dice che $f$ e' un isomorfismo.
    
    Se esiste un isomorfismo tra gli spazi $V$ e $W$ allora si dice che $V$ e' isomorfo a $W$, e si indica con $V \cong W$.
\end{definition}

\begin{remark}
    Le seguenti affermazioni sono equivalenti:
    \begin{itemize}
        \item $f$ e' bigettiva;
        \item $f$ e' invertibile;
        \item $f$ e' un isomorfismo.
    \end{itemize}
\end{remark}

Gli isomorfismi preservano la linearita' dello spazio vettoriale e tutte le sue proprieta', come ci dicono le seguenti proposizioni.

\begin{proposition}
    Sia $V$ uno spazio vettoriale, $\alpha = \ang{\bm{v_1}, \dots, \bm{v_n}}$ una base di $V$. Allora se $f : V \to W$ e' un isomorfismo segue che $\beta = \ang{f(\bm{v_1}), \dots, f(\bm{v_n})}$ e' una base di $W$ (cioe' gli isomorfismi mappano basi in basi).
\end{proposition}
\begin{proof}
    Dato che $f$ e' un isomorfismo allora $f$ e' bigettiva.

    Dunque dato che $f$ e' iniettiva essa mappa un insieme di vettori indipendenti (come la base $\alpha$ di $V$) in un insieme di vettori linearmente indipendenti per la proposizione \ref{indipendenti_mappati_indipendenti}, dunque $\beta$ e' un insieme di vettori linearmente indipendenti. 

    Inoltre, dato che $f$ e' surgettiva, per la proposizione \ref{base_mappata_generatori_codominio} essa mappa una base di $V$ in un insieme di generatori del codominio $W$, dunque i vettori di $\beta$ generano $W$.

    Dunque $\beta$ e' un insieme di generatori linearmente indipendenti, e quindi e' una base di $W$.
\end{proof}

\begin{proposition}
    Se $V$ e' uno spazio vettoriale di dimensione $n = \dim V$, allora $V$ e' isomorfo a tutti e soli gli spazi vettoriali di dimensione $n$.
\end{proposition}
\begin{proof}
    Deriva direttamente dalla proposizione precedente: infatti ogni isomorfismo che ha come dominio $V$ deve portare una base di $V$ in una base di $W$, dunque la dimensione di $V$ deve essere uguale a quella di $W$.
\end{proof}

Quindi per calcolare una base di un sottospazio $W$ di uno spazio $V$ spesso conviene passare allo spazio dei vettori colonna $\R^n$ isomorfo allo spazio $V$, calcolare la base del sottospazio $\tilde{W}$ isomorfo a $W$ e infine tornare allo spazio di partenza.

\begin{example}
    Sia $V = \R[x]^{\leq 2}$ e sia $W \subseteq V$ il sottospazio di $V$ tale che $p \in W \iff p(2) = 0$. Dimostrare che $W$ e' un sottospazio e trovarne una base.
\end{example}
\begin{solution}
    Svolgiamo i due punti separatamente.
    \begin{enumerate}
        \item Dimostriamo che $W$ e' un sottospazio di $V$.
        \begin{itemize}
            \item Sia $\bm{0_V} \in V$ tale che $\bm{0_V}(x) = 0 + 0x + 0x^2$. Allora $\bm{0_V}(2) = 0 + 0\cdot 2 + 0 \cdot 4 = 0$, dunque $\bm{0_V} \in W$.
            \item Supponiamo $p, q \in W$ e mostriamo che $p+q \in W$. Dunque \[
                (p+q)(2) = p(2) + q(2) = 0 + 0 = 0    
            \] dunque $p + q \in W$.
            \item Supponiamo $p \in W$ e mostriamo che $kp \in W$ per un generico $k \in \R$. Dunque \[
                (kp)(2) = kp(2) = k \cdot 0 = 0    
            \] cioe' $kp \in W$ per ogni $k \in \R$.
        \end{itemize} 
        Dunque abbiamo dimostrato che $W$ e' un sottospazio di $V$.
        \item Cerchiamo ora una base per $W$.
         
        Consideriamo un generico $p \in V$, cioe' $p(x) = a + bx + cx^2$. La condizione che definisce $W$ e' $p(2) = a + 2b + 4c = 0$. 
    
        Passiamo ora allo spazio isomorfo $\R^3$. Il vettore corrispondente a $p$ in $\R^3$ e' $\bm{\tilde{p}} = \begin{psmallmatrix} a \\ b \\ c \end{psmallmatrix}$, mentre la condizione di appartenenza allo spazio $\widetilde{W} \subseteq \R^3$ isomorfo a $W$ e' sempre $a+2b+4c = 0$. Cerchiamo una base di $\widetilde{W}$ passando alla forma parametrica, cioe' cercando di esplicitare la condizione di appartenenza allo spazio e inserendola nella definizione stessa del vettore.
        Dato che la condizione e' data dal sistema $a+2b+4c = 0$ che ha due variabili libere, scelgo $b, c$ libere ottenendo $a = -2b - 4c$. Sostituendo in $\bm{\tilde{p}}$:\[
            \bm{\tilde p} = \begin{pmatrix} a \\ b \\ c \end{pmatrix} = \begin{pmatrix}
                -2b-4c\\b\\c
            \end{pmatrix} = b\begin{pmatrix} -2 \\ 1 \\ 0 \end{pmatrix} + c\begin{pmatrix} -4 \\ 0 \\ 1 \end{pmatrix}
        \]
        Dato che ogni vettore generico di $\widetilde{W}$ puo' essere scritto come combinazione lineare dei due vettori $\bm{\tilde{w}_1} = \begin{psmallmatrix} -2 \\ 1 \\ 0 \end{psmallmatrix}$ e $\bm{\tilde{w}_2} = \begin{psmallmatrix} -4 \\ 0 \\ 1 \end{psmallmatrix}$, allora segue che essi sono generatori di $W$. 
            
        Controlliamo ora che siano linearmente indipendenti riducendo a scalini per riga (secondo la proposizione \ref{estrarre_una_base}) la matrice che ha come colonne $\bm{\tilde{w}_1}$ e $\bm{\tilde{w}_2}$.
        \begin{equation*}
            \begin{pmatrix}[c|c]
                -2 & -4 \\ 1 & 0 \\ 0 & 1
            \end{pmatrix} \xrightarrow[]{R_2 + \frac12R_1}
            \begin{pmatrix}[c|c]
                -2 & -4 \\ 0 & -2 \\ 0 & 1
            \end{pmatrix} \xrightarrow[]{R_3 + \frac12R_2}
            \begin{pmatrix}[c|c]
                -2 & -4 \\ 0 & -2 \\ 0 & 0
            \end{pmatrix}
        \end{equation*}
        Dato che ci sono tanti pivot quante colonne segue che tutti i vettori originali sono indipendenti.
        I vettori $\bm{\tilde{w}_1}$ e $\bm{\tilde{w}_2}$ sono quindi indipendenti e generano $\widetilde{W}$: segue che $\ang{\bm{\tilde{w}_1}, \bm{\tilde{w}_1}}$ e' una base di $\widetilde{W}$, quindi $\dim \widetilde{W} = 2$.

        Tornando allo spazio originale, i vettori corrispondenti alla base sono quindi $w_1(x) = (-2 + x)$ e $w_2(x) = (-4 + x^2)$. L'insieme ordinato $\ang{(-2+x), (-4+x^2)}$ forma dunque una base di $W$ e dunque $\dim W = 2$.
    \end{enumerate}
\end{solution}

\section{Matrice associata ad una funzione}

Come avevamo visto nel primo capitolo, le matrici sono associate ad applicazioni lineari da vettori colonna in vettori colonna. Possiamo generalizzare questo concetto e definire una matrice associata ad ogni applicazione lineare.

\begin{definition}
    Siano $V, W$ spazi vettoriali, $f : V \to W$ lineare, $\alpha$ base di $V$ e $\beta$ base di $W$. Allora si dice chiama \textbf{matrice associata all'applicazione lineare} $f$ la matrice $[f]^{\alpha}_{\beta}$ tale che
    \begin{equation}
        \forall \bm{v} \in V. \quad \left( f(\bm{v}) \right)_{\beta} = [f]^{\alpha}_{\beta} \cdot (\bm{v})_{\alpha}.
    \end{equation}
    Cioe' se $f$ mappa $\bm{v} \mapsto \bm{w}$ allora $[f]^{\alpha}_{\beta}$ e' una matrice che porta (tramite il prodotto) il vettore colonna delle coordinate di $\bm{v}$ rispetto ad una base $\alpha$ nel vettore colonna delle coordinate di $\bm{w}$ rispetto ad una base $\beta$.
\end{definition}

Per trovare la matrice associata ad $f$ rispetto alle basi $\alpha = \ang{\bm{v_1}, \dots, \bm{v_n}}$ e $\beta = \ang{\bm{w_1}, \dots, \bm{w_m}}$ possiamo seguire questo procedimento:
\begin{itemize}
    \item calcoliamo $f(\bm{v_1}) = \bm{u_1}, \dots, f(\bm{v_n}) = \bm{u_n}$;
    \item scriviamo $\bm{u_i}$ in termini della base $\beta$, cioe' \begin{equation*}
        \bm{u_i} = a_{1i}\bm{w_1} + \dots + a_{mi}\bm{w_m} \iff [\bm{u_i}]_{\beta} = \begin{pmatrix}
            a_{1i} \\ \vdots \\ a_{mi}
        \end{pmatrix};
    \end{equation*}
    \item notiamo che dato che $\bm{v_i}$ e' l'$i$-esimo vettore della base $\alpha$, allora la sua rappresentazione in termini della base sara' un vettore colonna con tutti $0$ tranne un $1$ in posizione $i$;
    \item per la proposizione \ref{j-esima_colonna} il risultato del prodotto $[f]^{\alpha}_{\beta} \cdot (\bm{v_i})_{\alpha}$ sara' l'$i$-esima colonna della matrice $[f]^{\alpha}_{\beta}$, ma $[f]^{\alpha}_{\beta} \cdot (\bm{v_i})_{\alpha}$ deve essere uguale a $[f(v_i)]_{\beta} = [\bm{u_i}]_{\beta}$, dunque l'$i$-esima colonna di $[f]^{\alpha}_{\beta}$ sara' data dal vettore colonna $[\bm{u_i}]_{\beta}$;
    \item dunque la matrice avra' per colonne i vettori $[\bm{u_1}]_{\beta}, \dots, [\bm{u_n}]_{\beta}$, cioe'
    \begin{equation}
        [f]^{\alpha}_{\beta} = \begin{pmatrix}
            a_{11} & a_{12} & \dots & a_{1n} \\
            a_{21} & a_{22} & \dots & a_{2n} \\
            \vdots & \vdots & \vdots& \vdots \\
            a_{m1} & a_{m2} & \dots & a_{mn} \\
        \end{pmatrix}.
    \end{equation}
\end{itemize}

\begin{example}
    Sia $V = \M_{2\times 2}(\R)$ e sia $\alpha = \ang{\begin{psmallmatrix}1&0\\0&0\end{psmallmatrix}, \begin{psmallmatrix}0&1\\0&0\end{psmallmatrix}, \begin{psmallmatrix}0&0\\1&0\end{psmallmatrix}, \begin{psmallmatrix}0&0\\0&1\end{psmallmatrix}}$ una sua base.    
    Sia $A = \begin{psmallmatrix}0&1\\1&0\end{psmallmatrix} \in V$ e $f : V \to V$ tale che $f(B) = AB - BA$.
    \begin{enumerate}
        \item Dimostrare che $f$ e' lineare.
        \item Calcolare $[f]^{\alpha}_{\alpha}$.
        \item Dare una base di $\Imm{f}$.
        \item Dare una base di $\ker f$.
    \end{enumerate}
\end{example}
\begin{solution}
    Verifichiamo i quattro punti.
    \begin{enumerate}
        \item Dimostriamo che $f$ e' lineare.
            \begin{alignat*}{2}
                &\text{(a) } f(\bm{0}) = f\left(\begin{psmallmatrix}0&0\\0&0\end{psmallmatrix}\right) = A\begin{psmallmatrix}0&0\\0&0\end{psmallmatrix} - \begin{psmallmatrix}0&0\\0&0\end{psmallmatrix}A = \begin{psmallmatrix}0&0\\0&0\end{psmallmatrix} - \begin{psmallmatrix}0&0\\0&0\end{psmallmatrix} = \bm 0 \\
                &\begin{alignedat}{1}
                    \text{(b) } f(B + C) &= A(B + C) - (B + C)A \\
                    &= AB + AC - BC - CA \\
                    &= (AB - BA) + (AC - CA) \\
                    &= f(B) + f(C)
                \end{alignedat}\\
                &\begin{alignedat}{1}
                    \text{(c) } f(kB) &= A(kB) - (kB)A \\
                    &= k(AB) - k(BA) \\
                    &= k(AB - BA) \\
                    &= kf(B)
                \end{alignedat}
            \end{alignat*}
            dunque $f$ e' lineare.
        \item Seguo il procedimento per ottenere $[f]^{\alpha}_{\alpha}$. Innanzitutto calcolo il risultato di $f$ sui vettori della base $\alpha$:
        \begin{alignat*}{1}
            &f\left(\begin{psmallmatrix}1&0\\0&0\end{psmallmatrix}\right) = 
            \begin{psmallmatrix}0&1\\1&0\end{psmallmatrix}\begin{psmallmatrix}1&0\\0&0\end{psmallmatrix} - \begin{psmallmatrix}1&0\\0&0\end{psmallmatrix}\begin{psmallmatrix}0&1\\1&0\end{psmallmatrix} = \begin{psmallmatrix}0&0\\1&0\end{psmallmatrix} - \begin{psmallmatrix}0&1\\0&0\end{psmallmatrix} = \begin{psmallmatrix}0&-1\\1&0\end{psmallmatrix} \\
            &f\left(\begin{psmallmatrix}0&1\\0&0\end{psmallmatrix}\right) = 
            \begin{psmallmatrix}0&1\\1&0\end{psmallmatrix}\begin{psmallmatrix}0&1\\0&0\end{psmallmatrix} - \begin{psmallmatrix}0&1\\0&0\end{psmallmatrix}\begin{psmallmatrix}0&1\\1&0\end{psmallmatrix} = \begin{psmallmatrix}0&0\\0&1\end{psmallmatrix} - \begin{psmallmatrix}1&0\\0&0\end{psmallmatrix} = \begin{psmallmatrix}-1&0\\0&1\end{psmallmatrix} \\
            &f\left(\begin{psmallmatrix}0&0\\1&0\end{psmallmatrix}\right) = 
            \begin{psmallmatrix}0&1\\1&0\end{psmallmatrix}\begin{psmallmatrix}0&0\\1&0\end{psmallmatrix} - \begin{psmallmatrix}0&0\\1&0\end{psmallmatrix}\begin{psmallmatrix}0&1\\1&0\end{psmallmatrix} = \begin{psmallmatrix}1&0\\0&0\end{psmallmatrix} - \begin{psmallmatrix}0&0\\0&1\end{psmallmatrix} = \begin{psmallmatrix}1&0\\0&-1\end{psmallmatrix} \\
            &f\left(\begin{psmallmatrix}0&0\\0&1\end{psmallmatrix}\right) = 
            \begin{psmallmatrix}0&1\\1&0\end{psmallmatrix}\begin{psmallmatrix}0&0\\0&1\end{psmallmatrix} - \begin{psmallmatrix}0&0\\0&1\end{psmallmatrix}\begin{psmallmatrix}0&1\\1&0\end{psmallmatrix} = \begin{psmallmatrix}0&1\\0&0\end{psmallmatrix} - \begin{psmallmatrix}0&0\\1&0\end{psmallmatrix} = \begin{psmallmatrix}0&1\\-1&0\end{psmallmatrix}
        \end{alignat*}
        dunque le loro coordinate rispetto alla base $\alpha$ sono
        \begin{align*}
            &[f(\bm{v_1})]_{\alpha} = \begin{pmatrix}
                0 \\ -1 \\ 1 \\ 0
            \end{pmatrix} &[f(\bm{v_2})]_{\alpha} = \begin{pmatrix}
                -1 \\ 0 \\ 0 \\ 1
            \end{pmatrix}
            \\&[f(\bm{v_3})]_{\alpha} = \begin{pmatrix}
                1 \\ 0 \\ 0 \\ -1
            \end{pmatrix} &[f(\bm{v_4})]_{\alpha} = \begin{pmatrix}
                0 \\ 1 \\ -1 \\ 0
            \end{pmatrix}
        \end{align*}
        cioe' \begin{equation*}
            [f]^{\alpha}_{\alpha} = \begin{pmatrix}
                0 & -1 & 1 & 0 \\ -1 & 0 & 0 & 1 \\
                1 & 0 & 0 & -1 \\ 0 & 1 & -1 & 0
            \end{pmatrix}.
        \end{equation*}
        \item Per la proposizione \ref{base_mappata_generatori_immagine} sappiamo che l'insieme
        $\{f(\bm{v_1}), f(\bm{v_2}), f(\bm{v_3}), f(\bm{v_4})\}$ e' un insieme di generatori dell'immagine della funzione. 
        Per eliminare i vettori indipendenti passiamo all'isomorfismo con $\R^4$ tramite la base di partenza $\alpha$. Chiamiamo $\widetilde{W}$ lo spazio isomorfo a $\Imm{f}$, allora notiamo che il ruolo di $f$ nel nuovo spazio e' dato dalla matrice $[f]^{\alpha}_{\alpha}$, dunque il corrispondente insieme di generatori di $\widetilde{W}$ sara' \begin{gather*}
            \left\{ [f]^{\alpha}_{\alpha}[v_1]_{\alpha}, [f]^{\alpha}_{\alpha}[v_2]_{\alpha}, [f]^{\alpha}_{\alpha}[v_3]_{\alpha}, [f]^{\alpha}_{\alpha}[v_4]_{\alpha}\right\} \\
            \intertext{che e' uguale a }
            \left\{\begin{psmallmatrix} 0 \\ -1 \\ 1 \\ 0 \end{psmallmatrix}, \begin{psmallmatrix} -1 \\ 0 \\ 0 \\ 1 \end{psmallmatrix}, \begin{psmallmatrix} 1 \\ 0 \\ 0 \\ -1 \end{psmallmatrix}, \begin{psmallmatrix} 0 \\ 1 \\ -1 \\ 0 \end{psmallmatrix}\right\}  
        \end{gather*}
        Semplifichiamolo tramite mosse di colonna: \begin{gather*}
            \begin{pmatrix}[c|c|c|c]
                0 & -1 & 1 & 0 \\ -1 & 0 & 0 & 1 \\
                1 & 0 & 0 & -1 \\ 0 & 1 & -1 & 0
            \end{pmatrix} \xrightarrow[R_2 + R_3]{R_1 + R_4}
            \begin{pmatrix}[c|c|c|c]
                0 & 0 & 1 & 0 \\ 0 & 0 & 0 & 1 \\
                0 & 0 & 0 & -1 \\ 0 & 0 & -1 & 0
            \end{pmatrix} \xrightarrow[]{}
            \begin{pmatrix}[c|c|c|c]
                1 & 0 & 0 & 0\\0 & 1 & 0 & 0 \\
                0 & -1 & 0 & 0\\ -1 & 0 & 0 & 0
            \end{pmatrix}
        \end{gather*}
        dunque i vettori $\begin{psmallmatrix} 1 \\ 0 \\ 0 \\ -1 \end{psmallmatrix}, \begin{psmallmatrix} 0 \\ 1 \\ -1 \\ 0 \end{psmallmatrix}$ sono indipendenti e generano $\widetilde{W}$, dunque sono una base di $\widetilde{W}$.

        Tornando allo spazio originale otteniamo che una base di $\Imm{f}$ e' data da \[
            \beta = \ang{\begin{pmatrix} 1 & 0 \\ 0 & -1 \end{pmatrix}, \begin{pmatrix} 0 & 1 \\ -1 & 0 \end{pmatrix}}.
        \] e dunque $\dim \Imm{f} = 2$.
        \item Per definizione di kernel \[
            \ker f = \left\{ \begin{psmallmatrix}x&y\\z&t \end{psmallmatrix}\in \M_{2\times 2}(\R) \mid f\left(\begin{psmallmatrix}x&y\\z&t \end{psmallmatrix}\right) = \begin{psmallmatrix}0&0\\0&0 \end{psmallmatrix}\right\}.
        \] Sia $\widetilde{V} \subseteq \R^4$ lo spazio isomorfo a $\ker f$ tramite l'isomorfismo dato dalle coordinate dei vettori rispetto alla base $\alpha$. Allora \begin{alignat*}{1}
            \widetilde{V} &= \left\{ \begin{pmatrix}x\\y\\z\\t \end{pmatrix}\in \R^4 \mid [f]^{\alpha}_{\alpha}\begin{pmatrix}x\\y\\z\\t \end{pmatrix} = \begin{pmatrix}0\\0\\0\\0 \end{pmatrix}\right\} \\
            &= \left\{ \begin{pmatrix}x\\y\\z\\t \end{pmatrix}\in \R^4 \mid \begin{pmatrix}
                0 & -1 & 1 & 0 \\ -1 & 0 & 0 & 1 \\
                1 & 0 & 0 & -1 \\ 0 & 1 & -1 & 0
            \end{pmatrix}\begin{pmatrix}x\\y\\z\\t \end{pmatrix} = \begin{pmatrix}0\\0\\0\\0 \end{pmatrix}\right\}
        \end{alignat*}
        Dunque $\widetilde{V}$ e' formato da tutti e solo i vettori $\bm{x} \in \R^4$ che sono soluzione del sistema lineare $[f]^{\alpha}_{\alpha}\bm{x} = \bm{0}$. Risolviamolo tramite eliminazione gaussiana:
        \begin{gather*}
            \begin{pmatrix}
                0  & -1 & 1  & 0  \\ 
                -1 & 0  & 0  & 1  \\
                1  & 0  & 0  & -1 \\ 
                0  & 1  & -1 & 0
            \end{pmatrix} \xrightarrow[]{scambio}
            \begin{pmatrix}
                1  & 0  & 0  & -1 \\
                0  & 1  & -1 & 0  \\
                0  & -1 & 1  & 0  \\ 
                -1 & 0  & 0  & 1
            \end{pmatrix} \xrightarrow[R_4 + R_1]{R_3 + R_2} \\
            \xrightarrow[R_4 + R_1]{R_3 + R_2} \begin{pmatrix}
                1  & 0  & 0  & -1 \\
                0  & 1  & -1 & 0  \\
                0  & 0  & 0  & 0  \\ 
                0  & 0  & 0  & 0
            \end{pmatrix} \iff \left\{
                \begin{array}{@{}roror }
                    x & - & t & = & 0 \\
                    y & - & z & = & 0
                \end{array}
            \right. \iff \left\{
                \begin{array}{@{}ror }
                    x & = & t\\
                    y & = & z
                \end{array}
            \right.\\
            \intertext{dunque scegliendo $z, t \in \R$ libere otteniamo}
            \iff \begin{pmatrix} x \\ y \\ z \\t \end{pmatrix} = \begin{pmatrix} t \\ z \\ z \\ t \end{pmatrix} = z\begin{pmatrix} 0 \\ 1 \\ 1 \\ 0 \end{pmatrix} + t\begin{pmatrix} 1 \\ 0 \\ 0 \\ 1 \end{pmatrix}
        \end{gather*}
        Dunque $\tilde{\gamma} = \ang{\begin{psmallmatrix} 0 \\ 1 \\ 1 \\ 0 \end{psmallmatrix}, \begin{psmallmatrix} 1 \\ 0 \\ 0 \\ 1 \end{psmallmatrix}}$ e' un insieme di generatori di $\widetilde{V}$. Inoltre sono anche indipendenti (poiche' hanno pivot ad altezze diverse), dunque $\tilde{\gamma}$ e' una base di $\widetilde{V}$. 
            
        Tornando tramite la base $\alpha$ allo spazio iniziale otteniamo che \[
            \gamma = \ang{\begin{pmatrix} 0 & 1 \\ 1 & 0 \end{pmatrix}, \begin{pmatrix} 1 & 0 \\ 0 & 1 \end{pmatrix}}  
        \] e' una base di $\ker f$, dunque $\dim \ker f = 2$.
    \end{enumerate}
\end{solution}