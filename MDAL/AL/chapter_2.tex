\chapter{Spazi vettoriali}

\section{Spazi vettoriali e prime proprieta'}

\subsection{Spazi vettoriali}
\begin{definition}
    Si dice \textbf{spazio vettoriale su un campo $\K$} un insieme $V$ di elementi, detti \textbf{vettori}, insieme con due operazioni $+ : V \times V \to V$ e $\cdot : \K \times V \to V$ e un elemento $\bm{0_V} \in V$ che soddisfano i seguenti assiomi:
    \[\forall \bm{v}, \bm{w}, \bm{u} \in V, \quad\forall h, k \in \K \]
    \begin{align}
        &\text{1.} &&(\bm{v} + \bm{w}) \in V                                        &\text{(chiusura di V rispetto a $+$)} \\      
        &\text{2.} &&\bm{v} + \bm{w} = \bm{w} + \bm{v}                              &\text{(commutativita' di $+$)} \\
        &\text{3.} &&(\bm{v} + \bm{w}) + \bm{u} = \bm{w} + (\bm{v} + \bm{u})        &\text{(associativita' di $+$)} \\
        &\text{4.} &&\bm{0_V} + \bm{v} = \bm{v} + \bm{0_V} = \bm{v}                 &\text{($\bm{0_V}$ el. neutro di $+$)} \\
        &\text{5.} &&\exists (-\bm{v}) \in V. \quad\bm{v} + (\bm{-v}) = \bm{0_V}    &\text{(opposto per $+$)} \\
        &\text{6.} &&k\bm{v} \in V                                                  &\text{(chiusura di V rispetto a $\cdot$)} \\
        &\text{7.} &&k(\bm{v} + \bm{w}) = k\bm{v} + k\bm{w}                         &\text{(distributivita' 1)} \\
        &\text{8.} &&(k + h)\bm{v}= k\bm{v} + h\bm{v}                               &\text{(distributivita' 2)} \\
        &\text{9.} &&(kh)\bm{v}= k(h\bm{v})                                         &\text{(associativita' di $\cdot$)} \\
        &\text{10.}&&1\bm{v}= \bm{v}                                                &\text{(1 el. neutro di $\cdot$)}
    \end{align}
\end{definition}
 
Spesso il campo $\K$ su cui e' definito uno spazio vettoriale $V$ e' il campo dei numeri reali $\R$ o il campo dei numeri complessi $\C$. Supporremo che gli spazi vettoriali siano definiti su $\R$ a meno di diverse indicazioni. Le definizioni valgono comunque in generale anche su campi $\K$ diversi da $\R$ o $\C$.

\begin{example}
    Possiamo fare diversi esempi di spazi vettoriali. Ad esempio sono spazi vettoriali:
    \begin{enumerate}
        \item i vettori geometrici dove:
        \begin{itemize}
            \item l'elemento neutro e' il vettore nullo;
            \item la somma e' definita tramite la regola del parallelogramma;
            \item il prodotto per scalare e' definito nel modo usuale;
        \end{itemize}
        \item i vettori colonna $n \times 1$ o i vettori riga $1 \times n$ dove:
        \begin{itemize}
            \item l'elemento neutro e' il vettore composto da $n$ elementi $0$;
            \item la somma e' definita come somma tra componenti;
            \item il prodotto per scalare e' definito come prodotto tra lo scalare e ciascuna componente;
        \end{itemize}
        \item le matrici $n \times m$, indicate con $\M_{n \times m}(\K)$;
        \item i polinomi di grado minore o uguale a $n$, indicati con $\K[x]^{\leq n}$;
        \item tutti i polinomi, indicati con $\K[x]$.
    \end{enumerate}
\end{example}

\begin{definition}
    Sia $V$ uno spazio vettoriale, $A \subset V$. Allora si dice che $A$ e' un sottospazio vettoriale di $V$ (o semplicemente sottospazio) se
    \begin{align}
        &\bm{0_V} \in A \\
        &(\bm{v} + \bm{w}) \in A    &&\forall \bm{v}, \bm{w} \in A \\
        &(k\bm{v}) \in A            &&\forall k \in \R, \bm{v} \in A
    \end{align}
\end{definition}

\begin{proposition}
    Le soluzioni di un sistema omogeneo $A\bm{x} = \bm{0}$ con $n$ variabili formano un sottospazio di $\R^n$.
\end{proposition}
\begin{proof}
    Chiamiamo $S$ l'insieme delle soluzioni. Dato che le soluzioni sono vettori colonna di $n$ elementi, $S \subset \R^n$. Verifichiamo ora le condizioni per cui $S$ e' un sottospazio di $\R^n$:
    \begin{enumerate}
        \item $\bm{0}$ appartiene a $S$, poiche' $A\bm{0} = \bm{0}$;
        \item Se $\bm{x}, \bm{y}$ appartengono ad $S$, allora $A(\bm{x} + \bm{y}) = A\bm{x} + A\bm{y} = \bm{0} + \bm{0} = \bm{0}$, dunque $\bm{x} + \bm{y} \in S$;
        \item Se $\bm{x}$ appartiene ad $S$, allora $A(k\bm{x}) = kA\bm{x} = k\bm{0} = \bm{0}$, dunque $k\bm{x} \in S$.
    \end{enumerate}
    Dunque $S$ e' un sottospazio di $\R^n$.
\end{proof}

\subsection{Combinazioni lineari e span}
\begin{definition}
    Sia $V$ uno spazio vettoriale e $\bm{v_1}, \bm{v_2}, \dots, \bm{v_n} \in V$. Allora il vettore $\bm{v} \in V$ si dice combinazione lineare di $\bm{v_1}, \bm{v_2}, \dots, \bm{v_n}$ se 
    \begin{equation}
        \bm{v}= a_1\bm{v_1} + a_2\bm{v_2} + \dots + a_n\bm{v_n} 
    \end{equation}
    per qualche $a_1, a_2, \dots, a_n \in \R$.
\end{definition}

\begin{definition}
    Sia $V$ uno spazio vettoriale e $\bm{v_1}, \dots, \bm{v_n} \in V$. Si indica con $\Span{\bm{v_1}, \dots, \bm{v_n}}$ l'insieme dei vettori che si possono ottenere come combinazione lineare di $\bm{v_1}, \dots, \bm{v_n}$:
    \begin{equation}
        \Span{\bm{v_1}, \dots, \bm{v_n}} = \left\{a_1\bm{v_1} + \dots + a_n\bm{v_n} \mid a_1, \dots, a_n \in \R\right\}
    \end{equation}
\end{definition}

\begin{proposition}
    Sia $A \in \M_{n \times m}(\R)$ e siano $\bm{a_1}, \bm{a_2}, \dots, \bm{a_m} \in \R^n$ le sue colonne. Allora l'immagine della matrice e' uguale allo span delle sue colonne.
\end{proposition}
\begin{proof}
    L'immagine della matrice e' l'insieme di tutti i vettori del tipo $A \cdot \begin{pmatrix}
        x_1 & \dots & x_m
    \end{pmatrix}^T$ al variare di $x_1, \dots, x_m \in \R$. 
    \begin{alignat*}
        {1}
        A\begin{pmatrix} x_1 \\ \vdots \\ x_m \end{pmatrix}
            &= A\left(\begin{pmatrix} x_1 \\ \vdots \\ 0 \end{pmatrix} + \dots + \begin{pmatrix} 0 \\ \vdots \\ x_m \end{pmatrix}\right)\\
            &= A\left(x_1\begin{pmatrix} 1 \\ \vdots \\ 0 \end{pmatrix} + \dots + x_m\begin{pmatrix} 0 \\ \vdots \\ 1 \end{pmatrix}\right)\\
            &= x_1A\begin{pmatrix} 1 \\ \vdots \\ 0 \end{pmatrix} + \dots + x_mA\begin{pmatrix} 0 \\ \vdots \\ 1 \end{pmatrix}\\
        \intertext{Ma sappiamo per la proposizione \ref{j-esima_colonna} che moltiplicare una matrice per un vettore che contiene tutti $0$ tranne un $1$ in posizione $j$ ci da' come risultato la $j$-esima colonna della matrice, dunque:}
            &= x_1\bm{a_1} + \dots + x_m\bm{a_m} 
    \end{alignat*}
    Ma i vettori che appartengono allo span delle colonne di $A$ sono tutti e solo del tipo $x_1\bm{a_1} + \dots + x_m\bm{a_m}$, dunque $\Imm{A} = \Span{\bm{a_1}, \bm{a_2}, \dots, \bm{a_m}}$, come volevasi dimostrare.
\end{proof}

\begin{proposition}
    Sia $V$ uno spazio vettoriale, $\bm{v_1}, \dots, \bm{v_n} \in V$. Allora $A = \Span{v_1, \dots, v_n} \subset V$ e' un sottospazio di $V$.
\end{proposition}
\begin{proof}
    Dimostriamo che valgono le tre condizioni per cui $A$ e' un sottospazio di $V$:
    \begin{enumerate}
        \item $\bm{0_V}$ appartiene ad $A$, in quanto basta scegliere $a_1 = \dots = a_n = 0$;
        \item Siano $\bm{v}, \bm{w} \in A$. Allora per qualche $a_1, \dots, a_n, b_1, \dots, b_n \in \R$ vale che \begin{alignat*}{1}
            \bm{v} + \bm{w} &= (a_1\bm{v_1} + \dots + a_n\bm{v_n}) + (b_1\bm{v_1} + \dots + b_n\bm{v_n}) \\
            &= (a_1 + b_1)\bm{v_1} + \dots + (a_n + b_n)\bm{v_n} \in A
        \end{alignat*}
        \item Siano $\bm{v} \in A, k \in \R$. Allora per qualche $a_1, \dots, a_n \in \R$ vale che \begin{alignat*}{1}
            k\bm{v} &= k(a_1\bm{v_1} + \dots + a_n\bm{v_n})  \\
            &= (ka_1)\bm{v_1} + \dots + (ka_n)\bm{v_n} \in A
        \end{alignat*}
    \end{enumerate}
    cioe' $A$ e' un sottospazio di $V$.
\end{proof}

Vale anche l'implicazione inversa: ogni sottospazio di $V$ puo' essere descritto come span di alcuni suoi vettori.

\begin{proposition}
    Ogni sottospazio vettoriale di $R^n$ puo' essere descritto in due forme:
    \begin{itemize}
        \item forma parametrica: come span di alcuni vettori, cioe' come immagine di una matrice;
        \item forma cartesiana: come insieme delle soluzioni di un sistema lineare omogeneo, cioe' come kernel di una matrice.
    \end{itemize}
\end{proposition}
\begin{remark}
    Per essere piu' precisi dovremmo parlare di immagine e di kernel dell'applicazione lineare associata alla matrice. 
\end{remark}
\begin{example}
    Consideriamo il sottospazio di $R^3$ generato dall'insieme delle soluzioni dell'equazione $3x + 4y + 5z = 0$ (forma cartesiana) e chiamiamolo $W$.
    Cerchiamo di esprimere $W$ in forma parametrica: \begin{alignat*}{1}
        W &= \left\{ \begin{pmatrix} x \\ y \\ z \end{pmatrix} \in \R^3 \mid 3x + 4y + 5z = 0\right\} \\
        \intertext{Scegliamo $y, z$ libere, da cui segue $x = -\frac{4}{3}y -\frac{5}{3}z$. Sostituendolo otteniamo: }
        &= \left\{ \begin{pmatrix} -\frac{4}{3}y -\frac{5}{3}z \\ y \\ z \end{pmatrix} \mid y, z \in \R \right\}\\
        &= \left\{ y\begin{pmatrix} -\frac{4}{3} \\ 1 \\ 0 \end{pmatrix} + z\begin{pmatrix} -\frac{5}{3} \\ 0 \\ 1 \end{pmatrix} \mid y, z \in \R \right\}\\
        &= \left\{ y\begin{pmatrix} -\frac{4}{3} \\ 1 \\ 0 \end{pmatrix} + z\begin{pmatrix} -\frac{5}{3} \\ 0 \\ 1 \end{pmatrix} \mid y, z \in \R \right\}\\
        &= \Span{\begin{pmatrix} -\frac{4}{3} \\ 1 \\ 0 \end{pmatrix}; \begin{pmatrix} -\frac{5}{3} \\ 0 \\ 1 \end{pmatrix}}
    \end{alignat*}

    Se torniamo indietro notiamo che \begin{alignat*}{1}
        W &= \left\{ \begin{pmatrix} -\frac{4}{3}y -\frac{5}{3}z \\ y \\ z \end{pmatrix} \mid y, z \in \R \right\}\\
        &= \left\{ \begin{pmatrix} -\frac{4}{3}y -\frac{5}{3}z \\ y + 0z \\ 0y+z \end{pmatrix} \mid y, z \in \R \right\}\\
        &= \left\{ \begin{pmatrix} -\frac{4}{3} & -\frac{5}{3} \\ 1 & 0 \\ 0 & 1 \end{pmatrix}\begin{pmatrix} y \\ z \end{pmatrix} \mid y, z \in \R \right\}
    \end{alignat*}
    che e' la definizione di immagine della matrice $\begin{psmallmatrix}
        -\frac{4}{3} & -\frac{5}{3} \\ 1 & 0 \\ 0 & 1 
    \end{psmallmatrix}$.

    Dunque $W = \Imm{\begin{psmallmatrix}
        -\frac{4}{3} & -\frac{5}{3} \\ 1 & 0 \\ 0 & 1 
    \end{psmallmatrix}}$.
\end{example}
\begin{example}
    Consideriamo il sottospazio di $R^3$ generato dallo span dei vettori $\begin{psmallmatrix} 1 \\ 2 \\ 3 \end{psmallmatrix}$, $\begin{psmallmatrix} 4 \\ 5 \\ 6 \end{psmallmatrix}$ (forma parametrica) e chiamiamolo $W$.
    Cerchiamo di esprimere $W$ in forma cartesiana: \begin{alignat*}{1}
        W &= \Span{\begin{pmatrix} 1 \\ 2 \\ 3 \end{pmatrix}, \begin{pmatrix} 4 \\ 5 \\ 6 \end{pmatrix}} \\
            &= \left\{ \begin{pmatrix} x \\ y \\ z \end{pmatrix} \in \R^3 \mid \exists a, b \in \R. \begin{pmatrix} x \\ y \\ z \end{pmatrix} = a\begin{pmatrix} 1 \\ 2 \\ 3 \end{pmatrix} + b\begin{pmatrix} 4 \\ 5 \\ 6 \end{pmatrix} \right\}\\
            &= \left\{ \begin{pmatrix} x \\ y \\ z \end{pmatrix} \in \R^3 \mid \exists a, b \in \R. \begin{pmatrix} x \\ y \\ z \end{pmatrix} = \begin{pmatrix} a + 4b \\ 2a+5b \\ 3a+6b \end{pmatrix}\right\}\\
            &= \left\{ \begin{pmatrix} x \\ y \\ z \end{pmatrix} \in \R^3 \mid \exists a, b \in \R. \begin{pmatrix} x \\ y \\ z \end{pmatrix} = \begin{pmatrix} 1 & 4 \\ 2&5 \\ 3&6 \end{pmatrix} \begin{pmatrix}a \\ b\end{pmatrix}\right\}
    \end{alignat*}
    dunque e' sufficiente capire in che casi il sistema ha soluzione.
    Risolviamo il sistema e imponiamo che non ci siano equazioni impossibili:
    \begin{gather*}
        \begin{pmatrix}[cc|c]
            1&4&x \\ 2&5&y \\ 3&6&z 
        \end{pmatrix} \xrightarrow[R_2 - 2R_1]{R_3 - 3R_1}
        \begin{pmatrix}[cc|c]
            1&4&x \\ 0&-3&y-2x \\ 0&-6&z-3x 
        \end{pmatrix} \xrightarrow[R_3 - 2R_2]{}
        \begin{pmatrix}[cc|c]
            1&4&x \\ 0&-3&y-2x \\ 0&0&x-2y+z 
        \end{pmatrix}
    \end{gather*}
    Dato che non devono esserci equazioni impossibili, segue che tutti i vettori di $W$ sono della forma $\begin{psmallmatrix}x\\y\\z\end{psmallmatrix}$ con $x - 2y + z = 0$. Dunque \[
        W = \left\{ \begin{pmatrix}
            x\\y\\z
        \end{pmatrix}\in \R^3 \mid x-2y+z = 0\right\}    
    \] e' la forma cartesiana di $W$.

    Notiamo che dire che $x, y, z \in \R$ sono tali che $x-2y+z = 0$ e' equivalente a dire che \[
        \begin{pmatrix}
            1 &-2 &1
        \end{pmatrix} \cdot \begin{pmatrix}
            x \\ y \\ z
        \end{pmatrix} = 0
    \]
    cioe' $W$ e' formato da tutti e solo i vettori che fanno parte del kernel della matrice $A = \begin{pmatrix} 1 &-2 &1 \end{pmatrix}$, cioe' $W = \ker \begin{pmatrix} 1 &-2 &1 \end{pmatrix}$.
\end{example}

\begin{definition}
    Sia $V$ uno spazio vettoriale, $\bm{v_1}, \dots, \bm{v_n} \in V$. Allora l'insieme $\left\{ \bm{v_1}, \dots, \bm{v_n} \right\}$ si dice insieme di vettori linearmente indipendenti se
    \begin{equation}
        a_1\bm{v_1} + \dots + a_n\bm{v_n} = \bm{0_V} \iff a_1 = \dots = a_n = 0
    \end{equation}
    cioe' se l'unica combinazione lineare di $\bm{v_1}, \dots, \bm{v_n}$ che da' come risultato il vettore nullo e' quella con $a_1 = \dots = a_n = 0$.
\end{definition}

Possiamo usare una definizione alternativa di dipendenza lineare, equivalente alla precedente, tramite questa proposizione:
\begin{proposition}
    Sia $V$ uno spazio vettoriale, $\bm{v_1}, \dots, \bm{v_n} \in V$. Allora l'insieme dei vettori $\left\{ \bm{v_1}, \dots, \bm{v_n} \right\}$ e' linearmente dipendente se e solo se almeno uno di essi e' esprimibile come combinazione lineare degli altri. 
\end{proposition}
\begin{proof}
    Dimostriamo entrambi i versi dell'implicazione.
    \begin{itemize}
        \item Supponiamo che $\left\{ \bm{v_1}, \dots, \bm{v_n} \right\}$ sia linearemente dipendente, cioe' che esistano $a_1, \dots, a_n$ non tutti nulli tali che \[
            a_1\bm{v_1} + a_2\bm{v_2} + \dots + a_n\bm{v_n} = \bm{0_V}   
        .\]
        Supponiamo senza perdita di generalita' $a_1 \neq 0$, allora segue che \[
            \bm{v_1} = -\frac{a_2}{a_1}\bm{v_1} - \dots - \frac{a_n}{a_1}\bm{v_n}
        \]
        dunque $\bm{v_1}$ puo' essere espresso come combinazione lineare degli altri vettori.
        \item Supponiamo che il vettore $\bm{v_1}$ sia esprimibile come combinazione lineare degli altri (senza perdita di generalita'), cioe' che esistano $k_2, \dots, k_n \in \R$ tali che \[
            \bm{v_1} = k_2\bm{v_2} + \dots + k_n\bm{v_n}
        .\]
        Consideriamo una generica combinazione lineare di $v_1, v_2, \dots, v_n$:
        \begin{alignat*}
            {1}
            & a_1\bm{v_1} + a_2\bm{v_2} + \dots + a_n\bm{v_n} \\
            = & a_1(k_2\bm{v_2} + \dots + k_n\bm{v_n}) + a_2\bm{v_2} + \dots + a_n\bm{v_n} \\
            = & (a_1k_2 + a_2)\bm{v_2} + \dots + (a_1k_n + a_n)\bm{v_n}
        \end{alignat*}
        Se scegliamo $a_1 \in \R$ libero, $a_i = -a_1k_i$ per ogni $2 \leq i \leq n$, otterremo
        \begin{alignat*}{1}
            & (a_1k_2 + a_2)\bm{v_2} + \dots + (a_1k_n + a_n)\bm{v_n} \\
            = & (a_1k_2 - a_1k_2)\bm{v_2} + \dots + (a_1k_n - a_1k_n)\bm{v_n} \\
            = & 0\bm{v_2} + \dots + 0\bm{v_n} \\
            = & \bm{0_V}
        \end{alignat*}
        dunque esiste una scelta dei coefficienti $a_1, a_2, \dots, a_n$ diversa da $a_1 = \dots = a_n = 0$ per cui la combinazione lineare da' come risultato il vettore nullo, cioe' l'insieme dei vettori non e' linearmente indipendente.
    \end{itemize}
    
\end{proof}

Inoltre per comodita' spesso si dice che i vettori $\bm{v_1}, \dots, \bm{v_n}$ sono indipendenti, invece di dire che l'insieme formato da quei vettori e' un insieme linearmente indipendente.

\begin{proposition} \label{span_Gauss}
    Sia $V$ uno spazio vettoriale e $\bm{v_1}, \dots, \bm{v_n} \in V$. Allora per ogni $k \in \R$ e per ogni $i, j \leq n$.
    \begin{equation}
        \Span{\bm{v_1}, \dots, \bm{v_i}, \bm{v_j}, \dots, \bm{v_n}} = \Span{\bm{v_1}, \dots, \bm{v_i} + k\bm{v_j}, \bm{v_j}, \dots, \bm{v_n}}.
    \end{equation}
\end{proposition}
\begin{proof}
    Supponiamo che $v \in \Span{\bm{v_1}, \dots, \bm{v_i}, \bm{v_j}, \dots, \bm{v_n}}$. Allora per definizione esisteranno $a_1, \dots, a_n \in \R$ tali che
    \begin{alignat*}{1}
        v &= a_1\bm{v_1} + \dots + a_i\bm{v_i} + a_j\bm{v_j} + \dots + a_n\bm{v_n} \\
        \intertext{Aggiungiamo e sottraiamo $a_ik\bm{v_j}$ al secondo membro.}
        &= a_1\bm{v_1} + \dots + a_i\bm{v_i} + a_j\bm{v_j} + \dots + a_n\bm{v_n} + a_ik\bm{v_j} - a_ik\bm{v_j}\\
        &= a_1\bm{v_1} + \dots + a_i\bm{v_i} + a_ik\bm{v_j} + a_j\bm{v_j} - a_ik\bm{v_j} + \dots + a_n\bm{v_n}\\
        &= a_1\bm{v_1} + \dots + a_i(\bm{v_i} + k\bm{v_j}) + (a_j - a_ik)\bm{v_j} + \dots + a_n\bm{v_n}\\
        \implies v &\in \Span{\bm{v_1}, \dots, \bm{v_i} + k\bm{v_j}, \bm{v_j}, \dots, \bm{v_n}}. 
    \end{alignat*}

    Si dimostra l'altro verso nello stesso modo.

    Dunque in entrambi gli insiemi ci sono gli stessi elementi, cioe' i due span sono uguali.
\end{proof}

Notiamo inoltre che se scambiamo due vettori o se moltiplichiamo un vettore per uno scalare otteniamo uno span equivalente a quello di partenza. Quindi possiamo "semplificare" uno span di vettori tramite mosse di Gauss per colonna, come suggerisce la prossima proposizione.

\begin{proposition} \label{span_colonne_indipendenti}
    Siano $\bm{v_1}, \dots, \bm{v_n} \in \R^m$ dei vettori colonna. Allora per stabilire quali di questi vettori sono indipendenti consideriamo la matrice $A$ che contiene come colonna $i$-esima il vettore colonna $v_i$ e riduciamola a scalini per colonna. Lo span delle colonne non nulle della matrice ridotta a scalini e' uguale allo span di $\bm{v_1}, \dots, \bm{v_n}$.
\end{proposition}
\begin{proof} 
    Consideriamo la matrice $\bar{A}$ ridotta a scalini. Allora per la proposizione \ref{span_Gauss} lo span delle sue colonne e' uguale allo span dei vettori iniziali. 

    Tutte le colonne nulle possono essere eliminate da questo insieme, in quanto il vettore nullo e' sempre linearmente dipendente.

    Le colonne rimanenti sono sicuramente linearmente indipendenti: infatti dato che la matrice e' a scalini per colonna per annullare il primo pivot dobbiamo annullare il primo vettore, per annullare il secondo dobbiamo annullare il secondo e cosi' via. Dunque lo span dei vettori colonna non nulli rimanenti e' uguale allo span dei vettori iniziali.
\end{proof}

Notiamo che alla fine di questo procedimento otteniamo vettori colonna che sono diversi dai vettori iniziali, ma questi vettori hanno pivot ad "altezze diverse".

\begin{example}
    Siano $\bm{v_1}, \bm{v_2}, \bm{v_3}, \bm{v_4} \in \R^3$ tali che \[
        \bm{v_1} = \begin{pmatrix}
            1 \\ 2 \\ 3
        \end{pmatrix}, \bm{v_2} = \begin{pmatrix}
            3 \\ 7 \\ 4
        \end{pmatrix}, \bm{v_3} = \begin{pmatrix}
            2 \\ 4 \\ 6
        \end{pmatrix}, \bm{v_4} = \begin{pmatrix}
            -1 \\ 7 \\ 2
        \end{pmatrix}.
    \] Si trovi un insieme di vettori di $\R^3$ indipendenti con lo stesso span di $\bm{v_1}, \bm{v_2}, \bm{v_3}, \bm{v_4}$.
\end{example}
\begin{solution}
    Per la proposizione precedente mettiamo i vettori come colonne di una matrice e semplifichiamola tramite mosse di colonna:
    \begin{gather*}
        \begin{pmatrix}[c|c|c|c]
            1 & 3 & 2 & -1 \\ 2 & 7 & 4 & 7 \\ 3 & 4 & 6 & 2
        \end{pmatrix} \xrightarrow[C_4 + C_1]{C_2 - 3C_1, C_3 - 2C_1} \begin{pmatrix}
            [c|c|c|c]
            1 & 0 & 0 & 0 \\ 2 & 1 & -2 & 1 \\ 3 & -5 & -3 & -7
        \end{pmatrix} \\ 
        \xrightarrow[C_4 - C_2]{C_3 + 2C_2} \begin{pmatrix}
            [c|c|c|c]
            1 & 0 & 0 & 0 \\ 2 & 1 & 0 & 0 \\ 3 & -5 & -13 & -2
        \end{pmatrix} \xrightarrow[]{C_4 - \frac{2}{13}C_3} \begin{pmatrix}
            [c|c|c|c]
            1 & 0 & 0 & 0 \\ 2 & 1 & 0 & 0 \\ 3 & -5 & -13 & 0
        \end{pmatrix}
    \end{gather*}
    Dunque i vettori $\bm{w_1} = \begin{psmallmatrix} 1 \\ 2 \\ 3 \end{psmallmatrix}, \bm{w_2} = \begin{psmallmatrix} 0 \\ 1 \\ -5 \end{psmallmatrix}, \bm{w_3} = \begin{psmallmatrix} 0 \\ 0 \\ -13 \end{psmallmatrix}$ sono indipendenti e per la proposizione precedente vale che \[
        \Span{\bm{v_1}, \bm{v_2}, \bm{v_3}, \bm{v_4}} = \Span{\bm{w_1}, \bm{w_2}, \bm{w_3}}.
    \]
\end{solution}

\subsection{Generatori e basi}
\begin{definition}
    Sia $V$ uno spazio vettoriale, $\bm{v_1}, \dots, \bm{v_n} \in V$. Allora si dice che ${\bm{v_1}, \dots, \bm{v_n}}$ e' un insieme di generatori di $V$, oppure che l'insieme ${\bm{v_1}, \dots, \bm{v_n}}$ genera $V$, se
    \begin{equation}
        \Span{\bm{v_1}, \dots, \bm{v_n}} = V.
    \end{equation}
\end{definition}

Per comodita' spesso si dice che i vettori $\bm{v_1}, \dots, \bm{v_n}$ sono generatori di $V$, invece di dire che l'insieme formato da quei vettori e' un insieme di generatori.

\begin{definition}
    Sia $V$ uno spazio vettoriale, $\bm{v_1}, \dots, \bm{v_n} \in V$. Allora si dice che $\mathcal{B} = \ang{\bm{v_1}, \dots, \bm{v_n}}$ e' una base di $V$ se
    \begin{itemize}
        \item i vettori $\bm{v_1}, \dots, \bm{v_n}$ generano $V$;
        \item i vettori $\bm{v_1}, \dots, \bm{v_n}$ sono linearmente indipendenti.
    \end{itemize}
\end{definition}

\begin{definition}
    Sia $V$ uno spazio vettoriale. Allora il numero di vettori in una sua base si dice dimensione dello spazio vettoriale $V$, e si indica con $\dim V$.
\end{definition}

Sapendo che un insieme di vettori genera un sottospazio di $\R^n$ (o $\R^n$ stesso) si puo' trovare una base del sottospazio (o di $\R^n$) disponendo i vettori come colonne di una matrice e semplificandoli, come abbiamo visto in precedenza. Tuttavia se vogliamo \textbf{estrarre una base} dal nostro insieme di vettori allora possiamo procedere in un modo leggermente diverso, che utilizza le mosse di Gauss per riga.

\begin{proposition}\label{estrarre_una_base}
    Siano $\bm{v_1}, \dots, \bm{v_m} \in \R^n$ dei vettori che generano $V \subseteq \R^n$ sottospazio di $\R^n$. Allora possiamo porre i vettori come colonne di una matrice e ridurla a scalini per riga. Alla fine del procedimento i vettori che originariamente erano nelle colonne con i pivot sono indipendenti e generano $V$, dunque formano una base di $V$.
\end{proposition}
\begin{proof}
    Consideriamo i $k$ vettori indipendenti che sono nell'insieme $\bm{v_1}, \dots, \bm{v_m}$ e chiamiamoli $\bm{w_1}, \dots, \bm{w_k}$.
    Consideriamo una loro combinazione lineare qualunque $x_1\bm{w_1} + \dots + x_k\bm{w_k}$ e la poniamo uguale a $\bm{0}$; questo e' equivalente a dire \[
        A\begin{pmatrix}
            x_1 \\ \vdots \\ x_k
        \end{pmatrix} = \bm{0}
    \] dove $A$ e' la matrice le cui colonne sono i vettori $\bm{w_1}, \dots, \bm{w_k}$. 
    
    Dato che i $k$ vettori sono indipendenti l'unica soluzione di questo sistema e' il vettore nullo, dunque il sistema ha una sola soluzione e quindi deve avere $0$ variabili libere, cioe' il numero di pivot della matrice ridotta a scalini deve essere uguale al numero di colonne.

    Se aggiungessimo vettori non indipendenti a questo insieme per definizione di dipendenza lineare allora non avremmo piu' una singola soluzione, dunque le colonne che abbiamo aggiunto non possono contenere pivot.
\end{proof}

Notiamo che alla fine del procedimento non otteniamo dei vettori colonna che generano il nostro sottospazio, ma dobbiamo andarli a scegliere dall'insieme iniziale: in questo senso possiamo estrarre una base da un insieme di generatori.

\begin{example}
    Sia $V \subseteq \R^4$ tale che $V = \Span{\bm{c_1}, \bm{c_2}, \bm{c_3}, \bm{c_4}}$ dove \[
        \bm{c_1} = \begin{pmatrix}
            2 \\ 0 \\ 1 \\ 1
        \end{pmatrix}, \bm{c_2} = \begin{pmatrix}
            3 \\ -2 \\ -2 \\ 0
        \end{pmatrix}, \bm{c_3} = \begin{pmatrix}
            1 \\ 0 \\ -1 \\ 1
        \end{pmatrix}, \bm{c_4} = \begin{pmatrix}
            0 \\ 1 \\ -2 \\ \frac{1}{3}
        \end{pmatrix}.
    \] Si estragga una base di $V$ da questi quattro vettori.
\end{example}
\begin{solution}
    Utilizziamo il metodo proposto dalla proposizione precedente. 
    \begin{gather*}
        \begin{pmatrix}
            2&3&1&0\\0&-2&0&1\\1&-2&-1&-2\\1&0&\frac{1}{3}&\frac13
        \end{pmatrix} \xrightarrow[R_4 - \frac{1}{2}R_2]{R_3 - \frac{1}{2}R_2}
        \begin{pmatrix}
            2&3&1&0\\0&-2&0&1\\0&-\frac72&-\frac32&-2\\0&-\frac{3}{2}&\frac{1}{6}&\frac13
        \end{pmatrix} \xrightarrow[R_4 \times 6]{R_2\times \frac12, R_3\times 2} \\
        \begin{pmatrix}
            2&3&1&0\\0&-1&0&\frac12\\0&-7&-3&-2\\0&-9&1&2
        \end{pmatrix} \xrightarrow[R_4-9R_2]{R_3 -7R_2} 
        \begin{pmatrix}
            2&3&1&0\\0&-1&0&\frac12\\0&0&-3&-\frac{15}{2}\\0&0&-1&-\frac{5}{2}
        \end{pmatrix}\\ \xrightarrow[]{R_4 -\frac13R_3} 
        \begin{pmatrix}
            2&3&1&0\\0&-1&0&\frac12\\0&0&-3&-\frac{15}{2}\\0&0&0&0
        \end{pmatrix}.
    \end{gather*}
    Notiamo dunque che i pivot sono nelle colonne $1$, $2$ e $3$, che corrispondono ai vettori $\bm{c_1}, \bm{c_2}, \bm{c_3}$ che per la proposizione precedente sono indipendenti e generano $V$, dunque $\ang{\bm{c_1}, \bm{c_2}, \bm{c_3}}$ e' una base di $V$.
\end{solution}

\begin{proposition}\label{base=dim_gener_indip}
    Sia $V$ uno spazio vettoriale e sia $\left\{\bm{v_1}, \dots, \bm{v_n} \right\}$ un insieme di $n$ vettori di $V$. Se valgono due dei seguenti fatti
    \begin{itemize}
        \item $n = \dim V$;
        \item $\left\{\bm{v_1}, \dots, \bm{v_n} \right\}$ e' un insieme di generatori di $V$;
        \item $\left\{\bm{v_1}, \dots, \bm{v_n} \right\}$ sono linearmente indipendenti;
    \end{itemize}
    allora vale anche il terzo e $\ang{\bm{v_1}, \dots, \bm{v_n}}$ e' una base di $V$.
\end{proposition}

\begin{example}
    Consideriamo lo spazio dei polinomi di grado minore o uguale a due $\R[x]^{\leq 2}$. Mostrare che $\alpha = \ang{1, (x-1), (x-1)^2}$ e' una base di $\R[x]^{\leq 2}$.
\end{example}
\begin{solution}
    Sappiamo che la base standard di $\R[x]^{\leq 2}$ e' la base $\ang{1, x, x^2}$, dunque $\dim \left( \R[x]^{\leq 2} \right) = 3$. Dato che la base $\alpha$ ha esattamente $3$ vettori, per la proposizione \ref{base=dim_gener_indip} ci basta dimostrare una tra:
    \begin{itemize}
        \item i tre vettori sono indipendenti;
        \item i tre vettori generano $\R[x]^{\leq 2}$.
    \end{itemize}
    Per esercizio, verifichiamole entrambe.
    \begin{itemize}
        \item Verifichiamo che sono linearmente indipendenti: consideriamo una generica combinazione lineare dei tre vettori e poniamola uguale al vettore $\bm{0} = 0 + 0x + 0x^2$. \begin{alignat*}{1}
            a \cdot 1 + b \cdot (x - 1) + c \cdot (x - 1)^2 &= 0+0x+0x^2 \\
            \iff a + bx - b + cx^2 -2cx + c &= 0+0x+0x^2 \\
            \iff (a-b+c) +(b-2c)x + cx^2 &= 0+0x+0x^2
        \end{alignat*} Dunque $a, b, c$ devono soddisfare il seguente sistema:
        \begin{equation*}
            \left\{
            \begin{array}{@{}rororor }
            a & - & b & + & c  & = & 0 \\
              &   & b & - & 2c & = & 0 \\
              &   &   &   & c  & = & 0 \\
            \end{array}  
            \right.
        \end{equation*}
        che ha soluzione solo per $a = b = c = 0$. Dunque i tre vettori sono indipendenti e, sapendo che $\dim \left( \R[x]^{\leq 2} \right) = 3$, sono una base di $\R[x]^{\leq 2}$.
        \item Verifichiamo che i tre vettori generano $\R[x]^{\leq 2}$. Un modo per farlo e' verificare che i vettori che compongono la base canonica di $\R[x]^{\leq 2}$ sono nello span di $\{1, (x-1), (x-1)^2\}$: infatti, dato che la base canonica genera tutto lo spazio, se essa e' nello span anche tutto il resto dello spazio sara' nello span dei nostri tre vettori.
        \begin{itemize}
            \item $1 = 1\cdot 1 + 0\cdot (x-1) + 0 \cdot (x-1)^2$, dunque $1 \in \Span{1, (x-1), (x-1)^2}$
            \item $x = 1\cdot 1 + 1\cdot (x-1) + 0 \cdot (x-1)^2$, dunque $x \in \Span{1, (x-1), (x-1)^2}$
            \item Dato che non e' immediato vedere come scrivere $x^2$ in termini di $1, (x-1), (x-1)^2$ cerchiamo di trovare i coefficienti algebricamente:
            \begin{alignat*}{1}
                x^2 &= a \cdot 1 + b\cdot (x-1) + c \cdot (x-1)^2 \\
                    &= (a-b+c) +(b-2c)x + cx^2
            \end{alignat*}
            dunque uguagliando i coefficienti dei termini dello stesso grado otteniamo
            \begin{equation*}
                \left\{\begin{array}{@{}rororor }
                    a & - & b & + & c  & = & 0 \\
                      &   & b & - & 2c & = & 0 \\
                      &   &   &   & c  & = & 1 \\
                \end{array} \right. \implies 
                \left\{\begin{array}{@{}rororor }
                    a & = & 1 \\
                    b & = & 2 \\
                    c & = & 1 \\
                \end{array} \right.
            \end{equation*}
            Quindi $x^2$ e' esprimibile come combinazione lineare di $1, (x-1), (x-1)^2$ (in particolare $x^2 = 1 + 2(x-1) + (x-1)^2$), dunque \[x^2 \in \Span{1, (x-1), (x-1)^2}.\]
        \end{itemize}
        Abbiamo quindi verificato che i vettori che formano la base canonica di $\R[x]^{\leq 2}$ fanno parte dello span dei nostri tre vettori, dunque se la base canonica genera tutto lo spazio anche $\{1, (x-1), (x-1)^2\}$ sono generatori. Inoltre, dato che $\dim (\R[x]^{\leq 2}) = 3$ segue che $\ang{1, (x-1), (x-1)^2}$ e' una base di $\R[x]^{\leq 2}$.
    \end{itemize}
\end{solution}

\begin{definition}
    Sia $V$ uno spazio vettoriale, $\bm{v} \in V$ e $\mathcal{B} = \ang{\bm{v_1}, \dots, \bm{v_n}}$ una base di $V$. Allora si dice vettore delle coordinate di $\bm{v}$ rispetto a $\mathcal{B}$ il vettore colonna
    \begin{equation}
        [\bm{v}]_{\mathcal{B}} = \begin{bmatrix}
                                    a_1 \\
                                    a_2 \\
                                    \vdots \\
                                    a_n
                                 \end{bmatrix} \in \R^n
    \end{equation}
    tale che \begin{equation}
        \bm{v} = a_1\bm{v_1} + \dots + a_n\bm{v_n}
    \end{equation}
\end{definition}

\begin{proposition}
    Sia $V$ uno spazio vettoriale, $\bm{v} \in V$ e $\mathcal{B} = \ang{\bm{v_1}, \dots, \bm{v_n}}$ una base di $V$. Allora le coordinate di $\bm{v}$ rispetto a $\mathcal{B}$ sono uniche.
\end{proposition}
\begin{proof}
    Supponiamo per assurdo che esistano due vettori colonna distinti $\bm{a}$, $\bm{b}$ che rappresentino le coordinate di $\bm{v}$ rispetto a $\mathcal{B}$. Allora
    \begin{alignat*}
        {1}
        \bm{0_V}  &= \bm{v} - \bm{v} \\
                &= (a_1\bm{v_1} + \dots + a_n\bm{v_n}) - (b_1\bm{v_1} + \dots + b_n\bm{v_n}) \\
                &= (a_1 - b_1)\bm{v_1} + \dots + (a_n - b_n)\bm{v_n}
    \end{alignat*}
    Ma per definizione di base $\bm{v_1}, \dots, \bm{v_n}$ sono linearmente indipendenti, dunque l'unica combinazione lineare che da' come risultato il vettore $\bm{0_V}$ e' quella in cui tutti i coefficienti sono $0$. Da cio' segue che
    \begin{gather*}
        a_1 - b_1 = a_2 - b_2 = \dots = a_n - b_n = 0 \\
        \implies \bm{a} = \begin{bmatrix}
            a_1 \\
            a_2 \\
            \vdots \\
            a_n
        \end{bmatrix}
        = 
        \begin{bmatrix}
            b_1 \\
            b_2 \\
            \vdots \\
            b_n
        \end{bmatrix} = \bm{b}
    \end{gather*}
    cioe' i due vettori sono uguali. Ma cio' e' assurdo poiche' abbiamo supposto $\bm{a} \neq \bm{b}$, dunque le coordinate di $\bm{v}$ rispetto a $\mathcal{B}$ devono essere uniche.
\end{proof}

\begin{example}
    Sia $V \subseteq \M_{2 \times 2}(\R)$ tale che $V$ e' il sottospazio delle matrici simmetriche. Trovare una base di $V$ e trovare le coordinate di $\bm{u} = \begin{psmallmatrix}
        3 & 4 \\ 4 & 6
    \end{psmallmatrix} \in V$ rispetto alla base trovata.
\end{example}
\begin{solution}
    Cerco di esprimere un generico vettore $\bm{v} \in V$ in termini della condizione che definisce il sottospazio.
    \begin{alignat*}
        {1}
        V &= \left\{ \begin{pmatrix} a&b\\b&c \end{pmatrix}\mid a, b, c \in \R\right\} \\
        \intertext{Isolando i contributi di $a$, $b$ e $c$ ottengo}
        &= \left\{ \bm{v} \in \M_{2 \times 2}(\R) \mid \exists a, b, c \in \R. \bm{v} = \begin{pmatrix} a&0\\0&0 \end{pmatrix} + \begin{pmatrix} 0&b\\b&0 \end{pmatrix} + \begin{pmatrix} 0&0\\0&c \end{pmatrix}\right\} \\
        &= \left\{ \bm{v} \in \M_{2 \times 2}(\R) \mid \exists a, b, c \in \R. \bm{v} = a\begin{pmatrix} 1&0\\0&0 \end{pmatrix} + b\begin{pmatrix} 0&1\\1&0 \end{pmatrix} + c\begin{pmatrix} 0&0\\0&1 \end{pmatrix}\right\} \\
        &= \Span{\begin{pmatrix} 1&0\\0&0 \end{pmatrix}, \begin{pmatrix} 0&1\\1&0 \end{pmatrix}, \begin{pmatrix} 0&0\\0&1 \end{pmatrix}} \\
    \end{alignat*}

    Ora dobbiamo mostrare che $\begin{psmallmatrix} 1&0\\0&0 \end{psmallmatrix}, \begin{psmallmatrix} 0&1\\1&0 \end{psmallmatrix}, \begin{psmallmatrix} 0&0\\0&1 \end{psmallmatrix}$ sono linearmente indipendenti. Consideriamo una loro generica combiazione lineare e imponiamola uguale a $0$:
    \begin{gather*}
        x\begin{pmatrix} 1&0\\0&0 \end{pmatrix} + y\begin{pmatrix} 0&1\\1&0 \end{pmatrix} + z\begin{pmatrix} 0&0\\0&1 \end{pmatrix} = \begin{pmatrix} 0&0\\0&0 \end{pmatrix} \\
        \iff \begin{pmatrix} x&y\\y&z \end{pmatrix} = \begin{pmatrix} 0&0\\0&0 \end{pmatrix} \\
        \iff x = y = z = 0.
    \end{gather*}
    Dunque $\mathcal{B} = \ang{\begin{psmallmatrix} 1&0\\0&0 \end{psmallmatrix}, \begin{psmallmatrix} 0&1\\1&0 \end{psmallmatrix}, \begin{psmallmatrix} 0&0\\0&1 \end{psmallmatrix}}$ e' una base di $V$.

    Per trovare le coordinate di $\bm{u}$ esprimiamo in termini della base:
    \begin{equation*}
        \begin{pmatrix}
            3 & 4 \\ 4 & 6
        \end{pmatrix} = 3\begin{pmatrix} 1&0\\0&0 \end{pmatrix} + 4\begin{pmatrix} 0&1\\1&0 \end{pmatrix} + 6\begin{pmatrix} 0&0\\0&1 \end{pmatrix}
    \end{equation*}
    dunque $[\bm{u}]_{\mathcal{B}} = \begin{pmatrix}
        3 \\ 4 \\ 6
    \end{pmatrix}$.
\end{solution}

Notiamo che sembra esserci una relazione biunivoca tra un vettore di $V$ e le sue coordinate in $\R^n$ rispetto ad una base. Infatti (come vedremo nella prossima parte) la relazione tra vettore di $V$ e vettore colonna delle sue coordinate e' un isomorfismo: essi rappresentano lo stesso oggetto sotto forme diverse. Quindi spesso per fare calcoli (ad esempio semplificare un insieme di vettori per trovare una base) possiamo passare allo spazio isomorfo $\R^n$, sfruttare i vettori colonna e le matrici (ad esempio facendo mosse di Gauss per riga o per colonna) e infine passare di nuovo allo spazio originale.

\section{Applicazioni lineari}

\begin{definition}
    Siano $V, W$ spazi vettoriali. Allora un'applicazione $f : V \to W$ si dice lineare
    se
    \begin{align}
        &f(\bm{0_V}) = \bm{0_W} \\
        &f(\bm{v} + \bm{w}) = f(\bm{v}) + f(\bm{w}) &&\forall v, w \in V \\
        &f(k\bm{v}) = kf(\bm{v})                    &&\forall v\in V, k \in \R 
    \end{align}
    $V$ si dice dominio dell'applicazione lineare, $W$ si dice codominio.
\end{definition}

\begin{definition}
    Siano $V, W$ spazi vettoriali e sia $f : V \to W$ lineare. Allora si dice immagine di $f$ l'insieme \begin{equation}
        \Imm{f} = \left\{ f(\bm{v}) \mid \bm v \in V\right\}.
    \end{equation}
\end{definition}

\begin{remark}
    Se $f : V \to W$ allora $\Imm{f} \subseteq W$. In particolare si puo' dimostrare che $\Imm{f}$ e' un sottospazio di $W$, e dunque che $0 \leq \dim\Imm{f} \leq \dim W$.
\end{remark}

\begin{definition}
    Siano $V, W$ spazi vettoriali e sia $f : V \to W$ lineare. Allora si dice kernel (o nucleo) di $f$ l'insieme \begin{equation}
        \ker{f} = \left\{ \bm{v} \in V \mid f(\bm v) = \bm{0_W}\right\}.
    \end{equation}
\end{definition}

\begin{theorem} 
    [delle dimensioni] \label{th_dimensioni}
    Siano $V, W$ spazi vettoriali e sia $f : V \to W$ lineare. Allora vale il seguente fatto:
    \begin{equation}
        \dim V = \dim \Imm f + \dim \ker f.
    \end{equation}
\end{theorem}

\subsection{Applicazioni iniettive e surgettive}

Le applicazioni lineari sono funzioni, dunque possono essere iniettive e surgettive, ma essendo lineari hanno delle proprieta' particolari.

\begin{definition}
    Siano $V, W$ spazi vettoriali e sia $f : V \to W$ lineare. Allora $f$ si dice iniettiva se per ogni $\bm{v}, \bm{u} \in V$ vale che $f(\bm{v}) = f(\bm{u})$ se e solo se $\bm{v} = \bm{u}$.
\end{definition}

\begin{proposition}\label{ker_funzione_iniettiva}
    Siano $V, W$ spazi vettoriali e sia $f : V \to W$ lineare. Allora $f$ e' iniettiva se e solo se $\ker f = \{\bm{0_V}\}$. 
\end{proposition}
\begin{proof}
    Notiamo che dato che $f$ e' lineare allora per definizione $f(\bm{0_V}) = \bm{0_W}$, dunque $\bm{0_V} \in \ker f$.
    \begin{description}
        \item [($\implies$).] Supponiamo che $f$ sia iniettiva e supponiamo che per qualche $\bm{v} \in V$ valga $\bm{v} \in \ker f$. Allora per definizione di kernel $f(\bm{v}) = \bm{0_W} = f(\bm{0_V})$, dunque per iniettivita' di $f$ da $f(\bm{v}) = f(\bm{0_V})$ segue che $\bm v = \bm{0_V}$. Dunque $\ker f = \{\bm{0_V}\}$.
        \item [($\impliedby$).] Supponiamo che $\ker f = \left\{ \bm{0_V}\right\}$. Per dimostrare che $f$ e' iniettiva e' sufficiente dimostrare che per ogni $\bm{v}, \bm{w} \in V$ segue che $f(\bm{v}) = f(\bm{w}) \implies \bm{v} = \bm{w}$.
        \begin{alignat*}{1}
            &f(\bm{v}) = f(\bm{w}) \\
            \iff &f(\bm{v}) - f(\bm{w}) = \bm{0_W} \\
            \iff &f(\bm{v} - \bm{w}) = \bm{0_W} \\
            \iff &\bm{v} - \bm{w} \in \ker f \\
            \intertext{ma l'unico elemento di $\ker f$ e' $\bm{0_V}$, dunque}
            \implies &\bm{v} - \bm{w} = \bm{0_V}\\
            \iff &\bm{v} = \bm{w}
        \end{alignat*}
        cioe' $f$ e' iniettiva. \qedhere
    \end{description}
\end{proof}

\begin{corollary}\label{iniettiva_allora_dimIm_uguale_dimV}
    Se $f$ e' iniettiva allora $\dim \Imm{f} = \dim V$.
\end{corollary}
\begin{proof}
    Infatti per la proposizione \ref{ker_funzione_iniettiva} $\dim \ker f = 0$, dunque per il teorema delle dimensioni (\ref{th_dimensioni}) segue che $\dim V = \dim \Imm{f} + \dim \ker f = \dim \Imm{f}$.
\end{proof}

\begin{corollary}
    Siano $V, W$ spazi vettoriali tali che $\dim V > \dim W$. Allora non puo' esistere $f : V \to W$ iniettiva.
\end{corollary}
\begin{proof}
    Infatti per il corollario \ref{iniettiva_allora_dimIm_uguale_dimV} segue che $\dim \Imm{f} = \dim V$, ma $\dim \Imm{f} < \dim W$ dunque non puo' essere che $\dim V > \dim W$.
\end{proof}

\begin{proposition}\label{indipendenti_mappati_indipendenti}
    Siano $V, W$ spazi vettoriali, $\bm{v_1}, \dots, \bm{v_n} \in V$ linearmente indipendenti e sia $f : V \to W$ lineare. Se $f$ e' iniettiva allora segue che $f(\bm{v_1}), \dots, f(\bm{v_n})$ sono linearmente indipendenti. 
\end{proposition}
\begin{proof}
    Consideriamo una combinazione lineare di $f(\bm{v_1}), \dots, f(\bm{v_n})$ e dimostriamo che imponendola uguale al vettore nullo segue che i coefficienti devono essere tutti nulli.
    \begin{alignat*}
        {1}
        &x_1f(\bm{v_1}) + \dots + x_nf(\bm{v_n}) = \bm{0_W}\\
        \iff &f(x_1\bm{v_1} + \dots + x_n\bm{v_n}) = \bm{0_W}\\
        \intertext{Per la proposizione \ref{ker_funzione_iniettiva} segue che}
        \iff &x_1\bm{v_1} + \dots + x_n\bm{v_n} = \bm{0_V}\\
        \intertext{Ma i vettori $\bm{v_1}, \dots, \bm{v_n}$ sono linearmente indipendenti, dunque l'unica combinazione lineare che li annulla e' quella a coefficienti nulli, cioe'}
        \iff &x_1 = \dots = x_n = 0
    \end{alignat*}
    Quindi una combinazione lineare di $f(\bm{v_1}), \dots, f(\bm{v_n})$ e' uguale al vettore nullo se e solo se tutti i coefficienti sono nulli, dunque $f(\bm{v_1}), \dots, f(\bm{v_n})$ sono linearmente indipendenti.
\end{proof}

\begin{definition}
    Siano $V, W$ spazi vettoriali e sia $f : V \to W$ lineare. Allora $f$ si dice surgettiva se per ogni $\bm{w} \in W$ esiste $\bm{v} \in V$ tale che $f(\bm{v}) = \bm{w}$.
\end{definition}

\begin{remark}
    Una funzione $f : V \to W$ e' surgettiva se e solo se $\Imm{f} = W$.
\end{remark}

\begin{proposition}\label{base_mappata_generatori_immagine}
    Sia $f : V \to W$. Allora se $\ang{\bm{v_1}, \dots, \bm{v_n}}$ e' una base di $V$ segue che $\left\{ f(\bm{v_1}), \dots, f(\bm{v_n})\right\}$ e' un insieme di generatori di $W$.
\end{proposition}
\begin{proof}
    Sia $\bm{w} \in \Imm{f}$ generico; allora questo equivale a dire che esiste $\bm{v} \in V$ tale che $f(\bm{v}) = \bm{w}$.
    Dato che $\ang{\bm{v_1}, \dots, \bm{v_n}}$ e' una base di $V$, allora possiamo scrivere $\bm{v}$ come $a_1\bm{v_1} + \dots a_n\bm{v_n}$, dunque 
    \begin{equation*}
        \bm{w} = f(a_1\bm{v_1} + \dots + a_n\bm{v_n}) = a_1f(\bm{v_1}) + \dots + a_nf(\bm{v_n}).
    \end{equation*}
    Dunque per la generalita' di $w$ segue che ogni elemento di $\Imm{f}$ appartiene allo span di $f(\bm{v_1}), \dots, f(\bm{v_n})$, cioe' $\left\{ f(\bm{v_1}), \dots, f(\bm{v_n})\right\}$ e' un insieme di generatori di $W$.
\end{proof}

\begin{corollary}\label{base_mappata_generatori_codominio}
    Sia $f : V \to W$ surgettiva. Allora se $\ang{\bm{v_1}, \dots, \bm{v_n}}$ e' una base di $V$ segue che $\left\{ f(\bm{v_1}), \dots, f(\bm{v_n})\right\}$ e' un insieme di generatori di $W$.
\end{corollary}
\begin{proof}
    Segue direttamente dalla proposizione \ref{base_mappata_generatori_immagine}: infatti se una funzione e' surgettiva allora $\Imm{f} = W$, dunque se $\left\{ f(\bm{v_1}), \dots, f(\bm{v_n})\right\}$ e' un insieme di generatori di $\Imm f$ segue che e' anche un insieme di generatori di $W$.
\end{proof}

\subsection{Isomorfismi}

\begin{definition}
    Siano $V, W$ spazi vettoriali e sia $f : V \to W$ lineare. Allora $f$ si dice bigettiva se $f$ e' sia iniettiva che surgettiva.
\end{definition}

\begin{definition}
    Una funzione $f : V \to W$ si dice invertibile se esiste $f^{-1} : W \to V$ tale che \begin{equation}
        f(\bm{v}) = \bm{w} \iff f^{-1}(\bm{w}) = \bm{v}
    \end{equation}
    Se $f$ e' invertibile allora $f^{-1}$ e' unica e si chiama inversa di $f$.
\end{definition}

\begin{remark}
    Un'applicazione lineare e' invertibile se e solo se e' bigettiva. 
\end{remark}

\begin{definition}
    Siano $V, W$ spazi vettoriali e sia $f : V \to W$ lineare. Allora se $f$ e' bigettiva si dice che $f$ e' un isomorfismo.
    
    Se esiste un isomorfismo tra gli spazi $V$ e $W$ allora si dice che $V$ e' isomorfo a $W$, e si indica con $V \cong W$.
\end{definition}

\begin{remark}
    Le seguenti affermazioni sono equivalenti:
    \begin{itemize}
        \item $f$ e' bigettiva;
        \item $f$ e' invertibile;
        \item $f$ e' un isomorfismo.
    \end{itemize}
\end{remark}

Gli isomorfismi preservano la linearita' dello spazio vettoriale e tutte le sue proprieta', come ci dicono le seguenti proposizioni.

\begin{proposition}
    Sia $V$ uno spazio vettoriale, $\alpha = \ang{\bm{v_1}, \dots, \bm{v_n}}$ una base di $V$. Allora se $f : V \to W$ e' un isomorfismo segue che $\beta = \ang{f(\bm{v_1}), \dots, f(\bm{v_n})}$ e' una base di $W$ (cioe' gli isomorfismi mappano basi in basi).
\end{proposition}
\begin{proof}
    Dato che $f$ e' un isomorfismo allora $f$ e' bigettiva.

    Dunque dato che $f$ e' iniettiva essa mappa un insieme di vettori indipendenti (come la base $\alpha$ di $V$) in un insieme di vettori linearmente indipendenti per la proposizione \ref{indipendenti_mappati_indipendenti}, dunque $\beta$ e' un insieme di vettori linearmente indipendenti. 

    Inoltre, dato che $f$ e' surgettiva, per la proposizione \ref{base_mappata_generatori_codominio} essa mappa una base di $V$ in un insieme di generatori del codominio $W$, dunque i vettori di $\beta$ generano $W$.

    Dunque $\beta$ e' un insieme di generatori linearmente indipendenti, e quindi e' una base di $W$.
\end{proof}

\begin{proposition}
    Se $V$ e' uno spazio vettoriale di dimensione $n = \dim V$, allora $V$ e' isomorfo a tutti e soli gli spazi vettoriali di dimensione $n$.
\end{proposition}
\begin{proof}
    Deriva direttamente dalla proposizione precedente: infatti ogni isomorfismo che ha come dominio $V$ deve portare una base di $V$ in una base di $W$, dunque la dimensione di $V$ deve essere uguale a quella di $W$.
\end{proof}

Quindi per calcolare una base di un sottospazio $W$ di uno spazio $V$ spesso conviene passare allo spazio dei vettori colonna $\R^n$ isomorfo allo spazio $V$, calcolare la base del sottospazio $\tilde{W}$ isomorfo a $W$ e infine tornare allo spazio di partenza.

\begin{example}
    Sia $V = \R[x]^{\leq 2}$ e sia $W \subseteq V$ il sottospazio di $V$ tale che $p \in W \iff p(2) = 0$. Dimostrare che $W$ e' un sottospazio e trovarne una base.
\end{example}
\begin{solution}
    Svolgiamo i due punti separatamente.
    \begin{enumerate}
        \item Dimostriamo che $W$ e' un sottospazio di $V$.
        \begin{itemize}
            \item Sia $\bm{0_V} \in V$ tale che $\bm{0_V}(x) = 0 + 0x + 0x^2$. Allora $\bm{0_V}(2) = 0 + 0\cdot 2 + 0 \cdot 4 = 0$, dunque $\bm{0_V} \in W$.
            \item Supponiamo $p, q \in W$ e mostriamo che $p+q \in W$. Dunque \[
                (p+q)(2) = p(2) + q(2) = 0 + 0 = 0    
            \] dunque $p + q \in W$.
            \item Supponiamo $p \in W$ e mostriamo che $kp \in W$ per un generico $k \in \R$. Dunque \[
                (kp)(2) = kp(2) = k \cdot 0 = 0    
            \] cioe' $kp \in W$ per ogni $k \in \R$.
        \end{itemize} 
        Dunque abbiamo dimostrato che $W$ e' un sottospazio di $V$.
        \item Cerchiamo ora una base per $W$.
         
        Consideriamo un generico $p \in V$, cioe' $p(x) = a + bx + cx^2$. La condizione che definisce $W$ e' $p(2) = a + 2b + 4c = 0$. 
    
        Passiamo ora allo spazio isomorfo $\R^3$. Il vettore corrispondente a $p$ in $\R^3$ e' $\bm{\tilde{p}} = \begin{psmallmatrix} a \\ b \\ c \end{psmallmatrix}$, mentre la condizione di appartenenza allo spazio $\widetilde{W} \subseteq \R^3$ isomorfo a $W$ e' sempre $a+2b+4c = 0$. Cerchiamo una base di $\widetilde{W}$ passando alla forma parametrica, cioe' cercando di esplicitare la condizione di appartenenza allo spazio e inserendola nella definizione stessa del vettore.
        Dato che la condizione e' data dal sistema $a+2b+4c = 0$ che ha due variabili libere, scelgo $b, c$ libere ottenendo $a = -2b - 4c$. Sostituendo in $\bm{\tilde{p}}$:\[
            \bm{\tilde p} = \begin{pmatrix} a \\ b \\ c \end{pmatrix} = \begin{pmatrix}
                -2b-4c\\b\\c
            \end{pmatrix} = b\begin{pmatrix} -2 \\ 1 \\ 0 \end{pmatrix} + c\begin{pmatrix} -4 \\ 0 \\ 1 \end{pmatrix}
        \]
        Dato che ogni vettore generico di $\widetilde{W}$ puo' essere scritto come combinazione lineare dei due vettori $\bm{\tilde{w}_1} = \begin{psmallmatrix} -2 \\ 1 \\ 0 \end{psmallmatrix}$ e $\bm{\tilde{w}_2} = \begin{psmallmatrix} -4 \\ 0 \\ 1 \end{psmallmatrix}$, allora segue che essi sono generatori di $W$. 
            
        Controlliamo ora che siano linearmente indipendenti riducendo a scalini per riga (secondo la proposizione \ref{estrarre_una_base}) la matrice che ha come colonne $\bm{\tilde{w}_1}$ e $\bm{\tilde{w}_2}$.
        \begin{equation*}
            \begin{pmatrix}[c|c]
                -2 & -4 \\ 1 & 0 \\ 0 & 1
            \end{pmatrix} \xrightarrow[]{R_2 + \frac12R_1}
            \begin{pmatrix}[c|c]
                -2 & -4 \\ 0 & -2 \\ 0 & 1
            \end{pmatrix} \xrightarrow[]{R_3 + \frac12R_2}
            \begin{pmatrix}[c|c]
                -2 & -4 \\ 0 & -2 \\ 0 & 0
            \end{pmatrix}
        \end{equation*}
        Dato che ci sono tanti pivot quante colonne segue che tutti i vettori originali sono indipendenti.
        I vettori $\bm{\tilde{w}_1}$ e $\bm{\tilde{w}_2}$ sono quindi indipendenti e generano $\widetilde{W}$: segue che $\ang{\bm{\tilde{w}_1}, \bm{\tilde{w}_1}}$ e' una base di $\widetilde{W}$, quindi $\dim \widetilde{W} = 2$.

        Tornando allo spazio originale, i vettori corrispondenti alla base sono quindi $w_1(x) = (-2 + x)$ e $w_2(x) = (-4 + x^2)$. L'insieme ordinato $\ang{(-2+x), (-4+x^2)}$ forma dunque una base di $W$ e dunque $\dim W = 2$.
    \end{enumerate}
\end{solution}

\subsection{Matrice associata ad una funzione}

Come avevamo visto nel primo capitolo, le matrici sono associate ad applicazioni lineari da vettori colonna in vettori colonna. Possiamo generalizzare questo concetto e definire una matrice associata ad ogni applicazione lineare.

\begin{definition}
    Siano $V, W$ spazi vettoriali, $f : V \to W$ lineare, $\alpha$ base di $V$ e $\beta$ base di $W$. Allora si dice chiama \textbf{matrice associata all'applicazione lineare} $f$ la matrice $[f]^{\alpha}_{\beta}$ tale che
    \begin{equation}
        \forall \bm{v} \in V. \quad \left( f(\bm{v}) \right)_{\beta} = [f]^{\alpha}_{\beta} \cdot (\bm{v})_{\alpha}
    \end{equation}
\end{definition}
