\chapter{Matrici e sistemi lineari}

\section{Matrici}

\begin{definition}[Matrice]
    Si dice matrice $m \times n$ una tabella di $m$ righe e $n$ colonne i cui elementi appartengono ad un campo $\K$ fissato, della forma
    \begin{equation}
        A = \begin{pmatrix}
            a_{11}  &   a_{12}  & \dots     & a_{1n} \\
            a_{21}  &   a_{22}  & \dots     & a_{2n} \\
            \vdots  &   \vdots  & \vdots    & \vdots \\
            a_{m1}  &   a_{m2}  & \dots     & a_{mn} \\
        \end{pmatrix}
        = [A_{ij}]_{i\leq m, j \leq n}
    \end{equation}
    L'insieme delle matrici di dimensione $m \times n$ con coefficienti in un campo $\K$ si indica con $\M_{m \times n}(\K)$.
\end{definition}

\begin{definition}[Vettore]
    Si dice vettore colonna una matrice $n \times 1$ del tipo
    \begin{equation}
        \vec{v} = \begin{pmatrix}
            a_{1} \\
            a_{2} \\
            \vdots \\
            a_n
        \end{pmatrix}
    \end{equation}
    Si dice vettore riga una matrice $1 \times n$ del tipo
    \begin{equation}
        \vec{w} = \begin{pmatrix}
            a_{1} & a_2 & \dots & a_n
        \end{pmatrix}
    \end{equation}
    L'insieme dei vettori colonna di $n$ elementi appartenenti ad un campo $\K$ si indica con $\K^n$, mentre l'insieme dei vettori riga di $n$ elementi appartenenti ad un campo $\K$ si indica con $\K^{\times n}$.
\end{definition}

E' evidente che se i due vettori $\vec{v}$ e $\vec{w}$ hanno la stessa dimensione e contengono gli stessi elementi allora rappresentano la stessa informazione, ma sotto forme diverse. Verificheremo piu' avanti infatti che $\K^n$ e $\K^{\times n}$ sono isomorfi, cioe' contengono gli stessi elementi in due forme diverse.

\subsection{Matrici particolari}

\begin{definition}[Matrice quadrata]
    Si dice \textbf{matrice quadrata} una matrice il cui numero di righe e' uguale al numero di colonne.
\end{definition}

\begin{definition}[Matrice triangolare superiore]
    Si dice \textbf{matrice triangolare superiore} una matrice quadrata tale che tutti gli elementi sotto la diagonale principale siano $0$, cioe'
    \begin{equation}
        A \in \M_{n \times n}(\R) \text{ e' triangolare superiore}  \iff \forall i > j. \, (a_{ij}) = 0.
    \end{equation}
\end{definition}

\begin{definition}[Matrice triangolare inferiore]
    Si dice \textbf{matrice triangolare inferiore} una matrice quadrata tale che tutti gli elementi sopra la diagonale principale siano $0$, cioe'
    \begin{equation}
        A \in \M_{n \times n}(\R) \text{ e' triangolare inferiore} \iff \forall i < j. \, (a_{ij}) = 0.
    \end{equation}
\end{definition}

\begin{definition}[Matrice diagonale]
    Si dice \textbf{matrice diagonale} una matrice quadrata tale che tutti gli elementi che non appartengono alla diagonale principale siano $0$, cioe'
    \begin{equation}
        A \in \M_{n \times n}(\R) \text{ e' diagonale} \iff \forall i \neq j. \, (a_{ij}) = 0.
    \end{equation}
\end{definition}

\begin{definition}[Matrice identita']
    Si dice \textbf{matrice identita'} $n \times n$ una matrice diagonale per cui la diagonale principale contiene solo $1$, cioe'
    \begin{equation}
        I \in \M_{n \times n}(\R) \text{ e' la matrice identita'} \iff (a_{ij}) = \begin{cases}
            0   &\text{se } i \neq j \\
            1   &\text{se } i = j \\
        \end{cases}.
    \end{equation}
\end{definition}

Ecco un esempio di una matrice quadrata $4 \times 4$, una matrice triangolare superiore $4 \times 4$, una matrice triangolare inferiore $4 \times 4$, una matrice diagonale $4 \times 4$ e la matrice identita' $4 \times 4$.

\begin{gather*}
    Q = \begin{pmatrix}
        1   &5   &4  &5\\
        2   &7   &10 &1\\
        0   &34  &3  &-2\\
        6   &-2  &2  &12\\
    \end{pmatrix}, \quad
    T_s = \begin{pmatrix}
        1   &5  &4  &5\\
        0   &7  &10 &1\\
        0   &0  &3  &-2\\
        0   &0  &0  &12\\
    \end{pmatrix}, \quad
    T_i = \begin{pmatrix}
        1   &0   &0  &0\\
        2   &7   &0  &0\\
        0   &34  &3  &0\\
        6   &-2  &2  &12\\
    \end{pmatrix}, \quad    
    \\D = \begin{pmatrix}
        1   &0              &0  &0\\
        0   &-\frac{1}{3}   &0 &0\\
        0   &0              &-3  &0\\
        0   &0              &0  &7\\
    \end{pmatrix}, \quad
    I_{4} = \begin{pmatrix}
        1   &0  &0  &0\\
        0   &1  &0 &0\\
        0   &0  &1  &0\\
        0   &0  &0  &1\\
    \end{pmatrix}
\end{gather*}

\subsection{Operazioni sulle matrici}

Consideriamo le operazioni fondamentali che coinvolgono matrici.

\subsubsection{Somma di matrici}

Siano $A, B$ due matrici $m \times n$ a coefficienti reali. Allora possiamo definire un'operazione di somma $+ : \M_{m \times n}(\R) \times \M_{m \times n}(\R) \to \M_{m \times n}(\R)$ tale che \begin{equation}
    A + B = [A_{ij} + B_{ij}]_{ij}.
\end{equation}
Cioe' \begin{gather*}
    A = \begin{pmatrix}
        a_{11}  & \dots     & a_{1n} \\
        a_{21}  & \dots     & a_{2n} \\
        \vdots  & \vdots    & \vdots \\
        a_{m1}  & \dots     & a_{mn} \\
    \end{pmatrix},\quad
    B = \begin{pmatrix}
        b_{11}  & \dots     & b_{1n} \\
        b_{21}  & \dots     & b_{2n} \\
        \vdots  & \vdots    & \vdots \\
        b_{m1}  & \dots     & b_{mn} \\
    \end{pmatrix} \\
    \\
    \implies
    A + B = \begin{pmatrix}
        a_{11} + b_{11}  & \dots     & a_{1n} + b_{1n} \\
        a_{21} + b_{21}  & \dots     & a_{2n} + b_{2n} \\
        \vdots           & \vdots    & \vdots          \\
        a_{m1} + b_{m1}  & \dots     & a_{mn} + b_{mn} \\
    \end{pmatrix}   
\end{gather*}

\subsubsection{Prodotto per uno scalare}

Sia $A$ due matrici $m \times n$ a coefficienti reali e $k \in \R$. Allora possiamo definire un'operazione di prodotto per scalare $\cdot : \R \times \M_{m \times n}(\R) \to \M_{m \times n}(\R)$ tale che \begin{equation}
    kA = [kA_{ij}]_{ij}.
\end{equation}
Cioe' \begin{gather*}
    A = \begin{pmatrix}
        a_{11}  & \dots     & a_{1n} \\
        a_{21}  & \dots     & a_{2n} \\
        \vdots  & \vdots    & \vdots \\
        a_{m1}  & \dots     & a_{mn} \\
    \end{pmatrix}
    \implies
    kA = \begin{pmatrix}
        ka_{11}  & \dots     & ka_{1n} \\
        ka_{21}  & \dots     & ka_{2n} \\
        \vdots   & \vdots    & \vdots \\
        ka_{m1}  & \dots     & ka_{mn} \\
    \end{pmatrix}  
\end{gather*}

\subsubsection{Prodotto riga per colonna}

Consideriamo un vettore riga $\vec{v} \in \R^{\times n}$ e un vettore colonna $\vec{w} \in \R^n$. Allora definiamo il prodotto riga per colonna $\cdot : \R^{\times n} \times \R^n \to \R$ tale che:
\begin{equation}
    \vec v \cdot \vec w = \begin{pmatrix}
        v_1 & \dots & v_n 
    \end{pmatrix} \cdot \begin{pmatrix}
        w_1 \\ \vdots \\ w_n
    \end{pmatrix} = v_1w_1 + \dots v_nw_n = \sum_{i = 1}^n v_iw_i
\end{equation}

\subsubsection{Prodotto tra matrici}

Possiamo estendere il prodotto riga per colonna a due matrici generiche, a patto che il numero di colonne della prima matrice sia uguale al numero di colonne della seconda.
Quindi se $A \in \M_{n \times m}(\R), B \in \M_{m \times k}(\R)$ esistera' anche $C = A \cdot B \in \M_{n \times k}(\R)$ tale che l'elemento in posizione $i, j$ di $C$ sara' dato dal prodotto dell'$i$-esima riga di $A$ e della $j$-esima colonna di $B$.
\begin{equation}
    (c_{ij}) = \begin{pmatrix}
        a_{i1} & \dots & a_{im} 
    \end{pmatrix} \cdot \begin{pmatrix}
        b_{1j} \\ \vdots \\ b_{mj}
    \end{pmatrix} = a_{i1}b_{1j} + \dots a_{im}b_{mj} = \sum_{t = 1}^m a_{it}b_{tj}
\end{equation} 

Il prodotto tra matrici rispetta le proprieta':
\begin{align*}
    &\text{1. associativa: }    && (AB)C = A(BC) \\
    &\text{2. distributiva: }   && (A + B)C = AC + BC \\
    &                           && A(B + C) = AB + AC
\end{align*}
ma non la proprieta' commutativa: infatti in generale $AB \neq BA$ anche nel caso in cui entrambi i prodotti sono definiti (come nel caso delle matrici quadrate). 

\begin{example}
    Calcoliamo il prodotto tra $\begin{psmallmatrix} a & b \\ c & d \end{psmallmatrix}$ e $\begin{psmallmatrix} x & y\\ z & t \end{psmallmatrix}$.
\end{example}
\begin{solution}
    \begin{equation*}
        \begin{pmatrix} a & b \\ c & d \end{pmatrix} \cdot \begin{pmatrix} x & y\\ z & t \end{pmatrix} = \begin{pmatrix}
            ax + bz & ay + bt \\ cx + dz & cy + dt
        \end{pmatrix}
    \end{equation*}
\end{solution}

\begin{proposition} \label{j-esima_colonna}
    Sia $A$ una matrice $m \times n$ e $\vec v \in \R^n$ tale che \[(v_i) = \begin{cases}
        0 &\text{se } i \neq j \\
        1 &\text{se } i = j 
    \end{cases}
    \] per qualche $j \leq n$. Allora $A\vec v$ e' la $j$-esima colonna di $A$.
\end{proposition}
\begin{proof}
    Consideriamo il prodotto tra $A$ e $\vec v$: \begin{gather*}
        A\vec v = \begin{pmatrix}
            a_{11}  & \dots   & a_{1j} & \dots  & a_{1n} \\
            a_{21}  & \dots   & a_{2j} & \dots  & a_{2n} \\
            \vdots  & \vdots  & \vdots & \vdots & \vdots \\
            a_{m1}  & \dots   & a_{mj} & \dots  & a_{mn} \\
        \end{pmatrix} \cdot   \begin{pmatrix}
            0 \\ \vdots \\ 1 \\ \vdots \\ 0 
        \end{pmatrix} \\
        = \begin{pmatrix}
            0a_{11}  + \dots   + 1a_{1j} + \dots  + 0a_{1n} \\
            0a_{21}  + \dots   + 1a_{2j} + \dots  + 0a_{2n} \\
            \vdots \\
            0a_{m1}  + \dots   + 1a_{mj} + \dots  + 0a_{mn} \\
        \end{pmatrix} = \begin{pmatrix}
            a_{1j}\\
            a_{2j}\\
            \vdots \\
            a_{mj} \\
        \end{pmatrix}
    \end{gather*}
    che e' esattamente la $j$-esima colonna di $A$.
\end{proof}

\subsubsection{Trasposizione}

Sia $A$ una matrice $m \times n$. Allora si dice \textbf{trasposta di $A$} la matrice $A^T$ di dimensione $n \times m$ tale che se $A = (a_{ij})$ allora $A^T = (b_{ij})$ dove $b_{ij} = a_{ji}$. 

Piu' semplicemente, la trasposta di una matrice si ottiene trasformando le sue righe in colonne. Ad esempio
\[
    \begin{pmatrix}
        a & b & c \\ d & e & f 
    \end{pmatrix}^T = \begin{pmatrix}
        a & d \\ b & e \\ c & f
    \end{pmatrix}
\]

Notiamo che:
\begin{itemize}
    \item la trasposta di un vettore colonna e' un vettore riga e viceversa: \begin{alignat*}{1}
        \begin{pmatrix} a \\ b \\ c \end{pmatrix}^T = \begin{pmatrix}
            a & b & c
        \end{pmatrix}, \quad&
        \begin{pmatrix} a & b & c \end{pmatrix}^T = \begin{pmatrix}
            a \\ b \\ c
        \end{pmatrix}
    \end{alignat*}
    \item la trasposta della trasposta di $A$ e' $A$: \[(A^T)^T = A\]
\end{itemize}

\section{Matrici a scalini}

\begin{definition}[Matrice a scalini per riga]
    Una matrice $A$ di dimensione $m \times n$ si dice a scalini per riga (o semplicemente a scalini) se:
    \begin{itemize}
        \item eventuali righe vuote sono in fondo alla matrice;
        \item il primo elemento non nullo di ogni riga e' in una colonna piu' a destra del primo elemento non nullo della riga precedente.
    \end{itemize}
\end{definition}

\begin{definition}[Pivot di riga]
    Sia $A$ una matrice ridotta a scalini. Allora il primo elemento non nullo di una riga viene detto \textbf{pivot} della riga.
\end{definition}

Ad esempio la matrice \[
    A = \begin{pmatrix}
        1   &2  &7  &5\\
        0   &0  &10 &1\\
        0   &0  &0  &-2\\
        0   &0  &0  &0\\
    \end{pmatrix}
\] e' a scalini ed ha come pivot gli elementi $(a_{11}) = 1, (a_{23}) = 10, (a_{34}) = -2$.

\begin{definition}[Matrice a scalini per colonna]
    Una matrice $A$ di dimensione $m \times n$ si dice a scalini per colonna se la sua trasposta $A^T$ e' a scalini per riga.
\end{definition}

\begin{definition}[Pivot di colonna]
    Sia $A$ una matrice ridotta a scalini per colonna. Allora il primo elemento non nullo di una colonna viene detto \textbf{pivot} della colonna.
\end{definition}

Ad esempio la matrice \[
    A = \begin{pmatrix}
        1   &0  &0  &0\\
        2   &0  &0  &0\\
        7   &10 &0  &0\\
        5   &1  &-2 &0\\
    \end{pmatrix}
\] e' a scalini per colonna ed ha come pivot gli elementi $(a_{11}) = 1, (a_{32}) = 10, (a_{43}) = -2$. Notiamo che $A^T$ e' la matrice dell'esempio precedente, che era a scalini per riga.

Prendiamo una generica matrice $A$ di dimensione $m \times n$. Possiamo trasformare ogni $A$ in una matrice a scalini $\bar{A}$ applicando le seguenti mosse, chiamate \textbf{mosse di Gauss per riga}:
\begin{itemize}
    \item scambio la riga $i$ con la riga $j$;
    \item sostituisco la riga $i$ con la somma di se stessa e di un multiplo della riga $j$;
    \item moltiplico la riga $i$ per un numero reale.
\end{itemize}

In particolare possiamo sfruttare il seguente \textbf{algoritmo di Gauss} per ridurre una matrice a scalini:
\begin{itemize}
    \item se la matrice contiene una sola riga, oppure tutte le righe contengono solo zeri, allora la matrice e' a scalini;
    \item altrimenti scelgo la riga $R$ con pivot piu' a sinistra possibile, chiamo $c$ la colonna su cui si trova il pivot di $R$ e applico il seguente procedimento: \begin{enumerate}
        \item scelgo una riga $R'$ con pivot sulla colonna $c$;
        \item sottraggo a $R'$ un multiplo di $R$ in modo che l'elemento nella riga $R'$ e nella colonna $c$ diventi $0$;
        \item ripeto questo procedimento fino a quando solo $R$ ha un pivot nella colonna $c$; 
    \end{enumerate}
    \item a questo punto $R$ e' l'unica riga con pivot sulla colonna $c$, dunque escludiamola dalla matrice e ripetiamo il procedimento sulle altre righe.
\end{itemize}

Per semplificare ancora piu' la matrice possiamo annullare le righe sopra i pivot sommando ad esse multipli della riga contenente il pivot: l'algoritmo che ne risulta viene chiamato \textbf{algoritmo di Gauss-Jordan}.

In casi che vedremo in seguito potra' essere utile utilizzare le \textbf{mosse di Gauss per colonna}, che ci permettono di ridurre la matrice a scalini per colonna applicando le seguenti mosse:
\begin{itemize}
    \item scambio la colonna $i$ con la colonna $j$;
    \item sostituisco la colonna $i$ con la somma di se stessa e di un multiplo della colonna $j$;
    \item moltiplico la colonna $i$ per un numero reale. 
\end{itemize}

\begin{definition}[Rango]
    Sia $A$ una matrice e sia $\bar{A}$ la matrice a scalini ottenuta tramite le mosse di Gauss per riga su $A$. Il numero di pivot della matrice $\bar{A}$ si dice \textbf{rango riga} di $A$ e si indica con $\rk{A}$.
\end{definition}

Ad esempio la matrice di prima \[
    A = \begin{pmatrix}
        1   &2  &7  &5\\
        0   &0  &10 &1\\
        0   &0  &0  &-2\\
        0   &0  &0  &0\\
    \end{pmatrix}
\]  ha 3 pivot, dunque $\rk{A} = 3$.

\section{Sistemi di equazioni lineari}

\begin{definition}[Equazione lineare e soluzione di un'equazione]
    Si dice \textbf{equazione lineare} nelle incognite $x_1, x_2, \dots, x_n \in \R$ un'equazione del tipo
    \begin{equation} \label{equazione_generica}
        a_1x_1 + a_2x_2 + \dots + a_nx_n = b
    \end{equation}
    con $a_1, a_2, \dots, a_n, b \in \R$ noti.
    Una \textbf{soluzione} di un'equazione lineare e' una $n$-upla $(x_1, \dots, x_n)$ per cui l'uguaglianza \ref{equazione_generica} vale.
    Due equazioni si dicono \textbf{equivalenti} se hanno le stesse soluzioni.
\end{definition}

\begin{definition}[Sistema di equazioni lineari]
    Si dice \textbf{sistema di equazioni lineari} un'insieme di $k$ equazioni lineari a $n$ incognite della forma
    \begin{equation}
        \left\{
        \begin{array}{@{}rororor }
        a_{11}x_1 & + & \dots & + & a_{1n}x_n & = & b_1 \\
        a_{21}x_1 & + & \dots & + & a_{2n}x_n & = & b_2 \\
        &&\vdots\\
        a_{k1}x_1 & + & \dots & + & a_{kn}x_n & = & b_k \\
        \end{array}
        \right.
    \end{equation}
    Una \textbf{soluzione} del sistema e' una $n$-upla $(x_1, \dots, x_n)$ che e' soluzione di tutte le $k$ equazioni del sistema.
    Due sistemi si dicono \textbf{equivalenti} se hanno le stesse soluzioni.
    Se $b_1 = b_2 = \dots = b_n = 0$ allora il sistema si dice \textbf{omogeneo}.
\end{definition}

Possiamo trasformare un sistema di equazioni in un'equazione tra vettori: infatti sappiamo che due vettori sono uguali se e solo se tutti i loro elementi nella stessa posizione sono uguali. Un sistema di $k$ equazioni a $n$ incognite diventa quindi:
\begin{equation*}
    \begin{pmatrix}
        a_{11}x_1 & + & \dots & + & a_{1n}x_n \\
        a_{21}x_1 & + & \dots & + & a_{2n}x_n \\
        \vdots    &   & \vdots &  & \vdots\\
        a_{k1}x_1 & + & \dots & + & a_{kn}x_n \\
    \end{pmatrix} = \begin{pmatrix}
        b_1 \\ b_2 \\ \vdots \\ b_k
    \end{pmatrix}
\end{equation*}
Notiamo inoltre che 
\begin{equation*}
    \begin{pmatrix}
        a_{11}x_1 & + & \dots & + & a_{1n}x_n \\
        a_{21}x_1 & + & \dots & + & a_{2n}x_n \\
        \vdots    &   & \vdots &  & \vdots\\
        a_{k1}x_1 & + & \dots & + & a_{kn}x_n \\
    \end{pmatrix} = \begin{pmatrix}
        a_{11} & \dots & a_{1n} \\
        a_{21} & \dots & a_{2n} \\
        \vdots & \vdots& \vdots \\
        a_{k1} & \dots & a_{kn} \\
    \end{pmatrix} \cdot \begin{pmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{pmatrix}
    = A\vec{x}
\end{equation*}

Dunque arriviamo al seguente fatto:
\begin{proposition}[Sistema sotto forma di matrici]
    Ogni sistema di $k$ equazioni in $n$ incognite puo' essere scritto nella forma di una singola equazione matriciale della forma $A\vec{x} = \vec{b}$, dove:
    \begin{itemize}
        \item $A$ e' la matrice dei coefficienti, tale che $a_{ij}$ e' il coefficiente del termine $x_j$ nell'equazione nella riga $i$;
        \item $\vec{x} \in \R^n$ e' il vettore colonna delle incognite;
        \item $\vec{b} \in \R^k$ e' il vettore colonna dei termini noti.
    \end{itemize}
\end{proposition}
    
\begin{definition}[Matrice completa associata al sistema]
    Consideriamo un'equazione matriciale della forma $A\vec{x} = \vec b$. Allora la matrice $(A|\vec b)$ ottenuta aggiungendo alla matrice $A$ una colonna contenente il vettore colonna $\vec b$ si dice \textbf{matrice completa} associata al sistema lineare.
    \begin{equation}
        (A|\vec b) = \begin{pmatrix}[ccc|c]
            a_{11} & \dots & a_{1n} & b_1\\
            a_{21} & \dots & a_{2n} & b_2\\
            \vdots & \vdots& \vdots & \vdots\\
            a_{k1} & \dots & a_{kn} & b_k\\
        \end{pmatrix}.
    \end{equation}
    Se il sistema e' omogeneo si puo' omettere la colonna dei termini noti.
\end{definition}


Il metodo piu' veloce per risolvere un sistema lineare consiste nel ridurre la matrice completa $(A|\vec b)$ a scalini tramite le mosse di Gauss di riga: a quel punto possono esserci due situazioni:
\begin{itemize}
    \item se ci sono righe con solo zeri prima della barra verticale, ma con un coefficiente diverso da zero dopo, allora quell'equazione non ha soluzione in quanto e' della forma $0x_1 + \dots 0x_n = b_i \neq 0$, dunque il sistema e' impossibile;
    \item altrimenti il sistema ha almeno una soluzione.
\end{itemize}
Per stabilire le soluzioni del sistema si procede in questo modo:
\begin{itemize}
    \item si scelgono libere tutte le variabili che sono su colonne che non contengono pivot;
    \item si ricavano le altre variabili passando al sistema associato alla matrice a scalini ottenuta alla fine.
\end{itemize}

\begin{proposition}[Le mosse di Gauss per riga trasformano un sistema in un sistema equivalente]
    Le mosse di Gauss per riga applicate ad una matrice completa $(A|\vec b)$ trasformano la matrice in una nuova matrice il cui sistema associato e' equivalente all'originale.
\end{proposition}
\begin{proof} Dimostriamo che le tre mosse trasformano il sistema in un sistema equivalente.
    \begin{itemize}
        \item La prima mossa consente lo scambio di due righe, che non modifica il sistema.
        \item Consideriamo due righe della matrice \[
            \begin{pmatrix}[ccc|c]
                a_{i1} & \dots & a_{in} & b_i\\
                a_{j1} & \dots & a_{jn} & b_j\\
            \end{pmatrix} 
            \text{ che corrispondono a }
            \left\{
                \begin{array}{@{}rororor }
                a_{i1}x_1 & + & \dots & + & a_{in}x_n & = & b_i \\
                a_{j1}x_1 & + & \dots & + & a_{jn}x_n & = & b_j
                \end{array}
            \right.
        \]
        Dato che aggiungendo quantita' uguali ad entrambi i membri di un'equazione otteniamo un'equazione equivalente a quella data, possiamo aggiungere al primo membro della prima equazione $k(a_{j1}x_1 + \dots + a_{jn}x_n) = ka_{j1}x_1 + \dots + ka_{jn}x_n$ e al secondo membro $kb_j$, ottenendo: \[
            \left\{
                \begin{array}{@{}rororor }
                (a_{i1} + ka_{j1})x_1 & + & \dots & + & (a_{in} + ka_{jn})x_n & = & b_i + kb_j \\
                a_{j1}x_1 & + & \dots & + & a_{jn}x_n & = & b_j
                \end{array}
                \right.
        \] che equivale a \[
        \begin{pmatrix}[ccc|c]
            a_{i1} + ka_{j1}    & \dots & a_{in} + ka_{jn}  & b_i + kb_j\\
            a_{j1}              & \dots & a_{jn}            & b_j\\
        \end{pmatrix} 
        \]
        \item Dato che una riga della matrice indica un'equazione, allora possiamo moltiplicare entrambi i membri dell'equazione per uno stesso numero reale e ottenere un'equazione equivalente a quella data.
    \end{itemize}
\end{proof}

\begin{example}
    Risolvere il seguente sistema di equazioni.
    \begin{equation*}
        \left\{
                \begin{array}{@{}rorororor }
                x & + & y & + & z & + & t & = & 1 \\
                x & + & y & + & 2z & + & 2t & = & 2 \\
                2x & + & 2y & + & 3z & + & 3t & = & 3 \\
                \end{array}
            \right.
    \end{equation*}
\end{example}
\begin{solution}
    Troviamo le soluzioni semplificando la matrice completa associata al sistema tramite mosse di Gauss-Jordan.
    \begin{gather*}
        \begin{pmatrix}[cccc|c]
            1&1&1&1&1\\1&1&2&2&2\\2&2&3&3&3\\
        \end{pmatrix} \xrightarrow[R_2-R_1]{R_3-2R_1}
        \begin{pmatrix}[cccc|c]
            1&1&1&1&1\\0&0&1&1&1\\0&0&1&1&1\\
        \end{pmatrix} \xrightarrow[]{R_3-R_2} \\ \xrightarrow[]{R_3-R_2}
        \begin{pmatrix}[cccc|c]
            1&1&1&1&1\\0&0&1&1&1\\0&0&0&0&0\\
        \end{pmatrix} \xrightarrow[]{R_2-R_1}
        \begin{pmatrix}[cccc|c]
            1&1&0&0&0\\0&0&1&1&1\\0&0&0&0&0\\
        \end{pmatrix}
    \end{gather*} 
    Le colonne senza pivot sono quelle della $y$ e della $t$, dunque scegliamo $y$ e $t$ libere e torniamo al sistema associato, ottenendo:
    \begin{equation*}
        \left\{
                \begin{array}{@{}ror}
                x & = & -y \\
                y & = & y \\
                z & = & 1-t \\
                t & = & t \\
                \end{array}
            \right.
    \end{equation*}
\end{solution}

\begin{theorem}[Teorema di Rouché-Capelli] \label{rouche_capelli}
    Sia $A\vec{x} = \vec b$ un'equazione matriciale. Allora essa ha soluzione se e solo se $\rk{A|\vec b} = \rk{A}$.
\end{theorem}
\begin{intuition}
    Infatti se il rango delle due matrici e' diverso significa che una riga nulla nella matrice $A$ ha un pivot nella matrice $(A|\vec b)$, cioe' che c'e' un'equazione del tipo $0x_1 + \dots 0x_n = b_i \neq 0$ che e' impossibile.
\end{intuition}

\begin{proposition}[Un sistema omogeneo ammette sempre una soluzione]
    Un sistema omogeneo $A\vec{x} = \vec 0$ ammette sempre almeno una soluzione.
\end{proposition}
\begin{proof}
    Infatti viene sempre che $\rk{A|\vec b} = \rk{A|\vec 0} = \rk{A}$, dunque per Rouché-Capelli (\ref{rouche_capelli}) il sistema ha soluzione.
\end{proof}

\begin{proposition}
    Le soluzioni di un sistema lineare $A\vec x = \vec b$ sono tutte e solo della forma $\vec{x_0} + \vec{\bar{x}}$, dove $\vec{x_0}$ e' una qualunque soluzione del sistema omogeneo associato $A\vec x = \vec 0$ e $\vec{\bar{x}}$ e' una soluzione particolare di $A\vec x = \vec b$.
\end{proposition}
\begin{proof}
    Dimostriamo innanzitutto che $\vec{x_0} + \vec{\bar{x}}$ e' soluzione.
    \[A(\vec{x_0} + \vec{\bar{x}}) = A\vec{x_0} + A\vec{\bar{x}} = \vec 0 + \vec b = \vec b\]

    Dimostriamo poi che se $\vec x$ e' una soluzione di $A\vec x = \vec b$, allora $\vec x - \vec{\bar x}$ e' soluzione del sistema omogeneo.
    \[A(\vec{x} - \vec{\bar{x}}) = A\vec{x} - A\vec{\bar{x}} = \vec b - \vec b = \vec 0\]
    che e' la tesi.
\end{proof}

Notiamo che per avere una e una sola soluzione per un sistema della forma $A\vec x = \vec b$ e' necessario che il numero di equazioni sia uguale al numero di incognite, cioe' che la matrice dei coefficienti sia quadrata. Inoltre abbiamo bisogno di un'altra condizione, che viene specificata dalla proposizione seguente.

\begin{proposition}\label{sistema_quadrato_n_pivot_unica_soluzione}
    Sia $A \in \M_{n \times n}(\R)$, $\vec{x}, \vec{b} \in \R^n$. Allora il sistema $A\vec x = \vec b$ ha una e una sola soluzione se e solo se $\rk{A} = n$.
\end{proposition}
\begin{proof}
    Riduciamo la matrice completa $(A | \vec b)$ a scalini, ottenendo la matrice $(A' | \vec b')$.

    Se $\rk{A} = n$, allora $\rk{A | \vec b} = n$, dunque $(A' | \vec b')$ avra' $n$ pivot. Dato che non ci sono possibili scelte di variabili libere, sicuramente avremo una e una sola soluzione.

    Se $\rk{A} < n$, allora $\rk{A'} < n$. Abbiamo due casi: \begin{itemize}
        \item se $\rk{A' | \vec b'} > \rk{A'}$ allora per il teorema  di Rouché-Capelli (\ref{rouche_capelli}) il sistema non ha soluzione;
        \item se $\rk{A' | \vec b'} = \rk{A'} < n$ allora il sistema avra' delle variabili libere, dunque avra' infinite soluzioni.
    \end{itemize}
    Dunque se $\rk{A} < n$ il sistema non puo' avere una ed una sola soluzione.
\end{proof}

\begin{example}
    Discutere il numero di soluzioni del seguente sistema al variare di $k$.
    \begin{equation*}
        \left\{
                \begin{array}{@{}rororor }
                x & + & ky & + & (1+4k)z & = & 1+4k \\
                2x & + & (k+1)y & + & (2+7k)z & = & 1+7k \\
                3x & + & (k+2)y & + & (3+9k)z & = & 1+9k \\
                \end{array}
            \right.
    \end{equation*}
\end{example}
\begin{solution}
    Troviamo le soluzioni semplificando la matrice completa associata al sistema tramite mosse di Gauss-Jordan.
    \begin{gather*}
        \begin{pmatrix}[ccc|c]
            1&k&1+4k&1+4k\\2&k+1&2+7k&1+7k\\3&k+1&3+9k&1+9k\\
        \end{pmatrix} \xrightarrow[R_2-2R_1]{R_3-3R_1}
        \begin{pmatrix}[ccc|c]
            1&k&1+4k&1+4k\\0&1-k&-k&-k-1\\0&2-2k&-3k&-3k-2\\
        \end{pmatrix}\\ \xrightarrow[]{R_3-2R_2} 
        \begin{pmatrix}[ccc|c]
            1&k&1+4k&1+4k\\0&1-k&-k&-k-1\\0&0&-k&-k\\
        \end{pmatrix}
    \end{gather*} 
    Discutiamo l'esistenza e il numero di soluzioni.
    \begin{itemize}
        \item Se $k \neq 0$ e $k \neq 1$ allora la matrice e' quadrata ed ha un pivot per riga, dunque ammette una e una sola soluzione.
        \item Se $k = 0$ allora la matrice diventa \[
            \begin{pmatrix}[ccc|c]
                1&0&1&1\\0&1&0&-1\\0&0&0&0
            \end{pmatrix}    
        \] che ha $2$ pivot e una colonna senza pivot, dunque ha una variabile libera e nessuna equazione impossibile, quindi ha infinite soluzioni e la dimensione dell'insieme delle soluzioni e' $1$.
        \item Se $k = 1$ allora la matrice diventa \[
            \begin{pmatrix}[ccc|c]
                1&0&5&5\\0&0&-1&-2\\0&0&-1&-1
            \end{pmatrix} \xrightarrow[]{R_3-R_2} 
            \begin{pmatrix}[ccc|c]
                1&0&5&5\\0&0&-1&-2\\0&0&0&1
            \end{pmatrix}
        \] che ha un'equazione impossibile, dunque non ammette soluzioni.
    \end{itemize}
\end{solution}

\section{Matrici come applicazioni lineari}

Abbiamo visto che moltiplicando una matrice per un vettore colonna con la giusta dimensione otteniamo un nuovo vettore colonna. Possiamo quindi interpretare una matrice come una funzione che manda vettori in vettori:
\begin{definition}[Applicazione lineare associata ad una matrice]
    Sia $A \in \M_{n \times m}(\R)$. Allora si dice \textbf{applicazione lineare associata alla matrice} la funzione $L_A : \R^m \to \R^n$ tale che \begin{equation}
        \forall \vec x \in \R^m. \quad L_A(\vec x) = A \vec x
    \end{equation}
\end{definition}

\begin{proposition}[Linearita']
    Sia $A \in \M_{n \times m}(\R)$ una matrice, $L_A : \R^m \to \R^n$ la sua applicazione lineare associata. Allora valgono le seguenti:
    \begin{align}
        &L_A(\vec 0) = \vec 0 \\
        &L_A(\vec v + \vec w) = L_A(\vec v) + L_A(\vec w) &\forall \vec v, \vec w \in \R^m \\
        &L_A(k\vec v) = kL_A(\vec v) &\forall \vec v \in \R^m, k \in \R. 
    \end{align}
\end{proposition}
\begin{proof}
    Per definizione di applicazione lineare:
    \begin{itemize}
        \item $L_A(\vec 0) = A\vec 0 = \vec 0$;
        \item $L_A(\vec v + \vec w) = A(\vec v + \vec w) = A\vec v + A\vec w = L_A(\vec v) + L_A(\vec w)$;
        \item $L_A(k\vec v) = A(k\vec v) = kA\vec v = kL_A(\vec v)$
    \end{itemize}
    che e' la tesi.
\end{proof}

\begin{proposition}[Composizione come prodotto tra matrici]
    Siano $A \in \M_{n \times m}(\R)$, $B \in \M_{m \times k}(\R)$ e $L_A : \R^n \to \R^m$ e $L_B : \R^m \to \R^k$. Allora la funzione composta $L_B \circ L_A : \R^n \to \R^k$ e' associata alla matrice $B\cdot A$.
\end{proposition}
\begin{proof}
    Sia $\vec{x} \in \R^n$. Allora
    \[
        (L_B \circ L_A)(\vec{x}) = L_B(L_A(\vec{x})) = L_B(A\vec{x}) = B(A\vec{x}) = (BA)\vec{x}
    \]
    che e' la tesi.
\end{proof}

\begin{definition}[Immagine di un'applicazione lineare]
    Sia $A \in \M_{n \times m}(\R)$ una matrice, $L_A : \R^m \to \R^n$ la sua applicazione lineare associata. Allora si dice \textbf{immagine} dell'applicazione $L_A$ (o della matrice $A$) l'insieme:
    \begin{equation}
        \Imm{L_A} = \{L_A(\vec x) \mid \vec x \in \R^m\}.
    \end{equation}
\end{definition}

\begin{definition}[Kernel di un'applicazione lineare]
    Sia $A \in \M_{n \times m}(\R)$ una matrice, $L_A : \R^m \to \R^n$ la sua applicazione lineare associata. Allora si dice \textbf{kernel} dell'applicazione $L_A$ (o della matrice $A$) l'insieme:
    \begin{equation}
        \ker{L_A} = \{\vec x \mid L_A(\vec x) = \vec 0\}.
    \end{equation}
\end{definition}

\begin{remark}
    Il kernel di una matrice $A \in \M_{n \times m}(\R)$ e' l'insieme dei vettori colonna $\vec{x} \in \R^m$ tali che $A\vec{x} = \vec{0}$, cioe' e' l'insieme delle soluzioni del sistema omogeneo $A\vec{x}= \vec{0}$.
\end{remark}

Notiamo quindi che, essendoci una corrispondenza tra il kernel di una matrice e l'insieme delle soluzioni del sistema omogeneo associato alla matrice, le mosse di Gauss di riga non modificano il kernel di una matrice. Infatti sappiamo che le mosse non modificano l'insieme delle soluzioni del sistema omogeneo, dunque non modificheranno neanche il kernel.